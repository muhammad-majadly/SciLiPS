{
  "paper_id": "87",
  "paper_title": "87",
  "sections": [
    {
      "section": "FrontMatter",
      "chunks": [
        "XCube: Large-Scale 3D Generative Modeling using Sparse Voxel Hierarchies Xuanchi Ren1,2,3 Jiahui Huang1 Xiaohui Zeng1,2,3 Ken Museth1 Sanja Fidler1,2,3 Francis Williams1 1NVIDIA 2University of Toronto 3Vector Institute Sparse Voxel Resolution = 10243 Normal Semantics Figure 1. XCube (X 3). Our model generates high-resolution (up to 10243) sparse 3D voxel hierarchies of objects and driving scenes in under 30 seconds. The voxels are enriched with arbitrary attributes such as semantics, normals, and TSDF from which mesh could be readily extracted. Here we show randomly sampled geometries using our model trained on ShapeNet, Objaverse, Karton City, and Waymo."
      ]
    },
    {
      "section": "Abstract",
      "chunks": [
        "We present XCube (abbreviated as X 3), a novel generative model for high-resolution sparse 3D voxel grids with arbitrary attributes. Our model can generate millions of voxels with a ﬁnest effective resolution of up to 10243 in a feed-forward fashion without time-consuming test-time optimization. To achieve this, we employ a hierarchical voxel latent diffusion model which generates progressively higher resolution grids in a coarse-to-ﬁne manner using a custom framework built on the highly efﬁcient VDB data structure. Apart from generating high-resolution objects, we demonstrate the effectiveness of XCube on large outdoor scenes at scales of 100 m×100 m with a voxel size as small as 10 cm. We observe clear qualitative and quantitative improvements over past approaches. In addition to unconditional generation, we show that our model can be used to solve a variety of tasks such as user-guided editing, scene completion from a single scan, and text-to-3D. More results and details can be found on our project webpage."
      ]
    },
    {
      "section": "Introduction",
      "chunks": [
        "Equipping machines with the ability to create and understand three-dimensional scenes and objects has long been a tantalizing pursuit, promising a bridge between the digital and physical worlds. The problem of 3D content generation lies at the heart of this endeavor. By modeling the distribution of objects and scenes, generative models can propose plausible shapes and their attributes from scratch, from user input, or from partial observations. There has been a surge of new and exciting works on generative models for 3D shapes in recent years. Initial work in this area, train on datasets of 3D shapes and leverage 3D priors to perform generation [17, 21]. While these works produce impressive results, their diversity and shape quality is fundamentally bounded by the size of 3D datasets (e.g. [5, 12]), as well as their underlying 3D representation. To address the diversity problem, one line of work [43, 65] proposed an elegant solution that leverages powerful 2D generative models to produce 3D structures using inverse rendering and a diffusion score distillation formulation. While they beneﬁt from the massive corpora of 2D image data and can generate highly diverse and high-quality shapes, the generated shapes usually suffer from the Janus face problem, and the generation process requires test-time optimization that is lengthy and computationally expensive. More recent works such as [30, 32] achieve state-of-the-art 3D generation by smartly combining a mix of 2D and 3D priors. They gain diversity from 2D data and spatial consistency from 3D data and speed up the generation process by operating directly in 3D. These works motivate a deeper investigation into the fundamentals of 3D priors for generation. In particular, current 3D priors are limited to low resolutions [26], and do not scale well to large outdoor scenes such as those in auThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.",
        "tonomous driving and robotics applications. These datasets of large-scale scenes are abundant and contain more data than those consisting solely of objects [56]. Thus, developing a scalable 3D generative prior has the potential to unlock new sources of training data and further push the boundary of what is possible in 3D generation. In this work, we aim to explore the limits of purely 3D generative priors, scaling them to high resolutions and large-scale scenes. Our model is capable of scaling to high-resolution outputs (e.g. 10243) by leveraging a novel sparse formulation and can produce outputs with high geometric complexity, by focusing dense geometry near the surface of a shape. Our method, X 3, is a novel hierarchical voxel latent diffusion model for generating high-resolution 3D objects and scenes with arbitrary attributes such as signed distances, normals, and semantics. Our model generates a latent Sparse Voxel Hierarchy — a hierarchy of 3D sparse voxel grids with latent features at each voxel — in a coarse-to-ﬁne manner. In particular, we model each level of the hierarchy as a latent diffusion model conditioned on the coarser level. The latent space at each level is encoded using a highly efﬁcient — both in terms of compute and memory — sparse structure Variational Autoencoder (VAE). Our generated representation enjoys several key beneﬁts: (1) it is fully 3D, enabling it to model intricate details at multiple resolutions, (2) it can output very high-resolution shapes (up to 10243 resolution) by leveraging sparsity, (3) the distribution at each level is easy to model since the coarse level need only model a rough shape, and ﬁner levels are concerned with local details, (4) our generated shapes support multi-scale user-guided editing by modifying coarser levels and regenerating ﬁner levels, (5) since our model leverages a latent diffusion model over a hierarchy of features, we are able to decode arbitrary multiscale attributes (e.g. semantics, TSDF) from those features. We demonstrate the effectiveness of our hierarchical voxel latent diffusion model on standard object datasets such as Objaverse [12] and ShapeNet [5] achieving state-of-theart results on unconditional and conditional generation from texts and category labels. We further demonstrate the scalability of our method by demonstrating high-quality unconditional and conditional (from a single lidar scan) generation on large outdoor scenes from the Waymo Open Dataset [56] and Karton City [1]. Finally, by leveraging a custom sparse 3D deep learning framework built on VDB [36], our model is capable of generating complex shapes at 10243 resolution containing millions of voxels in under 30 seconds."
      ]
    },
    {
      "section": "Related Work",
      "chunks": [
        "Generative Probabilistic Models. Common generative models include variational autoencoders (VAE) [25], generative adversarial networks (GAN) [16], normalizing ﬂows [46], autoregressive models (AR) [63], and more recently diffusion models (DM) [18, 53]. A popular method for generative modeling is latent diffusion that has been found useful in, e.g., images [42, 48, 62] and videos [3], where the diffusion process happens over the latent space of a simpler generative model (e.g. a VAE). Latent diffusion models allow for easy decoding of multiple attributes through different decoders. In our work, we employ a latent diffusion model over a hierarchy of sparse voxels. 3D Generative Models. The recent surge in the 3D generative modeling literature mostly focuses on object-level shape synthesis. One line of work opts for distilling 2D image priors into 3D via inverse rendering [28, 32, 65], while others [14, 23, 38, 40, 41, 69] focus on directly learning 3D priors from large-scale 3D datasets [11, 12]. Recently, hybrid 2D-3D approaches [31, 57] which better leverage both image priors and large 3D datasets have started to emerge. Fundamental to these works are good 3D priors that can instill multiview consistency without the need for expensive test-time optimization. This is the interest of our work. Works that tackle large-scale scene generation either choose a procedural approach that decouples the generation of different scene components (e.g. roads, buildings, etc.) [45, 55, 67], or a more generic approach that generates the entire scene at once [8, 24, 29]. While the former approaches usually provide more details, they are limited to generating a ﬁxed subset of possible scenes and require specialized data to train. The latter approaches are theoretically more ﬂexible but are currently bounded by their 3D representation power (hence producing fewer details), which is a problem we address in this work. 3D Representation for Generative Tasks. Point clouds [2, 34, 37, 69] are ﬂexible and adaptive, but cannot represent solid surfaces and pose challenges in architecture design. Triangle meshes [15, 39] are more expressive but are limited to a ﬁxed topology and hence hard to optimize. Neural ﬁelds [7, 33, 38] encode scene geometry implicitly in the network weights and lack an explicit inductive bias for effective distribution modeling. Tri-planes [6, 14, 52] can represent objects at high resolutions with reduced memory footprint, but fundamentally lack a geometric bias except for large axis-aligned planes, posing challenges when modelling larger scenes with complex geometry. Comparably, voxel grids [21, 51, 66] are ﬂexible, expressive for both chunky and thin structures, and support fast querying and processing. Sparse voxel hierarchies do not store voxel information for empty regions and hence are more efﬁcient. A popular approach in the literature is to implement these using octree variants [22, 60, 61, 64] or hash tables [58]. However, previous works either focus only on geometry [61, 71], are limited to an effective resolution of 2563 [22], do not consider hierarchical generation [72], or are not evaluated on real-world datasets [27]. In contrast, our method can generate high-resolution shapes from a hierarchical latent space, and is evaluated on large-scale, real-world scene datasets.",
        "Input Hierarchical Voxel Latent Diffusion VAE Decoder Generated Hierarchy Mesh & Texture Sparse Structure VAE Next Level … … Diffusion Sampling Process VAE Training Figure 2. Method. Sparse voxel grids within the hierarchy are ﬁrst encoded into compact latent representations using a sparse structure VAE. The hierarchical latent diffusion model then learns to generate each level of the latent representation conditioned on the coarser level in a cascaded fashion. The generated high-resolution voxel grids contain various attributes for different applications. Note that technically X1 is a dense latent grid, but illustrated as a sparse one for clarity."
      ]
    },
    {
      "section": "Method",
      "chunks": [
        "Our goal is to learn a generative model of large-scale 3D scenes represented as sparse voxel hierarchies. The hierarchy comprises of L levels of coarse-to-ﬁne voxel grids G = {G1, ..., GL} and their associated per-voxel attributes A = {A1, ..., AL} such as normals and semantics. Finer grids Gl+1 with smaller voxel sizes are strictly contained within the coarser grids Gl for l = 1, ..., L −1, and the ﬁnest level of grid GL contains the maximum amount of details. Our method trains a hierarchy of latent diffusion models over the sparse voxel grids G encoded by a hierarchy of sparse structure VAEs, as summarized in Fig. 2. We ﬁrst introduce the sparse structure variational autoencoder (VAE) that learns a compact latent representation of voxel grids in § 3.1. Then we describe our full diffusion probabilistic model that learns the joint distribution of the latent representation and the sparse voxel hierarchy in § 3.2. The training and sampling procedures are described in § 3.3, followed by the implementation details in § 3.4. 3.1. Sparse Structure VAE Motivation. The sparse structure VAE is designed to learn a compact latent representation of each voxel grid within the hierarchy and its associated attributes. Instead of directly modeling their joint distribution that comprises a mixture of continuous and discrete random variables, we encode them into a uniﬁed continuous latent representation, which is not only friendly to the downstream diffusion models during training and sampling [62, 69], but also facilitates the formulation of a hierarchical probabilistic model which we aim to demonstrate. Additionally, the latent representation, encoded in a coarser spatial resolution, serves as a compact yet meaningful proxy that saves the computation while preserving the expressivity [17, 48]. Represented as X = {X1, ..., XL}, the latent is also a featurized sparse voxel hierarchy correSubdivide Prune Subdivide Prune … Figure 3. VAE Decoder Architecture. Coarser levels of grids Gl are upsampled to ﬁner grids Gl+1 by iteratively subdividing existing voxels into octants and pruning excessive ones. Each level may contain many upsampling layers that double the resolution. sponding to a coarser version of G, with the voxel size of Xl being the same as Gl−1. Network Design. We choose to train separate VAEs that operate on each level l of the hierarchy independently. Hence for the ease of notation, we drop the subscript l in the following discussion. Here, we build the neural networks based on the operators over sparse voxel grids to model both the posterior distribution qφ(X|G, A) and the likelihood distribution pψ(G, A|X), with φ, ψ being the encoder and decoder weights respectively. For the encoder, we utilize the sparse convolutional neural network to process the input G and A by alternatively applying sparse convolution and max pooling operations, downsampling to the resolution of X. For the decoder, we borrow the structure prediction backbone from [20] that allows us to predict novel sparse voxels that are not present in the input. It starts from X and proceeds by progressively pruning excessive voxels and subdividing existing ones based on the prediction of a subdivision mask, and ﬁnally reaching the resolution of G after several upsampling layers. An illustration of the above decoding scheme is shown in Fig. 3. 3.2. Hierarchical Voxel Latent Diffusion Probabilistic Modeling. Existing 3D generation literature [17, 38] typically uses one level of latent diffusion (i.e. L = 1). While this is sufﬁcient to generate intricate scenes containing one single object, the resolution is still far from",
        "enough to generate large-scale outdoor scenes. The limited scalability of their underlying 3D representation and the absence of probabilistic modeling to capture the coarse-to-ﬁne nature of the data hinder the effectiveness of these methods. We solve this by marrying a hierarchical latent diffusion model [3, 19] with the sparse voxel representation. Specifically, we propose the following factorization of the joint distribution of grids and latents: p(G, A, X) = L Y l=1 pψl(Gl, Al|Xl)pθl(Xl|Cl−1), (1) where Cl−1 is the condition from the coarser level, with: Cl = ( c, l = 0 {Gl, Al, c}, l > 0 , (2) with c being an optional global condition such as a category label or a text prompt, and pθl(·) instantiated as a diffusion model with parameter θl which we elaborate on later. The above factorization assumes the Markov process (i.e. level l is only conditioned on its coarser level l −1), which is naturally induced from the geometric nature of the data. By doing so, we reduce the layers in each level of VAE and amortize both the computation and the representation power across multiple levels (see § 4.4 for empirical proof). Additionally, such a factorized modeling endows us with utmost ﬂexibility, enabling user controls by editing or resampling grids from different levels. Diffusion Model pθ. Here we omit subscript l again for clarity. A diffusion stochastic process transforms a complicated distribution of a random variable into the unit Gaussian distribution X0 ∼N(0, I) by iteratively adding white noise to it, following a Markov process [18]. One commonly used instantiation is the following: Xt|Xt−1 ∼N( p 1 −βtXt−1, βtI), (3) where 0 < βt ≪1 controls the amount of noise added for each step. The reverse process, on the other hand, removes the noise iteratively and reaches data distribution XT within a discrete number of steps T. It is usually modeled as: Xt−1|Xt ∼N(µθ(Xt, t), 1 −¯αt−1 1 −¯αt βtI), (4) with αt = 1 −βt, ¯αt = Qt s=0 αs, and µθ a parametrized learnable module. In practice, we re-parametrize µθ as: µθ = √αtXt −βt r ¯αt−1 1 −¯αt vθ, (5) so that the learnable module predicts v instead. This is in accordance with the v-parameterization in [49] that has been shown to facilitate training. We instantiate vθ(·) as a 3D sparse variant of the backbone used in [13], ensuring the grid structure of the decoded output from vθ matches the input. To inject condition Cl−1, we directly concatenate the feature from Al−1 with the network input recalling that Xl also shares the same grid structure with Gl−1. Timestep condition is implemented using AdaGN [48] and textual condition c is ﬁrst CLIPencoded [44] and then injected using cross attention. 3.3. Training and Sampling Loss Functions. We train the VAE and the diffusion model level-by-level independently. During the training of the levell VAE, we employ the following loss function: LVAE l = E{Gl,Al}[EXl∼qφ[BCE(Gl, ˜Gl)+ LAttr l (Al, ˜Al)] + λ KL(qφ(Xl) ∥p(Xl))], (6) where ˜Gl, ˜Al is the output of the VAE decoder ψ given Xl, and LAttr l is the loss supervising the attribute predictions (e.g., TSDF, semantics, etc.) with its speciﬁc form postponed to the supplementary material. BCE(·) is the binary cross entropy on the grid structure, making pψ a mixed product distribution. KL(· ∥·) is the KL divergence between the posterior and the prior p(Xl), which we set to unit Gaussian N(0, I), and λ is its weight. The training loss for the diffusion model is: LDM l = Et,Xl,ε∼N (0,I) \u0002 ∥vθl(Xl,t, t) −vref∥2 \u0003 , (7) where vref = √¯αtε−√1 −¯αtXl, t ∼[1, T], Xl is sampled from the VAE posterior, and Xl,t = √¯αtXl + √1 −¯αtε. Sampling. To sample from the joint distribution of Eq (1), one starts by drawing the coarest latent X1 from the diffusion model pθ1. Then, the decoder pψ1 is used to generate the coarsest grid G1 and its associated attributes A1 (which is then optionally reﬁned by the reﬁnement network). Conditioned on C1 = {G1, A1, c}, the diffusion model pθ2 is used to generate the next level of latent X2, and the process goes on until the highest resolution of {GL, AL} is met. We include TSDF in AL for all our experiments, which enables us to decode high-resolution meshes. For other tasks such as perception, we further allow for decoding other attributes such as semantics. We use DDIM [54] as our sampler. 3.4. Implementation Details In practice, we ﬁnd the following implementation details, specially tuned for the sparse voxel hierarchy generation case, to be helpful for better results: (1) Early dilation. In network layers with larger voxel sizes, we dilate the sparse voxel grids by one, so that the halo regions of the sparse topology also represent non-zero features. This helps later layers to better capture the local context and generate smooth structures. (2) Reﬁnement network. An inherent problem",
        "LION NFD NWD (Ours) Figure 4. ShapeNet [5] Qualitative Comparison. We show comparison of our method with LION [69], NFD [52], and NWD [21]. Our method is capable of generating intricate geometry and thin structures. Best viewed with 200% zoom-in. of our factorized modeling is error accumulation, where higher-resolution grids cannot easily ﬁx the artifacts from prior layers. We mitigate this by appending a reﬁnement network to the output of the VAE decoder that reﬁnes Gl and Al. The architecture of the reﬁnement network is similar to [20], and its training data is augmented by adding noise to the posterior of the VAE [19] before being decoded. Last but not least, our architecture and training details can be found in the supplementary. Sparse 3D Learning Framework. The use of sparse voxel grids motivates and enables us to build a custom 3D deep learning framework for sparse data in order to support higher spatial resolution and faster sampling. To this end, we leverage the VDB [36] structure to store our sparse 3D voxel grid. Thanks to its compact representation (taking only 11MB for 3.4 million of voxels) and fast look-up routine, we are able to implement common neural operators such as convolution and pooling in a very efﬁcient manner. Fully operating on the GPU (including grid building), our framework is able to process a 3D scene with 10243 in milliseconds, runs ∼3× faster with ∼0.5× of the memory usage than the current state-ofthe-art sparse 3D learning framework TorchSparse [58]. All our architectures are based on this custom framework."
      ]
    },
    {
      "section": "Experiments",
      "chunks": [
        "We conduct comprehensive experiments to evaluate the performance of our model. First, we demonstrate XCube’s ability to perform unconditional object-level 3D generation using ShapeNet [5] (§ 4.1), and conditional 3D generation from category and text using Objaverse [12] (§ 4.2). Next, we showcase high-resolution outdoor scene-level 3D generation using both the Karton City [1] and Waymo [56] datasets (§ 4.3), which is one of the ﬁrst results of this kind. Finally, we conduct ablation studies for our design choices (§ 4.4). Please refer to the supplementary for further results. Airplane Chair Car CD EMD CD EMD CD EMD Point-based method PVD [73] 69.55 60.89 57.68 54.95 64.89 54.61 LION [69] 65.10 60.15 56.72 54.28 60.61 54.94 Triplane-based method NFD [52] 57.55 53.47 54.87 54.06 69.49 71.96 Dense voxel-based method NWD [21] 59.78 53.84 56.35 57.98 61.75 58.54 LAS-Diffusion [72] 71.29 56.93 55.17 55.02 75.03 72.10 3DShape2VecSet [70] 62.75 61.01 54.06 56.79 86.85 80.91 Ours 52.85 49.75 53.99 48.60 57.96 54.43 Table 1. 1-NNA Comparison on ShapeNet [5]. The lower the better. Best scores highlighted in bold. 4.1. Object-level 3D Generation on ShapeNet Dataset. To benchmark XCube against prior methods, we use the widely-adopted ShapeNet [5] dataset. Following the experimental setup in [34, 52, 68, 69], we choose three speciﬁc categories: Airplane, Car and Chair, containing 4145, 7496, 6778 shapes respectively. To build the groundtruth voxel hierarchy for training, we voxelize each mesh at a 5123 resolution and use the train/val/test split from [10]. Evaluation. To evaluate the geometric quality of our synthesized output, we follow previous work [21, 68, 69] and use 1-NNA as our main metric (with both Chamfer distance (CD) and earth mover’s distance (EMD)). 1-NNA provides a comprehensive measure of both quality and diversity by measuring the distributional similarity between the generated shapes and the validation set [68, 69]. Please refer to the supplementary for more details and evaluations. Baselines. We compare XCube to state-of-the-art 3D generative models that leverage various latent and shape representations: PVD [73], LION [69], NFD [52], NWD [21], LAS-Diffusion [72], and 3DShape2VecSet [70]. PVD and LION employ point clouds as both latent and output-shape",
        "Figure 5. Close-up View of Our Generated Shape. The voxel grid is colored by predicted normal. XCube is able to generate a high level of detail, such as the car interior and airplane propellers. representations. NFD, NWD, and 3DShape2VecSet all use mesh as the output shape representation, but with different latent representations, i.e., triplanes, dense voxel grids, and unstructured latent vectors, respectively. Although LASDiffusion also uses sparse voxels similar to ours, it does not allow for generating arbitrary attributes and scale only to 1283 resolutions. In contrast, our method uses a sparse voxel hierarchy as a latent representation and outputs a sparse voxel grid, which can be readily converted to a mesh. Results. Tab. 1 provides a quantitative comparison of XCube against baseline approaches and shows the superiority of our approach over past work. The point-based methods are naturally restricted to generating coarse shapes (i.e., 2048 points), while our method is able to generate millions of voxels (500× larger). The triplane-based method (NFD) exhibits decent performance in categories such as Airplane and Chair with simple geometry. However, its effectiveness diminishes for the Car category with intricate geometry (such as the suspension), underscoring the challenges to generate complex geometric structures using such representations. 3DShape2VecSet suffers from a similar issue where its latent representation compresses the information too aggressively. While NWD and LAS-Diffusion are based on voxel grids, they are foundamentally limited by their low grid resolutions. The inverse wavelet transform in NWD is also lossy. In contrast, our method is able to generate high-resolution shapes with ﬁne details, as shown in Fig. 5. Fig. 4 shows a qualitative comparison, which is consistent with our quantitative results. User-guided Editing. Our method is based on a sparse voxel hierarchy, which is a natural representation for userguided editing. We demonstrate this ability by allowing users to edit the coarse-level shapes by adding or removing voxels in Goxel [9], a Minecraft-style 3D voxel editor. Fig. 6 shows several examples of user-guided editing. 4.2. Object-level 3D Generation on Objaverse Dataset. To further demonstrate XCube’s ability to perform object-level 3D generation, we evaluate it using the Figure 6. User-guided Editing. By adding (green) and deleting (red) coarse-level voxels, one can easily control the ﬁner 3D shape. Objaverse [12] dataset, which offers approximately 800K publicly available 3D models. For the text-to-3D experiment, we use the descriptive text automatically generated by Cap3D [35]. For category-conditional generation, we adopt the LVIS categorized object subset of [12], containing ∼40K 3D objects. We voxelize the 3D objects at a 5123 resolution to build the ground truth voxel hierarchy for training. Evaluation. We conduct a user study on Amazon Mturk to compare our method with Shap·E [23], a state-of-the-art text-to-3D method. Speciﬁcally, we gather 30 prompts from popular text-to-3D works [23, 40, 41, 59]. For each prompt, we ask 30 users to pick the 3D object (ours v.s. Shap·E w/o texture) that better matches the text prompt and exhibits higher geometric ﬁdelity. In total, we collect 900 pairwise comparisons. In 79.2% of the comparisons, participants vote for our generated 3D objects. Results. We provide qualitative results for text-to-3D in Fig. 7, and category-conditional generation in Fig. 9. The texture of our results is generated by off-the-shelf texture synthesis methods [4, 47]. We also compare our method with Shap·E [23] in Fig. 8. Our whole pipeline including generating the geometry and the texture for one object takes about 1 minute (30s for objects and 30s for textures). We observe that our method is able to generate more diverse 3D objects with higher geometric ﬁdelity and ﬁner details than Shap·E, as shown in Fig. 9. 4.3. Large-scale Scene-level 3D Generation Dataset. To demonstrate our model’s scalability and ability to generate large-scale high-resolution scenes, we train and evaluate it on the Waymo Open Dataset [56] which contains 1000 LiDAR scan sequences capturing different driving scenarios. Here, we extract the ground-truth dense scene geometry by accumulating the LiDAR scans and propagating the semantic labels. To construct the ground-truth sparse voxel hierarchy, we crop each of the extracted scenes to 102.4 m× 102.4 m chunks and voxelize the points and meshes at a resolution of 10243, resulting in a voxel size of",
        "“A 3D model of pineapple” “A 3D model of squirrel” “A 3D model of skull” “An eagle head” Figure 7. Text-to-3D Results on Objaverse [12]. For each sample we show the input text prompt, the generated sparse voxel grid colored by normal, the extracted mesh, and the textured mesh (using [47]). “A pair of shorts” “A cactus” (Ours) “A strong muscular man” Shap!E Figure 8. Comparison with Shap·E [23]. We can generate highquality shapes that better match the given prompts. 10 cm. To demonstrate the superiority of our representation power, we train a model on Karton City [1], a custom synthetic dataset of 20 blocks of synthetic city scenes using the same resolution settings as Waymo. We divide the 20 blocks into train/val splits and randomly crop 102.4 m× 102.4 m chunks in each split to generate 900/100 unique train/val scenes. Fig. 12 shows examples from the Karton City model and we include more results in the supplementary. Evaluation. To evaluate the quality of our generated results, we perform a user study using Amazon Mturk. Here we show 30 users 30 pairs of scenes (totaling 900 pairwise comparisons) where one scene is sampled from the validation set and the other is a random output generated by our model. We ask each user to rank which scene is more realistic. Out of the 900 comparisons, 66.3% favors our results over the ground truth, demonstrating that our generated outputs are of high quality. Fig. 10 shows several unconditional generations from our model as well as decoded semantics and normals. Single-Scan Conditioning. We demonstrate that our model <Dog> “A chair that looks like a root” <Lizard> Figure 9. Diversity of Our Generated Shapes. XCube can generate diverse shapes under the same text prompt or category label. can be used to perform conditional generation on large-scale scenes. In this qualitative experiment, we condition the model on a single input LiDAR scan and generate a complete scene with normals and semantics. Fig. 11 shows several completions using our method. Note that our input does not contain semantics, yet our model is able to generate plausible geometric and semantic completion results. The supplementary shows additional details and ﬁgures. 4.4. Ablation Study Progressive Pruning. We replace the progressive pruning part of our pipeline with a single pruning step for the 163 →1283 VAE on ShapeNet Chairs. We observe that the",
        "Curb & Lane Marker Vehicles Road Sidewalk Vegetation & Trees Building Sign & Pole Figure 10. Unconditional Samples on the Waymo Open Dataset [56]. The dashed boxes show a zoomed-in view and the solid boxes show the normal map for the extracted mesh. Best viewed with 200% zoom-in. Figure 11. Single-scan-conditioned Generation. The left column shows the input LiDAR scan and the right column shows our generated semantic mesh conditioned on the input. reconstruction accuracy (grid IoU) drops from 92.88% to 89.68%, indicating that progressive pruning is critical for preserving shape details and injecting 3D inductive bias. Furthermore, the GPU memory usage also increases by a factor of 3× when removing progressive pruning. Hierarchy Conﬁguration. As shown in Tab. 2, we compare the performance of our model with different hierarchy conﬁgurations on ShapeNet Chairs. We observe that: (1) the hierarchical model outperforms the single-level model, emphasizing the importance of a sparse voxel hierarchy in 3D generative modeling. (2) the model’s performance is robust to the resolution of the initial hierarchy level. We ﬁnd 163 is sufﬁcient for capturing the overall shape of the object. (3) using two-level and three-level models achieve compaModel CD (%) EMD (%) 163 →5123 59.31 57.46 163 →1283 →5123 53.99 48.60 323 →1283 →5123 55.39 51.40 43 →163 →1283 →5123 52.88 53.62 Table 2. Ablation of Different Resolutions and Depths of the Hierarchy. Metrics are in 1-NNA. The lower the better. Figure 12. Unconditional Samples on Karton City [1]. rable performance. For unconditional generation, we use a two-level model for fast sampling. And for the user-editing setting, we use a three-level model for easier editing."
      ]
    },
    {
      "section": "Limitations",
      "chunks": [
        "Limitations and Future Work. Due to current 3D datasets being still not on par with image datasets (such as [50]) , our text-to-3D model is hard to deal with complex prompts. In the future, we plan to extend our method in the setting of image-conditioning, as well as leveraging the learned prior as a fundamental model to support more downstream tasks."
      ]
    },
    {
      "section": "Discussion",
      "chunks": [
        "Conclusion. We presented X 3, a novel generative model for large-scale 3D scenes represented as a hierarchy of sparse 3D voxel grids. We proposed a hierarchical voxel latent diffusion model that learns the joint distribution of the latent representation and the sparse voxel hierarchy. The effectiveness of our method was demonstrated on both object-level and scene-level generation, reﬂecting our method’s capability of generating high-resolution 3D scenes with ﬁne details. "
      ]
    },
    {
      "section": "References",
      "chunks": [
        "[1] 3d karton city model. https://www.turbosquid. com/3d- models/3d- karton- city- 2- model1196110, 2023. Accessed: 2023-08-01. 2, 5, 7, 8 [2] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas J. Guibas. Learning representations and generative models for 3d point clouds. In International Conference on Machine Learning (ICML), pages 40–49, 2018. 2 [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 22563–22575, 2023. 2, 4 [4] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, and Kangxue Yin. Texfusion: Synthesizing 3d textures with text-guided image diffusion models. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023. 6 [5] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An informationrich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 1, 2, 5 [6] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance ﬁelds. In European Conference on Computer Vision (ECCV), pages 333–350, 2022. 2 [7] Zhiqin Chen and Hao Zhang. Learning implicit ﬁelds for generative shape modeling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5939–5948, 2019. 2 [8] Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Scenedreamer: Unbounded 3d scene generation from 2d image collections. arXiv preprint arXiv:2302.01330, 2023. 2 [9] Guillaume Chereau. Goxel: 3d voxel editor, 2023. Accessed: 2023-11-16. 6 [10] Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A uniﬁed approach for single and multi-view 3d object reconstruction. In European Conference on Computer Vision (ECCV), pages 628–644, 2016. 5 [11] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023. 2 [12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 13142–13153, 2023. 1, 2, 5, 6, 7 [13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, pages 8780–8794, 2021. 4 [14] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned from images. In Advances in Neural Information Processing Systems, 2022. 2 [15] Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, YuKun Lai, and Hao Zhang. SDM-NET: deep generative network for structured deformable mesh. ACM Transactions on Graphics (TOG), 38(6):243:1–243:15, 2019. 2 [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2 [17] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas O˘guz. 3dgen: Triplane latent diffusion for textured mesh generation. arXiv preprint arXiv:2303.05371, 2023. 1, [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020. 2, 4 [19] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high ﬁdelity image generation. Journal of Machine Learning Research, 2022. 4, 5 [20] Jiahui Huang, Zan Gojcic, Matan Atzmon, Or Litany, Sanja Fidler, and Francis Williams. Neural kernel surface reconstruction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4369– 4379, 2023. 3, 5 [21] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural wavelet-domain diffusion for 3d shape generation. In SIGGRAPH Asia, 2022. 1, 2, 5 [22] Moritz Ibing, Gregor Kobsik, and Leif Kobbelt. Octree transformer: Autoregressive 3d shape generation on hierarchically structured sequences. arXiv preprint arXiv:2111.12480, 2021. [23] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023. 2, 6, 7 [24] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio Torralba, and Sanja Fidler. Neuralﬁeld-ldm: Scene generation with hierarchical latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 8496–8506, 2023. 2 [25] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 2 [26] Jumin Lee, Woobin Im, Sebin Lee, and Sung-Eui Yoon. Diffusion probabilistic models for scene-scale 3d categorical data. arXiv preprint arXiv:2301.00527, 2023. 1 [27] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusionsdf: Text-to-shape via voxelized diffusion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 12642–12651, 2023. 2 [28] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, MingYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to3d content creation. In Proceedings of the IEEE Conference",
        "on Computer Vision and Pattern Recognition (CVPR), pages 300–309, 2023. 2 [29] Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei Chai, Aliaksandr Siarohin, Ming-Hsuan Yang, and Sergey Tulyakov. Inﬁnicity: Inﬁnite-scale city synthesis. arXiv preprint arXiv:2301.09637, 2023. 2 [30] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885, 2023. 1 [31] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. arXiv preprint arXiv:2306.16928, 2023. 2 [32] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. arXiv preprint arXiv:2303.11328, 2023. 1, 2 [33] Andrew Luo, Tianqin Li, Wen-Hao Zhang, and Tai Sing Lee. Surfgen: Adversarial 3d shape synthesis with explicit surface discriminators. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 16218–16228, 2021. 2 [34] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2837–2845, 2021. 2, 5 [35] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. In Advances in Neural Information Processing Systems, 2023. 6 [36] Ken Museth. VDB: high-resolution sparse volumes with dynamic topology. ACM Transactions on Graphics (TOG), 32(3):27:1–27:22, 2013. 2, 5 [37] George Kiyohiro Nakayama, Mikaela Angelina Uy, Jiahui Huang, Shi-Min Hu, Ke Li, and Leonidas Guibas. Difffacto: Controllable part-based 3d point cloud generation with cross diffusion. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023. 2 [38] Gimin Nam, Mariem Khliﬁ, Andrew Rodriguez, Alberto Tono, Linqi Zhou, and Paul Guerrero. 3d-ldm: Neural implicit 3d shape generation with latent diffusion models. arXiv preprint arXiv:2212.00842, 2022. 2, 3 [39] Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, and Peter W. Battaglia. Polygen: An autoregressive generative model of 3d meshes. In International Conference on Machine Learning (ICML), pages 7220–7229, 2020. 2 [40] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 2, 6 [41] Evangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski, Chaoyang Wang, Luc Van Gool, and Sergey Tulyakov. Autodecoding latent 3d diffusion models. In Advances in Neural Information Processing Systems, 2023. 2, 6 [42] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 2 [43] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), pages 8748–8763, 2021. 4 [45] Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, Alejandro Newell, Hei Law, Ankit Goyal, Kaiyu Yang, and Jia Deng. Inﬁnite photorealistic worlds using procedural generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 12630–12641, 2023. 2 [46] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In International Conference on Machine Learning (ICML), pages 1530–1538, 2015. 2 [47] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In SIGGRAPH, 2023. 6, 7 [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 10674–10685, 2022. 2, 3, 4 [49] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. 4 [50] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION5B: an open large-scale dataset for training next generation image-text models. In Advances in Neural Information Processing Systems, 2022. 8 [51] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar Averbuch-Elor. Vox-e: Text-guided voxel editing of 3d objects. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023. 2 [52] J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d neural ﬁeld generation using triplane diffusion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 20875–20886, 2023. 2, 5 [53] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning (ICML), pages 2256–2265, 2015. [54] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 4 [55] Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould. 3d-gpt: Procedural 3d modeling",
        "with large language models. arXiv preprint arXiv:2310.12945, 2023. 2 [56] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2443–2451, 2020. 2, 5, 6, 8 [57] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Viewset diffusion:(0-) image-conditioned 3d generative models from 2d data. arXiv preprint arXiv:2306.07881, 2023. 2 [58] Haotian Tang, Zhijian Liu, Xiuyu Li, Yujun Lin, and Song Han. Torchsparse: Efﬁcient point cloud inference engine. MLSys, 2022. 2, 5 [59] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efﬁcient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. 6 [60] Jia-Heng Tang, Weikai Chen, Jie Yang, Bo Wang, Songrun Liu, Bo Yang, and Lin Gao. Octﬁeld: Hierarchical implicit functions for 3d modeling. arXiv preprint arXiv:2111.01067, 2021. 2 [61] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox. Octree generating networks: Efﬁcient convolutional architectures for high-resolution 3d outputs. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2107–2115, 2017. 2 [62] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In Advances in Neural Information Processing Systems, pages 11287–11302, 2021. 2, 3 [63] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems, pages 4790–4798, 2016. 2 [64] Peng-Shuai Wang, Yang Liu, and Xin Tong. Dual octree graph networks for learning adaptive volumetric shape representations. ACM Transactions on Graphics (TOG), 41(4): 1–15, 2022. 2 [65] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Proliﬁcdreamer: High-ﬁdelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023. 1, 2 [66] Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Freeman, and Joshua B. Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. In Advances in Neural Information Processing Systems, pages 82–90, 2016. 2 [67] Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. Citydreamer: Compositional generative model of unbounded 3d cities. arXiv preprint arXiv:2309.00610, 2023. 2 [68] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge J. Belongie, and Bharath Hariharan. Pointﬂow: 3d point cloud generation with continuous normalizing ﬂows. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4540–4549, 2019. 5 [69] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. LION: latent point diffusion models for 3d shape generation. In Advances in Neural Information Processing Systems, 2022. 2, 3, 5 [70] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape representation for neural ﬁelds and generative diffusion models. ACM Transactions on Graphics (TOG), 42(4):92:1–92:16, 2023. 5 [71] Dongsu Zhang, Changwoon Choi, Jeonghwan Kim, and Young Min Kim. Learning to generate 3d shapes with generative cellular automata. In ICLR, 2021. 2 [72] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, and Heung-Yeung Shum. Locally attentional SDF diffusion for controllable 3d shape generation. ACM Transactions on Graphics (TOG), 42(4):91:1–91:13, 2023. 2, 5 [73] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 5806–5815, 2021. 5"
      ]
    }
  ]
}