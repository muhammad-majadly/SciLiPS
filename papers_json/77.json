{
  "paper_id": "77",
  "paper_title": "77",
  "sections": [
    {
      "section": "FrontMatter",
      "chunks": [
        "Towards Realistic Example-based Modeling via 3D Gaussian Stitching Xinyu Gao1∗ Ziyi Yang1∗ Bingchen Gong2∗ Xiaoguang Han3† Sipeng Yang1 Xiaogang Jin1† 1Zhejiang University 2The Chinese University of Hong Kong 3The Chinese University of Hong Kong (Shenzhen) Figure 1. Our method can seamlessly stitch multiple 3D Gaussian ﬁelds together interactively, resulting in new, highly detailed, and realistic objects. All of the geometric parts or models are derived from the BlendedMVS [59] and Mip360 [2] datasets. Project page: https://ingra14m.github.io/gs_stitching_website/."
      ]
    },
    {
      "section": "Abstract",
      "chunks": [
        "Using parts of existing models to rebuild new models, commonly termed as example-based modeling, is a classical methodology in the realm of computer graphics. Previous works mostly focus on shape composition, making them very hard to use for realistic composition of 3D objects captured from real-world scenes. This leads to combining multiple NeRFs into a single 3D scene to achieve seamless appearance blending. However, the current SeamlessNeRF method struggles to achieve interactive editing and harmonious stitching for real-world scenes due to its gradientbased strategy and grid-based representation. To this end, we present an example-based modeling method that com- * denotes equal contribution. † denotes the corresponding authors. bines multiple Gaussian ﬁelds in a point-based representation using sample-guided synthesis. Speciﬁcally, as for composition, we create a GUI to segment and transform multiple ﬁelds in real time, easily obtaining a semantically meaningful composition of models represented by 3D Gaussian Splatting (3DGS). For texture blending, due to the discrete and irregular nature of 3DGS, straightforwardly applying gradient propagation as SeamlssNeRF is not supported. Thus, a novel sampling-based cloning method is proposed to harmonize the blending while preserving the original rich texture and content. Our workﬂow consists of three steps: 1) real-time segmentation and transformation of 3DGS using a well-tailored GUI, 2) KNN analysis to identify boundary points in the intersecting area between the source and target models, and 3) two-phase optimization of the target model using sampling-based cloning and This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.",
        "gradient constraints. Extensive experimental results validate that our approach signiﬁcantly outperforms previous works in realistic synthesis, demonstrating its practicality."
      ]
    },
    {
      "section": "Introduction",
      "chunks": [
        "As we all know, 3D scenes typically contain multiple 3D objects composed of various parts. Example-based modeling [11] is a technique that involves combining different parts from different objects to create new ones. This is a common tool in Computer Graphics (CG) modeling, where objects are designed in a non-realistic CG fashion. In this paper, we consider realistic example-based modeling, where all parts are captured from the real world, as shown in Fig. 1. This task becomes prominent with the emergence of Neural Radiance Fields, which enables photorealistic 3D reconstruction and rendering. Among the various approaches designed for 3D modeling from multiple neural ﬁelds, a portion of the research [12, 32] is devoted to the inverse rendering process to achieve consistent lighting and shadowing. But these methods rarely consider a situation where the harmonious and seamless effect is required for merging or unifying two or more neural ﬁelds. SeamlessNeRF [14] is the ﬁrst work to tackle seamless merging, attempting to address the consistency problem by propagating gradients on synthesis cases. Nonetheless, due to its implicit grid-based representation, SeamlessNeRF can neither achieve ﬁne-grained editing (e.g. the face in the Santa case in Fig. 2) under real-world cases nor provide an interactive workﬂow in real-time. Additionally, its gradient-based strategy can produce signiﬁcant artifacts (see Fig. 9) and fails to propagate structural characteristics when the condition becomes more complex (e.g., the bottle in the left-upper corner in Fig. 1). Therefore, achieving a harmonious and photorealistic stitching result on real-world data remains an unsolved challenge that needs further exploration. To address the limitations mentioned above, we propose a new method for interactive editing and stitching multiple parts using explicit shape representation in 3D Gaussian Splatting. Our method has two signiﬁcant advantages. First, its point-based representation enables ﬁne-grained editing, allowing for detailed appearance optimization and the removal of artifacts. Second, its rasterizer pipeline provides a real-time interactive editing environment. Due to the discrete and irregular nature of 3D-GS, it is not feasible to conduct gradient propagation as SeamlessNeRF. Thus, we introduce a novel sampling-based optimization strategy that can seamlessly propagate not only color tones but also structural characteristics. Our evaluation benchmarks are primarily derived from real-world scenes, demonstrating our superior ability to handle complex cases. More speciﬁcally, our pipeline takes multiple scenes as input, containing source and target objects represented by 3DGS. We then carefully segment these objects and apply rigid transformations in order to create a semantically meaningful composite in 3D space. An intersection boundary region between the objects is also identiﬁed before blending. The next is the key step in our process which aims to optimize the appearance of the target objects so that their texture and color match those of the source object. We achieve this by using a two-phase optimization scheme: the ﬁrst phase involves sampling-based cloning (S-phase), and the second phase involves clustering-based tuning (T-phase). During the S-phase, the target ﬁeld is optimized using a heuristic sampling strategy that considers the structural characteristics at the boundary. Additionally, an efﬁcient 2D gradient constraint is applied to preserve the original texture content of the target ﬁeld. However, optimizing solely with S-phase may lead to the appearance of artifacts or unintended color features that do not ﬁt with the overall composite. Therefore, we address this issue with T-phase, where we utilize a pre-calculated feature palette derived from the source ﬁeld through aggregation and clustering. Subsequently, this palette is applied to tune the target ﬁeld. It is important to note that the two-phase optimization is a joint procedure, where losses from the S-phase are always maintained while losses from the T-phase are added later during optimization. In summary, our method makes the following contributions: • The ﬁrst work to use 3D-GS for realistic and seamless part compositing, enabling real-world example-based modeling. • A novel sampling-based optimization strategy is proposed, with which not only the texture color but also the structural characteristics can be propagated seamlessly. • A user-friendly GUI is carefully designed to support an interactive workﬂow of the modeling process in real time."
      ]
    },
    {
      "section": "Related Work",
      "chunks": [
        "2.1. Example-based Seamless Editing Seamless editing, particularly in the context of examplebased image and texture synthesis, is a well-studied editing technique in computer graphics and image processing. As for textures, example-based texture synthesis [8, 48] intends to seamlessly create textures at any size from exemplars, which has been widely employed in contemporary graphics pipelines and game engines. In 2D image synthesis, patchbased synthesis techniques have been widely researched to seamlessly combine visually inconsistent images [6, 38]. Meanwhile, Kwatra et al. [25] introduced “Texture Optimization,” which transfers photographic textures to a target image for example-based synthesis. To facilitate structural image editing tasks, “PatchMatch” [1] found approximate"
        ]
    },
    {
      "section": "metho",
      "chunks": [
        "Reconstruct Gaussian Scenes Segmentation & Transformation Local Space & Global Space & Loss Pre-computed ෡׏௫ Sampling Camera Palette Refering Color Loss (ࣦ௖௢௟௢௥ା௙௘௔Ǥ) ࣦ௧௨௡௘ ࣦ௚௥௔ௗǤ Optimizing Created 3D Models ~ 4 mins Sampling Camera ׏௫ Sampling Camera Sampling Camera ׏ ing Camera Refer ߲ܤ Figure 2. Overview of our framework. Our novel pipeline provides an interactive editing experience and has real-time previewing capabilities to visualize the optimizing process, allowing for the seamless and interactive combination of multiple Gaussian ﬁelds. nearest-neighbor correspondences between patches in images for seamless image region reshufﬂing. In terms of seamless editing in 3D objects, Rocchini et al. [41] and Dessein et al. [7] propose methods for stitching and blending textures on 3D objects, respectively, while Yu et al. [61] use the Poisson equation to implicitly modify the original mesh geometry via gradient ﬁeld manipulation. Additionally, example-based modeling can also generate novel models from parts of existing models [11], allowing untrained users to create interesting and detailed 3D designs, such as city building [34], things arrangements [9], mesh segmentation [19], and merging [23]. Recently, deep learning methods have leveraged generative models to generate diverse instances from a single exemplar [27, 51] or a cluster of examples [64]. Deﬁnitely, the example-based methodology is a valuable tool for creating diverse and novel content, which can reduce the workload for the artists or can be leveraged by procedural content generation programs. In our work, we combine this valuable idea with the advanced technique of 3DGS to create content directly from the real world. 2.2. Neural Scene Composition Neural scene composition primarily involves the synthesis of multiple neural objects represented by neural ﬁelds, such as free-viewport video [30, 47, 63], autonomous driving [10, 24, 37, 44, 55, 65] and scene understanding [15, 21, 42, 50, 52, 54, 56, 62]. And for those composition tasks with multiple pre-trained models, mesh scaffold [33, 53, 60] or texture extraction [5, 46] from the neural ﬁeld are preferred to achieve higher render speed or rather ﬁne-grained control. This type of work acts as a “bridge” between neural and traditional representations in order to improve performance using the classical graphics pipeline. A small portion of the work focuses on creating a mixed render pipeline for neural 3D scene composition tasks, combining traditional render techniques like ray tracing [40], shadow mapping [13], and ambient occlusion [12]. There are also a few works that focus on creating a compositional scene with generative models like diffusion models [39]. None of those works except Neural Imposter [31] and SeamlessNeRF [14] focus on example-based modeling by stitching multiple part NeRFs. However, part objects in Neural Imposter are just placed together without any appearance blending, which cannot support a general case of 3D modeling. SeamlessNeRF achieved harmonious results on a small-scale synthesis dataset, making it the ﬁrst work to discuss seamless example-based modeling with neural techniques today. However, SeamlessNeRF cannot handle real-world cases when the condition becomes more complex, nor can it perform interactive editing, which is commonly required in example-based modeling. On the contrary, our approach overcomes these limitations, performs well in real-world scenarios, and supports interactive editing using Gaussian ﬁelds. 2.3. 3D Gaussians 3D Gaussian Splatting [20] is a point-based rendering method that has recently gained popularity [4, 16, 28, 29, 45, 57, 58] due to its realistic rendering and signiﬁcantly faster training time than NeRFs. Compared to the implicit representation of NeRF, 3DGS is more advantageous for editing tasks. The superior advance lies in the fact that, unlike previous work that embedded an object in a certain neural ﬁeld (e.g., learnable grid or MLP network), once clus-",
        "(c) (a) (b) Figure 3. For a Gaussian point in the target ﬁeld, its (a) K-nearest neighbors in the source ﬁeld can be leveraged to justify whether this point belongs to the intersection boundary region. We use the boundary of (b) as an example to demonstrate the effectiveness of this strategy, as shown in (c). ters of Gaussians are optimized, they can be easily fused together and fed into the rasterizer. The 3DGS pipeline was born with an intrinsic property suitable for composition. 3. Seamless Gaussians Our approach starts with segmenting interesting parts from pre-trained Gaussian scenes. After acquiring target and source models represented by Gaussians, we carefully transform them to obtain a semantically meaningful composite. Then we optimize the target objects to achieve a harmonious composite through a two-phase (sampling-based cloning and clustering-based tuning) scheme. All these processes can be run interactively and previewed in real-time with our well-tailored GUI. 3.1. Segmenting and Transforming Gaussians Segmentation is the ﬁrst step in example-based modeling, which involves picking out interesting parts as the components of the ﬁnal artwork. Previous works have performed this task by providing guidance using 2D mask [3, 35] or injecting semantic label [21] into a neural ﬁeld. Now, beneﬁting from Gaussian representation (resembling point cloud), segmentation can become more practical at a ﬁner-grained level. In our pipeline, we show that a combination of a simple bounding box and a user brush can work very well for a clean mask (see Fig. 11). For instance, we can mask the sculpture with a brush to match the shape of Santa’s face (see Fig. 2). Transformation aims at placing multiple interesting parts Gi represented by Gaussians to form a semantically meaningful composite M, which can be denoted as: Gglobal i = F(Glocal i |ˆqi, ti, si), Gi ∈M (1) where F is the rigid transformation applied on one part of Gaussians with rotation ˆqi (represented in quaternion), translation ti, and scale si, transforming the part from its local space to the global space. Speciﬁcally, the partial attributes of each G should be modiﬁed, which includes position x, scaling s, rotation q (in quaternion), and feature f (represented as spherical harmonics). The position and Figure 4. Ablation study on the color loss in the S-phase. Without color loss, the propagation is inefﬁcient and will not begin. The cases shown above have been running for more than twice as long, but they are still trapped in insufﬁcient propagation. It is because, without color loss, only a small number of points’ features need to be updated at ﬁrst, as opposed to shared weights in an MLP applied to all points. That minor “forces” cannot drive the overall minimization of the gradient loss. scaling can be performed trivially, while the transformed rotation q′ and feature f ′ can be expressed as: q′ = qˆq, f ′ = Mbands(f | ˆq), (2) where Mbands means we use a set of matrices to rotate each band of SH coefﬁcients introduced by [18]. 3.2. Boundary Condition by KNN Analyzing After transformation, certain points in one ﬁeld approach another ﬁeld (see Fig. 3), forming intersection boundary regions between all Gaussians. For the sake of simplicity, we will use two Gaussians, source ﬁeld and target ﬁeld, to demonstrate our approach. Before optimization, the boundary points in the target ﬁeld must be identiﬁed, as this is the critical and initial condition for harmonization. For each Gaussian point in target ﬁeld T , we search its K-nearest neighbors in source ﬁeld S, which can be denoted by: {bi}K = KNN S (a), a ∈T , bi ∈S (3) where a is a point in the target ﬁeld, and bi is a point in the source ﬁeld. Whether a point a belongs to boundary ∂B can be identiﬁed as a ∈∂B iff.: K K X i |bi −a| < β and o(a) > τ, (4) where o(a) is the opacity of that Gaussian point, |bi −a| is the Euclidean distance between bi and a. τ and β are thresholds and we empirically set τ to 0.95, β to 0.05×L. L is the size of the composite. (e.g. measured by the bounding box). An additional method for a better boundary condition on real-world data is that we discard outliers in both ﬁelds",
        "scene collection w/o opt. ߣଵൌʹ ߣଵൌͲǤͳ ߣଵൌͳͲ Figure 5. Ablation study on the effectiveness of gradient loss for different weights. Experiments show that higher weights can help to preserve more content while preventing harmonization. Only S. Only T. Both Figure 6. Ablation study on sampling-based cloning (S.) and clustering-based tuning (T.). Here, “Both” means the full scheme. w/ ߶ w/ ߶ w/o ߶ w/o ߶ Figure 7. Ablation study on the impact of mapping function φ in the S-phase. Random effects make composition more realistic. (e.g. some Gaussian points are far from the others, which may occur in some scenes). We calculate referenced features for these boundary points in order to conﬁrm the boundary condition. For each a ∈∂B, its target feature is: ˆf(a) = 1 K K X i f ′(bi), a ∈∂B, bi ∈KNN S (a) (5) where f ′(bi) is the feature of bi after transformation. To achieve this boundary condition, we optimize boundary points toward their target features: Lfeature = X a∈∂B f ′(a) −ˆf(a)",
        "2 , (6) where f ′(a) is the feature of a and we directly apply this loss on SH coefﬁcients. 3.3. Sampling-based Cloning We propose sampling-based cloning as our “S-phase” in optimization. The core idea is how to seamlessly propagate the style in boundary through the remaining points in the target ﬁeld while preserving its rich content. In contrast to a regular grid suitable with a gradient-based strategy in SeamlessNeRF [14], Gaussian points are irregularly and discretely distributed in 3D space. As a result, alternative approaches need to be explored. A straightforward idea is that given a point in target ﬁeld T , one can calculate the feature difference between that point and its neighbors in T , resembling “Laplacian coordinates”. Then, one can use that “difference” as the regularizer while minimizing Lfeature. However, this naive approach may fail even before propagation begins (see Fig. 4). Furthermore, the boundary’s structural characteristics (such as the bottle-bell intersection in the rightupper corner of Fig. 9) necessitate seamless cloning, which signiﬁcantly improves the stitching quality. Hence, we propose an effective sampling strategy to explicitly propagate features for each remaining point outside the boundary. The core idea lies in the way of searching several “driven points” for a candidate. The color of the candidate is driven by those points. For each a ∈T −∂B, the optimizing target of its color in direction da is: ˆf(a, da) = 1 K K X i f ′(bi, db), a ∈T −∂B, bi ∈KNN ∂B (φ(a)) (7) where f(a, da) means sampling SH color in view direction da (from point a to camera), the same as f(b, db). The camera centers are uniformly sampled from the surface of a",
        "Figure 8. Results from BlendedMVS [59] and Mip360 [2] datasets, showing our method can produce realistic effects in real-world scenes. sphere centered on the composite object’s origin. It is important to note that the sampling strategy KNN(φ(a)) maps the locations of nearby candidate points a-s to the correlated neighboring “driven points” and inherits the continuity of the textures from those “driven points”. We use φ(x) = x + sin(γ · δx) to add random effect by disturbing KNN searching (see Fig. 7), where x is the position of a, δx is the distance between a and its nearest bi in boundary, and γ is empirically set to 10. A larger γ is suitable for higher structural frequencies. In this way, we can synthesize structurally aware stitching results. With (7), we add a color loss to the S-phase: Lcolor = X a∈T −∂B f ′(a, da) −ˆf(a, da)",
        "2 , (8) so that the color of those candidates can be optimized towards their target to achieve explicit feature propagation. To preserve the original rich content in T , we present a more efﬁcient gradient loss calculated in the local space of T , leveraging the guidance in 2D screen space: Lgrad = X x∈I ∇xIT (p) −ˆ∇xIT (p)",
        "2 , IT (p) = R(Glocal T , p), (9) where p is the randomly sampled camera in the local space of target ﬁeld T , I is the rendered color image by rasterizer R of 3DGS. We pre-calculate ˆ∇xI for each camera with the Sobel operator [43] before the optimization starts. We found that supervising gradients in screen space is more efﬁcient than the straightforward one, as shown in Fig. 10. 3.4. Clustering-based Tuning While S-phase optimization is effective in preserving local color consistency, relying solely on it may lead to misaligned global appearance, such as uneven brightness, hues, and saturation (See Fig. 6). Therefore, we propose using a clustering extracted color palette to perform global tuning, which we refer to as the ”T-phase” in optimization. This approach enhances the overall harmony of the composite by performing dynamic matches to a palette. To implement the T-phase, we ﬁrst aggregate and cluster the color of the source ﬁeld from various angles: {ci}N, {wi}N ←A(Gglobal S ), (10) where ci is the color (cluster center) in palette, wi is the sample percentage occupied by the center, and A stands for our aggregation algorithm. Our approach, inspired by Li et al.’s work [26], uses a streaming method to accelerate color aggregation. We start with three bins, collect color samples from a random view, and calculate the new color center for each bin by averaging the original center and new samples collected in it. The number of bins expands to accommodate far-off samples. Centers expire after 20 iterations with no sufﬁcient votes. We repeat this process until all color centers are stable. Once the aggregation process ﬁnishes, those color centers will form a palette (see Fig. 2). We employ the following loss in our T-phase as a pixel-wise summation:",
        "ours SeamlessNeRF VQA average score ↑ 0.836 0.662 Table 1. Quantitative comparison between ours and the baseline. ours ours SeamlessNeRF SeamlessNeRF Figure 9. Comparisons between our approach and the baseline [14]. SeamlessNeRF failed in all of these real-world scenarios. Ltune = X c∈I′ wχc ∥c −cχc∥2 2 , I′ ←{IT x (p)|α(x) > 0.95}, χc = arg min 1fifN {∥c −ci∥2 −wi} , (11) where p is the randomly sampled camera in the global space, and α is the alpha mask corresponding to IT . Both α and IT are rendered by rasterizer R. χ represents the target bin’s index, and it is determined by both the distance from color centers and the probability density of bins. Our ﬁnal total loss function can then be expressed as: Ltotal = Lfeature + Lcolor + λ1Lgrad + λ2Ltune, (12) where both λ1 and λ2 are empirically set to 2.0 in our experiments. 4. Experiment To test the effectiveness and generality of our approach, we conducted experiments on a variety of fascinating 3D objects. We interactively built 17 composite results, comprising a total of 31 part models: 17 from BlendedMVS [59], 4 from Mip360 [2], 8 from SeamlessNeRF datasets, and 2 created by ourselves in a graphics engine. Figure 10. Ablation on two kinds of gradient loss. The 2D gradient supervision (upper row) is more effective than the straightforward one since it focuses on the surface instead of the whole space. 4.1. Qualitative Comparison We compare our method to SeamlessNeRF [14], the ﬁrst and most recent work that approaches our goal. Fig. 9 depicts three comparison cases. In the ﬁrst case (clay & bread), SeamlessNeRF failed to achieve high-level geometry editability and struggled with artifacts caused by implicit representation. In the second case (bottle and bell), SeamlessNeRF failed to maintain a harmonious seamless effect due to applying the gradient-based strategy on the complex boundary. In the third case, SeamlessNeRF failed to propagate sufﬁcient color tones due to the complex gradients in the boundary. In addition, we show that the 2D-guided style-transfer method [36] cannot produce a seamless stitching, as shown in Fig. 13. On the contrary, ours can handle all of these situations while producing harmonious results. 4.2. Quantitative Comparison Currently, there is neither a specialized dataset providing ground truth nor an established metric to assess the realism of a 3D model’s appearance, making it challenging to evaluate the effectiveness of our approach quantitatively. Nevertheless, we force an evaluation utilizing VQA (Video Quality Assessment) methods, as outlined by Wu et al. [49], and explored the use of 2D projection in video display for assessment purposes. Our results, presented in Tab. 1, demonstrate that our average score surpasses the baseline. For a comprehensive understanding of the quantitative experiments, please refer to our supplementary materials. 4.3. Ablation Study Effectiveness of 2D Gradient Loss. Fig. 5 depicts the effect of gradient loss at various weights. Higher weights can help to preserve more content while obstructing harmonization. Fig. 10 demonstrates that 2D gradient loss with Sobel operator is signiﬁcantly more effective than the simple one mentioned in Sec. 3.3. Functionality of S-phase and T-phase. We demonstrate the efﬁcacy of our two-phase scheme in Fig. 6. The S-phase",
        "(a) w/ bounding box (b) w / opacity > 0.1 (c) w/ brush by user (d) 2D supervision Figure 11. We describe the segmentation workﬂow using our GUI and compare it to the result (d) from 2D mask supervision (for example, the Segment Anything Model (SAM) [22]). To segment with SAM, we re-implement the inverse-mask [3] strategy on 3DGS. A simple (a) bounding box with a (b) interactive (c) brush is demonstrated to be more practical in real-world scenes with numerous ﬂoaters. w/o sample strategy w/ sample strategy Figure 12. Ablation study on keeping view-dependent effects by sampling strategy in the S-phase. With that strategy used in Eq. (7), the upper-view color of paint on bell is properly propagated. Figure 13. We re-implement SNeRF’s strategy [36] based on 3DGS to show that style-transfer methods fail to achieve our effect. 0s 30s 1min 2min 3min 4min + T-phase S-phase 0s 30s 1min 2min 3min 4min + T-phase S-phase Figure 14. Visualize how our optimization gradually and efﬁciently converges. In our comparison, SNeRF [36] takes over 10 hours, while SeamlessNeRF [14] takes more than an hour. aids in seamless boundary formation, while the T-phase aids in global harmonization when only the S-phase is present. Sampling Strategy for View-dependent Effects. We ablate the sampling strategy in the S-phase (see Fig. 12) to show that view-dependent effects can be properly propagated using this strategy instead of random sampling. 4.4. Editor and Application To enable a practical and user-friendly workﬂow, we created an interactive GUI editor that can control and visualize any procedure in the entire process in real-time, including Gaussian segmentation and transformation, boundary identiﬁcation, and optimization (see Fig. 14 and refer to the supplementary video for more details). Our framework can generate high-ﬁdelity and seamless results across a wide range of real-world scenarios, providing distinct advantages in the direct creation of imaginative 3D models from reality."
      
      ]
    },
    {
      "section": "Acknowledgments",
      "chunks": [
        "Xiaogang Jin was supported by the National Natural Science Foundation of China (Grant:62036010)."
      ]
    },
    {
      "section": "Limitations",
      "chunks": [
        "5. Conclusions and Limitations We have developed a highly efﬁcient and effective interactive framework for creating realistic 3D models. The method involves stitching Gaussian components seamlessly to create a harmonious 3D model that is an accurate representation of the real world. Our approach has been tested on real-world datasets and has proved to be capable of handling complex cases with a user-friendly interface. This presents a promising avenue for example-based modeling directly from the real world. Limitations and Future Work. Currently, our work is unable to transform Gaussian models in a non-rigid manner. To enable greater ﬂexibility in composition, we can incorporate deformation methods such as ARAP [17] in the future. In addition, our method does not account for more complex scenes involving specular reﬂections, or translucent objects. To address this, future work will integrate physically-based rendering (PBR) techniques, which would help decouple view-dependent effects from material properties. Moreover, we currently does not apply minor deformations to the geometry in stitching components. In the future, we aim to leverage diffusion priors to further enhance visual effects and minimize manual effort in the stitching process."
      ]
    },
    {
      "section": "References",
      "chunks": [
        "[1] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. Patchmatch: A randomized correspondence algorithm for structural image editing. ACM Trans. Graph., 28(3), 2009. 2 [2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance ﬁelds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5470–5479, 2022. 1, 6, 7 [3] Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei Shen, Lingxi Xie, Dongsheng Jiang, Xiaopeng Zhang, and Qi Tian. Segment anything in 3d with nerfs. In NeurIPS, 2023. 4, 8 [4] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting, 2023. 3 [5] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. Mobilenerf: Exploiting the polygon rasterization pipeline for efﬁcient neural ﬁeld rendering on mobile architectures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16569–16578, 2023. 3 [6] Soheil Darabi, Eli Shechtman, Connelly Barnes, Dan B Goldman, and Pradeep Sen. Image melding: Combining inconsistent images using patch-based synthesis. ACM Transactions on graphics (TOG), 31(4):1–10, 2012. 2 [7] Arnaud Dessein, William AP Smith, Richard C Wilson, and Edwin R Hancock. Seamless texture stitching on a 3d mesh by poisson blending in patches. In 2014 IEEE International Conference on Image Processing (ICIP), pages 2031–2035. IEEE, 2014. 3 [8] Alexei A Efros and Thomas K Leung. Texture synthesis by non-parametric sampling. In Proceedings of the seventh IEEE international conference on computer vision, pages 1033–1038. IEEE, 1999. 2 [9] Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser, and Pat Hanrahan. Example-based synthesis of 3d object arrangements. ACM Transactions on Graphics (TOG), 31(6):1–11, 2012. 3 [10] Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu, Lanyun Zhu, Xiaowei Zhou, Andreas Geiger, and Yiyi Liao. Panoptic nerf: 3d-to-2d label transfer for panoptic urban scene segmentation. In 2022 International Conference on 3D Vision (3DV), pages 1–11. IEEE, 2022. 3 [11] Thomas Funkhouser, Michael Kazhdan, Philip Shilane, Patrick Min, William Kiefer, Ayellet Tal, Szymon Rusinkiewicz, and David Dobkin. Modeling by example. ACM transactions on graphics (TOG), 23(3):652–663, 2004. 2, 3 [12] Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, and Yao Yao. Relightable 3d gaussian: Real-time point cloud relighting with brdf decomposition and ray tracing. arXiv:2311.16043, 2023. 2, 3 [13] Xinyu Gao, Ziyi Yang, Yunlu Zhao, Yuxiang Sun, Xiaogang Jin, and Changqing Zou. A general implicit framework for fast nerf composition and rendering. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pages 1833– 1841, 2024. 3 [14] Bingchen Gong, Yuehao Wang, Xiaoguang Han, and Qi Dou. Seamlessnerf: Stitching part nerfs with gradient propagation. In SIGGRAPH Asia 2023 Conference Papers, pages 1–10, 2023. 2, 3, 5, 7, 8 [15] Yi-Hua Huang, Yan-Pei Cao, Yu-Kun Lai, Ying Shan, and Lin Gao. Nerf-texture: Texture synthesis with neural radiance ﬁelds. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1–10, 2023. 3 [16] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4220–4230, 2024. 3 [17] Takeo Igarashi, Tomer Moscovich, and John F Hughes. Asrigid-as-possible shape manipulation. ACM transactions on Graphics (TOG), 24(3):1134–1141, 2005. 8 [18] Joseph Ivanic and Klaus Ruedenberg. Rotation matrices for real spherical harmonics. direct determination by recursion. The Journal of Physical Chemistry, 100(15):6342– 6347, 1996. 4 [19] Sagi Katz, George Leifman, and Ayellet Tal. Mesh segmentation using feature point and core extraction. The Visual Computer, 21:649–658, 2005. 3 [20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk¨uhler, and George Drettakis. 3d gaussian splatting for real-time radiance ﬁeld rendering. ACM Transactions on Graphics, 42 (4), 2023. 3 [21] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance ﬁelds. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19729–19739, 2023. 3, 4 [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll´ar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. [23] Vladislav Kreavoy, Dan Julius, and Alla Sheffer. Model composition from interchangeable components. In 15th Paciﬁc Conference on Computer Graphics and Applications (PG’07), pages 129–138. IEEE, 2007. 3 [24] Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru, Leonidas J Guibas, Andrea Tagliasacchi, Frank Dellaert, and Thomas Funkhouser. Panoptic neural ﬁelds: A semantic object-aware neural scene representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12871–12881, 2022. [25] Vivek Kwatra, Irfan Essa, Aaron Bobick, and Nipun Kwatra. Texture optimization for example-based synthesis. ACM Trans. Graph., 24(3):795–802, 2005. 2 [26] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Liefeng Bo. Compressing volumetric radiance ﬁelds to 1 mb. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4222–4231, 2023. 6",
        "[27] Weiyu Li, Xuelin Chen, Jue Wang, and Baoquan Chen. Patch-based 3d natural scene generation from a single example. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16762–16772, 2023. 3 [28] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8508–8520, 2024. 3 [29] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards high-ﬁdelity text-to-3d generation via interval score matching, 2023. 3 [30] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efﬁcient neural radiance ﬁelds for interactive free-viewpoint video. In SIGGRAPH Asia Conference Proceedings, 2022. 3 [31] Ruiyang Liu, Jinxu Xiang, Bowen Zhao, Ran Zhang, Jingyi Yu, and Changxi Zheng. Neural impostor: Editing neural radiance ﬁelds with explicit shape manipulation. In Computer Graphics Forum, page e14981. Wiley Online Library, 2023. [32] Yuan Liu, Peng Wang, Cheng Lin, Xiaoxiao Long, Jiepeng Wang, Lingjie Liu, Taku Komura, and Wenping Wang. Nero: Neural geometry and brdf reconstruction of reﬂective objects from multiview images. ACM Trans. Graph., 42(4), 2023. 2 [33] Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, and Xiaojuan Qi. 3dgsr: Implicit surface reconstruction with 3d gaussian splatting. ACM Transactions on Graphics (TOG), 43(6):1–12, 2024. 3 [34] Paul Merrell. Example-based model synthesis. In Proceedings of the 2007 symposium on Interactive 3D graphics and games, pages 105–112, 2007. 3 [35] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Konstantinos G Derpanis, Jonathan Kelly, Marcus A Brubaker, Igor Gilitschenski, and Alex Levinshtein. Spin-nerf: Multiview segmentation and perceptual inpainting with neural radiance ﬁelds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20669–20679, 2023. 4 [36] Thu Nguyen-Phuoc, Feng Liu, and Lei Xiao. Snerf: Stylized neural implicit representations for 3d scenes. 41(4), 2022. 7, [37] Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and Felix Heide. Neural scene graphs for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2856–2865, 2021. 3 [38] Patrick P´erez, Michel Gangnet, and Andrew Blake. Poisson image editing. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 577–582. 2023. 2 [39] Ryan Po and Gordon Wetzstein. Compositional 3d scene generation using locally conditioned diffusion. arXiv preprint arXiv:2303.12218, 2023. 3 [40] Yi-Ling Qiao, Alexander Gao, Yiran Xu, Yue Feng, Jia-Bin Huang, and Ming C Lin. Dynamic mesh-aware radiance ﬁelds. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 385–396, 2023. 3 [41] Claudio Rocchini, Paolo Cignoni, Claudio Montani, and Roberto Scopigno. Multiple textures stitching and blending on 3d objects. In Rendering Techniques’ 99: Proceedings of the Eurographics Workshop in Granada, Spain, June 21–23, 1999 10, pages 119–130. Springer, 1999. 3 [42] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In SIGGRAPH Conference Proceedings, pages 1–10, 2022. 3 [43] Irwin Sobel, Gary Feldman, et al. A 3x3 isotropic gradient operator for image processing. a talk at the Stanford Artiﬁcial Project in, pages 271–272, 1968. 6 [44] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8248–8258, 2022. 3 [45] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efﬁcient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. 3 [46] Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Errui Ding, Jingdong Wang, and Gang Zeng. Delicate textured mesh recovery from nerf via adaptive surface reﬁnement. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 17739–17749, 2023. 3 [47] Liao Wang, Qiang Hu, Qihan He, Ziyu Wang, Jingyi Yu, Tinne Tuytelaars, Lan Xu, and Minye Wu. Neural residual radiance ﬁelds for streamably free-viewpoint videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 76–87, 2023. 3 [48] Li-Yi Wei, Sylvain Lefebvre, Vivek Kwatra, and Greg Turk. State of the art in example-based texture synthesis. Eurographics 2009, State of the Art Report, EG-STAR, pages 93– 117, 2009. 2 [49] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In International Conference on Computer Vision (ICCV), 2023. 7 [50] Qianyi Wu, Xian Liu, Yuedong Chen, Kejie Li, Chuanxia Zheng, Jianfei Cai, and Jianmin Zheng. Objectcompositional neural implicit surfaces. In Computer Vision– ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVII, pages 197– 213. Springer, 2022. 3 [51] Rundi Wu and Changxi Zheng. Learning to generate 3d shapes from a single example. ACM Trans. Graph., 41(6), 2022. 3 [52] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Learning object-compositional neural radiance ﬁeld for editable scene rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13779– 13788, 2021. 3",
        "[53] Bangbang Yang, Chong Bao, Junyi Zeng, Hujun Bao, Yinda Zhang, Zhaopeng Cui, and Guofeng Zhang. Neumesh: Learning disentangled neural mesh-based implicit ﬁeld for geometry and texture editing. In European Conference on Computer Vision, pages 597–614. Springer, 2022. 3 [54] Ziyi Yang, Yanzhen Chen, Xinyu Gao, Yazhen Yuan, Yu Wu, Xiaowei Zhou, and Xiaogang Jin. Sire-ir: Inverse rendering for brdf reconstruction with shadow and illumination removal in high-illuminance scenes. arXiv preprint arXiv:2310.13030, 2023. 3 [55] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, and Raquel Urtasun. Unisim: A neural closed-loop sensor simulator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1389–1399, 2023. 3 [56] Ziyi Yang, Xinyu Gao, Yang-Tian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, and Xiaogang Jin. Spec-gaussian: Anisotropic view-dependent appearance for 3d gaussian splatting. Advances in Neural Information Processing Systems, 37:61192–61216, 2024. 3 [57] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for highﬁdelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20331–20341, 2024. 3 [58] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Realtime photorealistic dynamic scene representation and rendering with 4d gaussian splatting. In International Conference on Learning Representations (ICLR), 2024. 3 [59] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A largescale dataset for generalized multi-view stereo networks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1787–1796, 2020. 1, 6, 7 [60] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron, and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-time view synthesis. In ACM SIGGRAPH 2023 Conference Proceedings, New York, NY, USA, 2023. Association for Computing Machinery. 3 [61] Yizhou Yu, Kun Zhou, Dong Xu, Xiaohan Shi, Hujun Bao, Baining Guo, and Heung-Yeung Shum. Mesh editing with poisson-based gradient ﬁeld manipulation. In ACM SIGGRAPH 2004 Papers, page 644–651, New York, NY, USA, 2004. Association for Computing Machinery. 3 [62] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma, Rongfei Jia, and Lin Gao. Nerf-editing: geometry editing of neural radiance ﬁelds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18353–18364, 2022. 3 [63] Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, and Jingyi Yu. Editable free-viewpoint video using a layered neural representation. ACM Trans. Graph., 40(4), 2021. 3 [64] Yunzhi Zhang, Shangzhe Wu, Noah Snavely, and Jiajun Wu. Seeing a rose in ﬁve thousand ways. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 962–971, 2023. 3 [65] Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes, 2023. 3"
      ]
    }
  ]
}