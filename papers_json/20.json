{
  "paper_id": "20",
  "paper_title": "LLMs’ morphological analyses of complex FST-generated Finnish words",
  "sections": [
    {
      "section": "FrontMatter",
      "chunks": [
        "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 242–254 August 15, 2024 ©2024 Association for Computational Linguistics LLMs’ morphological analyses of complex FST-generated Finnish words Anssi Moisio1, Mathias Creutz2, and Mikko Kurimo1 1Department of Information and Communications Engineering, Aalto University, Finland 2Department of Digital Humanities, University of Helsinki, Finland anssi.moisio@aalto.fi, mathias.creutz@helsinki.fi, mikko.kurimo@aalto.fi"
      ]
    },
    {
      "section": "Abstract",
      "chunks": [
        "Rule-based language processing systems have been overshadowed by neural systems in terms of utility, but it remains unclear whether neural NLP systems, in practice, learn the grammar rules that humans use. This work aims to shed light on the issue by evaluating state-of-theart LLMs in a task of morphological analysis of complex Finnish noun forms. We generate the forms using an FST tool, and they are unlikely to have occurred in the training sets of the LLMs, therefore requiring morphological generalisation capacity. We find that GPT-4-turbo has some difficulties in the task while GPT-3.5turbo struggles and smaller models Llama270B and Poro-34B fail nearly completely. 1 Do neural networks learn grammar? The debate on whether neural networks (NNs) can be accurate models of human language often revolves around the question whether NNs learn similar grammar rules as children do. In a famous instance of the debate, Rumelhart and McClelland (1986) argued that a NN can capture the implicit rules that govern how English verbs are inflected in the past tense. In a response, Pinker and Prince (1988) counter that explicit rules are indispensable to explain how children learn past tenses, and more generally to explain the psychology of language. Neural methods have gradually become more capable of modelling varied aspects of language, which could be viewed as supporting the implicit rules argument. (For updates on the past-tense debate see Kirov and Cotterell (2018); Corkery et al. (2019); Fukatsu et al. (2024).) The most recent instances of the debate are over large language models (LLMs), whose language-generation and task-solving capabilities have surprised many. The recent debate consequently concerns modelling human language more generally instead of focusing on specific phenomena such as verb inflection. Considering the success of LLMs, it is clear that they learn some implicit rule-abiding behaviour that enables them to process and generate language competently, but it is still not clear if they learn grammar similarly to humans, or if they learn and employ some other set of rules. Assessing grammatical knowledge learned by NNs is not straightforward, but there are at least two popular approaches. Training a classifier (called a ‘probe’ (Alain and Bengio, 2016) or a ‘diagnostic classifier’ (Hupkes et al., 2018), first developed by Shi et al. (2016); Adi et al. (2017)) to classify the internal representations of NNs has been used to inspect what aspects of grammar are encoded in them. Probing studies have found various syntactical information encoded in neural NLP systems (Jawahar et al., 2019; Tenney et al., 2018; Papadimitriou et al., 2021), but interpreting the results remains contentious (Voita and Titov, 2020; Immer et al., 2022). The other popular method is to directly inspect a neural LM’s next-unit predictions, or to train a classifier NN to predict which word is most acceptable, given sequence of previous words.",
        "In an influential work by Linzen et al. (2016), knowledge of subject-verb agreement in LSTM networks was assessed this way, and it was concluded that ‘LSTMs can learn to approximate structure-sensitive dependencies fairly well’. Similar targeted syntactic evaluation methods, inspired by methods in psycholinguistics (e.g. Crain and Fodor (1985); Stowe (1986)), have subsequently been employed to assess the knowledge of many different grammatical phenomena in NNs, for example anaphora or negative polarity items (Marvin and Linzen, 2018; Futrell et al., 2019; Jumelet and Hupkes, 2018; Hu et al., 2020). Larger test suites such as BLiMP (Warstadt et al., 2020) or SyntaxGym (Gauthier et al., 2020) are used as benchmarks to track advances in the field. The general conclusion has not changed much since that of Linzen et al.’s: the networks are fairly 242 good at acquiring the grammar rules. Sometimes results of a single study are interpreted as evidence that the NNs have acquired a syntactical rule completely (e.g. Wilcox et al. (2023)), but a closer inspection often proves such an interpretation premature (e.g. Lan et al. (2024)). Since there is no conclusive evidence that NNs learn from text the same grammar that people use, it remains an important task to delineate the instances where NNs, and LLMs in particular, adhere to and utilise grammar, and the instances where they do not. Designing targeted syntactic evaluation tests requires careful formulation of the sequences. For example, Wilcox et al. (2023) examined the understanding of filler-gap effects by comparing the probabilities of acceptable and unacceptable continuations for sentence pairs such as ‘I know what the lion devoured’ and ‘I know that the lion devoured’. The continuation ‘yesterday’ is assumed to be acceptable for the former but not the latter sequence. However, ‘yesterday’ could be an acceptable next word even for the latter sequence: consider the sentence ‘I know that the lion devoured yesterday’s leftovers.’ This example highlights the difficulty of designing test sentences of this sort. Instead of inspecting the next-unit predictions or training diagnostic classifiers, in this work we ask LLMs explicitly to perform a classification task, which is possible due to the flexible text generation capacity of the LLMs. This makes the evaluation relatively unambiguous. For example, asking an LLM directly ‘Is the verb “devour” transitive or intransitive?’ does not leave much room for confounding factors. The apparent limitation of this method is that even if a model fails in an explicit classification task like this, we cannot rule out the possibility that the model nevertheless encodes perfect implicit knowledge of the verb and how to use it in any context. However, we make the assumption in this work that if the LLMs had learned a grammar rule as perfectly as humans, they would be able to answer the explicit questions as competently as humans.",
        "This seems justified considering the type and difficulty of, and LLMs’ performance in, other tasks used to evaluate LLMs, such as academic and professional exams (OpenAI, 2023). This approach was also taken by Weissweiler et al. (2023), who assessed the morphological competence of GPT-3.5-turbo by asking it directly to fill in past tenses of words in a sentence, and concluded that it ‘massively underperforms purposebuilt systems’. Similarly, Weller-Di Marco and Fraser (2024) took a morphologically complex word W and asked GPT-3.5-turbo questions such as ‘What is the head noun of W?’. In this work we present LLMs directly and explicitly with a classification task to investigate the knowledge of Finnish morphology in LLMs. Although Finnish has relatively few speakers worldwide (<10 million), it is not a low-resource language, having about 32B tokens of available training texts (Luukkonen et al., 2023, 2024). Consequently, the state-of-the-art (SOTA) multilingual LLMs such as GPT-4 are fluent in Finnish, and could be expected to have a good grasp of the grammar, if the LLMs are in fact good at learning grammar from text. 2 Data and methods Previous datasets of inflected Finnish words include the MorphyNet (Batsuren et al., 2021) and UniMorph (Kirov et al., 2016; Batsuren et al., 2022) corpora. We chose not to use data from these datasets for two reasons. Firstly, complex words comprising unusually many morphemes make it possible to assess if the systems can generalise to many types of possible inflections instead of learning only the most common inflection types. The previous datasets do not include many extremely complex word forms, but these can be generated using a finite-state transducer (FST). Secondly, since the SOTA LLMs have been trained on very large datasets harvested from the Internet, it is likely that the previously published datasets are included in their training data, which would preclude fair assessment. We use the Omorfi tools (Pirinen, 2015; Pirinen et al., 2017) that are based on finite-state morphology (Koskenniemi, 1984; Beesley and Karttunen,"
      ]
    },
    {
      "section": "2003) to generate inflected forms of Finnish nouns.",
      "chunks": [
        "The Omorfi library includes some 500k lexemes, of which about 140k are nouns. We inflect the nouns in all possible combinations of number, grammatical case, and possessive suffix (see Table 1 for examples, and Appendix A for further details), which BASE +PL +INE +SG2 / +PL1 laite laitteet laitteissa laitteissasi / laitteissamme +TRA laitteiksi laitteiksesi / laitteiksemme Table 1: Examples of inflections of the word ‘laite’ (‘device’). PL means plural, INE and TRA are case classes, and SG2/PL1 are possessive suffixes. Inflections in each column include also those in the columns to their left. 243 creates about 25M word forms. A random sample of 2000 inflected nouns is used as a test set in our experiments. We are unaware of any assessment of the generation accuracy of Omorfi, so we performed manual evaluation of the first 200 words in the sample and found 6 incorrectly inflected words. We therefore estimate the generation accuracy to be around 97%, which creates an upper bound for the classification accuracy of the test set. We publish the test set and all code to reproduce the results at https://github.com/aalto-speech/ llm-morph-tests. We note, however, that once the data is published, it is subject to the same data contamination issue as the previous datasets mentioned above—the good thing is that one can always draw a new random sample from the full set of 25M forms. Uniform sampling of lexemes creates a bias towards low-frequency types that are correlated with regularity of the inflection (Kodner et al., 2023). We note that this is the case in our data, as we took a random sample of the lexemes, and this should be kept in mind when interpreting the results; there are probably not many irregularly inflected words, which makes the task easier. This is not an issue, however, given our research question of whether the LLMs have picked up even the most systematic inflection types from textual data. Prompt: Jäsennä taivutetut substantiivit tällä tavalla: taivutusmuoto – perusmuoto, luku, sijamuoto, omistusliite vedessämme – vesi, yksikkö, inessiivi, 1. persoonan monikko kinoksiksensa – kinos, monikko, translatiivi, 3. persoona peukalostanne – peukalo, yksikkö, elatiivi, 2. persoonan monikko huurteenani – huurre, yksikkö, essiivi, 1. persoonan yksikkö sängiltäsi – sänki, monikko, ablatiivi, 2. persoonan yksikkö koivuumme – koivu, yksikkö, illatiivi, 1. persoonan monikko kaistojaan – kaista, monikko, partitiivi, 3. persoona rehtiyksiesi – rehtiys, monikko, genetiivi, 2. persoonan yksikkö laaksoillani – laakso, monikko, adessiivi, 1. persoonan yksikkö talollenne – talo, yksikkö, allatiivi, 2. persoonan monikko kansoiltanne – kansa, Correct answer: monikko, ablatiivi, 2. persoonan monikko Table 2: An example 10-shot prompt. An English translation of the first two rows is: Parse the inflected nouns in this manner: inflected form – base form, number, grammatical case, possessive suffix. The following rows are the examples.",
        "We use n-shot prompts with n ∈{0, 1, 5, 10}, and for all n we use the same n first examples. For instance, the 5-shot prompts have the vedessämme, kinoksiksensa, peukalostanne, huurteenani, and sängiltäsi example rows. LLMs are prompted to give a morphological analysis given an inflected form and the base form. That is, the models should give the correct number, case, and possessive suffix classes of the inflected noun. The prompt, shown in Table 2, comprises a short description of the task and the desired format, after which there are 0, 1, 5, or 10 examples of the task before the test word. We test GPT-4-turbo-1106-preview (Achiam et al., 2023) (which outperformed GPT-4-0613 in preliminary experiments), GPT-3.5-turbo-1106, Llama2-70B (Touvron et al., 2023) (outperformed smaller Llama2 models and chat versions), and Poro-34B (Luukkonen et al., 2024), which is trained on Finnish, English, and programming code. For Poro and Llama2, we performed a coarse tuning of the temperature parameter on a validation set, and found no large differences but 0.5 to be marginally better than the others, so we used this value in the experiments with these models. For the GPT models we found a temperature of 0.0 to yield the best results, so this value is used for GPT-4-turbo and GPT-3.5-turbo. We did not tune the top_p parameter (of nucleus sampling) but used the default value 1.0. Additionally, we trained simple recurrent neural network (RNN) models to also classify words (one RNN for each category: number, case, and possessive suffix), using random samples of the FST-generated word forms as training data (excluding the test set). The aim of this comparison is to give some indication of the difficulty of the task, and to see if NNs can handle the task if they are specifically trained on this small subset of Finnish morphology. We took the RNN off the shelf of the Pytorch library1 without tuning any of its hyperparameters. It consists of three layers of size 128. 3"
      ]
    },
    {
      "section": "Results",
      "chunks": [
        "The rightmost plot in Figure 1 shows that besides GPT-4-turbo, the models perform poorly in the task. GPT-4-turbo is not close to perfect accuracy either, and the combined 10-shot result does not reach the result achieved by simple RNNs trained with 80k words. With training set sizes of 800, 4k, 8k, 40k, and 80k words, the RNNs achieved accuracies of 0.380, 0.765, 0.774, 0.821, and 0.840, respectively. 1From the tutorial at https://pytorch.org/tutorials/ intermediate/char_rnn_classification_tutorial 244 Figure 1: Results in the morphological analysis task. The first three plots from left in Figure 1 break down the classification task into the three component classification tasks: number, case, and possessive suffix. There are some differences in the strengths of the models: Llama outperforms GPT3.5 in the possessive suffix classification task, while GPT-3.5 performs better for other classification tasks. In number classification, Poro outperforms Llama, although Llama performs better in other tasks. Figure 2 shows the confusion matrices for GPT4-turbo classifications of cases for the 0-shot and 10-shot setups. From the 0-shot confusion matrix we can see that the model does predict all classes even though we did not provide it with the names of the classes we expected it to recognise. This is not surprising, since GPT-4-turbo has no difficulties if asked to inflect a Finnish word in all cases and to provide the names of the cases. It is obvious that GPT-4-turbo has a fair amount of both declarative knowledge (metalinguistic knowledge; it knows the classes) and procedural knowledge (knows how to inflect the words) of the Finnish morphology. Therefore, the challenge in this task comes presumably from the need to generalise to infrequently used, morphologically complex word forms. 4"
      ]
    },
    {
      "section": "Discussion",
      "chunks": [
        "4.1 Reasons behind the errors Most current SOTA LLMs use subword tokenisation methods such as BPE (Sennrich et al., 2016) that break down infrequent character sequences into multiple shorter tokens while keeping frequent sequences as single tokens. Intuitively, having long tokens that combine multiple morphemes into a sinFigure 2: Case label confusions of GPT-4-turbo in the 0-shot and 10-shot setups. See Appendix B for all confusion matrices. gle token could hinder the capacity to model morphology, since multiple embeddings would have to be learned for a single morpheme. Of the three model families, Poro uses the longest tokens, having an average of 3.55 characters per token in our test words, while the Llama average is 2.16 and the GPT average is 2.26. Furthermore, the average length of the last token of a word is even longer:"
      ]
    },
    {
      "section": "4.42 for Poro, 2.41 for Llama, and 2.78 for GPT.",
      "chunks": [
        "For example, the first two test words whose possessive suffix Poro classifies incorrectly and differences in the tokenisations of the different models are shown in Table 3. Both of these words have the 245 Figure 3: Possessive suffix label confusions of GPT-4turbo in the 0-shot and 10-shot setups. See Appendix B for all confusion matrices. Base form lyhty (lantern) tarttuma (infection) Test word lyhtyjämme tarttumassamme Poro tokens ly hty jämme t art t um assamme Llama tokens ly ht yj äm me tart t um ass am me GPT tokens ly ht y j äm me t art t um ass am me Table 3: BPE tokenisations of different models. first person plural possessive suffix, which always ends in ‘me’. The possessive suffix ‘me’ is combined with the case morpheme (partitive ‘jä’ in ‘lyhtyjämme’ and inessive ‘ssa’ in ‘tarttumassamme’) by Poro but not by GPT or Llama. This might be one reason Poro misclassifies these words, while GPT and Llama do not, and in general why Poro lags behind the other models in the possessive suffix classification task as seen in Figure 1. The possessive suffix is simple to recognise, if the tokenisation is conducive to the task: a rule that checks the last two letters of the word and assigns ‘ni’–>SG1; ‘si’–>SG2; ‘me’–>PL1; ‘ne’–>PL2; else–>3 would achieve 100% accuracy on our test set. Admittedly, the rule would have to be more complicated if there were also words without any possessive suffix, since these words could end in virtually any two letters: for instance, ‘vesi’ (‘water’) ends in ‘si’ but does not have any possessive suffix (SG2 form would be ‘vetesi’) as does the translative case ‘vedeksi’ without a possessive suffix (the translative case with SG2 suffix becomes ‘vedeksesi’). Class frequencies could also explain some of the confusions. For example, GPT-4 often confuses abessive cases as partitive, seen in Figure 2. In addition to partitive being often quite similar to abessive, for example the inflected forms ‘kättä’ and ‘kädettä’ of the base ‘käsi’ (‘hand’), partitive is also much more common than abessive: 16.2% versus 0.1% of occurrences in Kettunen (2005). 4.2 Interpretations and implications The results suggest that despite the versatile language generation capacity of GPT-4-turbo it has not acquired the rules of Finnish morphology as completely as could be expected based on its language generation capacity. Instead, GPT-4 employs some other set of heuristics to decide the next token, although these undoubtedly overlap somewhat with grammar rules. This is hardly a surprise given the literature reviewed in Section 1, where the general conclusion tends to be that NNs rarely use grammar rules systematically, although usually fairly well. The ineptitude of neural nets to follow grammar rules is related to systematic compositionality and inefficiency w.r.t training data set size, which are said to be weaknesses of neural nets compared to rule-based systems.",
        "Learning grammar enables systematic compositional generalisation (Fodor and Pylyshyn, 1988): learning a concise grammar rule such as ‘the suffix -nne indicates 2nd person plural possessive form’ would enable generalising to all possible 2nd person plural forms in Finnish, obviating the need to learn word-specific associations and therefore reducing the required training corpus size. GPT-4 reaches close to 100% accuracy in this simple task of classifying possessive suffixes (RNN reaches 100%, and it is obvious that Finnish speakers would also reach 100%). However, the fact that it still sometimes classifies words ending in ‘nne’ as 2nd person singular instead of plural (see Figure 3) betrays its incomplete grasp of the systematic possessive suffixes in Finnish. Similar arguments apply to the other two classification tasks and the combined classification task. 5"
      ]
    },
    {
      "section": "Conclusion",
      "chunks": [
        "We conclude that even a SOTA LLM, GPT-4-turbo, does not model Finnish morphology thoroughly enough to allow it to provide morphological analyses of rare and complex word forms with a high accuracy. Contrasting this with its impressive text generation capacity suggests that it utilises some other language processing heuristics, which clearly overlap somewhat with morphological rules since it rarely produces incorrect forms, but which preclude human-level systematic generalisation on our test set. GPT-4-turbo outperforms models such as GPT-3.5-turbo and Llama2-70B, however, by a large margin. 246 "
      ]
    },
    {
      "section": "Acknowledgements",
      "chunks": [
        "We thank the anonymous reviewers for their insightful comments and feedback. The work was supported by the Finnish Cultural Foundation grant 00240853 and the Academy of Finland grant"
      ]
    },
    {
      "section": "Limitations",
      "chunks": [
        "Our experiments are limited to only one language and only four LLMs, which of course means we cannot be certain how the models perform on different languages, or how other models perform in Finnish, even though we suggest our results shed some light on general questions of grammar represented in LLMs. We also have not optimised the prompt beyond trying out a few different phrasings, so we assume some other prompt could elicit better performance especially in the 0- and 1-shot setups. As noted in the introduction, we assess LLMs using explicit, metalinguistic questions about Finnish morphology. It is in principle possible that even if the models fail in this task, having a limited grasp of the morphological labels, they could succeed in using the words correctly in sentences and representing their meanings correctly."
      ]
    },
    {
      "section": "337073. The computational resources were provided by Aalto ScienceIT. The use of the GPT-3.5",
      "chunks": [
        "and GPT-4 systems via the Azure OpenAI API was provided by Aalto IT Services."
      ]
    }
  ]
}