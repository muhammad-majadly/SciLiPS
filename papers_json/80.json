{
  "paper_id": "80",
  "paper_title": "80",
  "sections": [
    {
      "section": "FrontMatter",
      "chunks": [
        "HUGS: Human Gaussian Splats Muhammed KocabasNRB Jen-Hao Rick ChangN James GabrielN Oncel TuzelN Anurag RanjanN NApple RMax Planck Institute for Intelligent Systems BETH Zurich Short (80 frames), in-the-wild monocular video … 30 min. Animatable human avatar & disentangled scene representation 60 FPS rendering time 60 FPS Novel poses & scenes Figure 1. Human Gaussian Splats (HUGS) is a neural rendering framework that trains on 50-100 frames of a monocular video containing a human in a scene. HUGS enables novel view rendering with novel human poses at 60 FPS by learning a disentangled representation that can also render the human in other scenes."
      ]
    },
    {
      "section": "Abstract",
      "chunks": [
        "Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g., cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when arWork done while MK was an intern at Apple. ticulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ∼100× faster to train over previous work. Our code will be announced here: https: //github.com/apple/ml-hugs"
      ]
    },
    {
      "section": "Introduction",
      "chunks": [
        "Photorealistic rendering and animation of human bodies is an important area of research with many applications in AR/VR, visual effects, visual try-on, movie production, etc. Early works [1–3] for creating human avatars relied on capturing high-quality data in a multi-camera capture setup, extensive compute, and lots of manual effort. Recent work addresses these problem by directly generating 3D avatars from videos using 3D parametric body modThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.",
        "els like SMPL [4, 5], which offers advantages such as efficient rasterization and the ability to adapt to unseen deformations. However, the fixed topological structure of parameteric models limit the modeling of clothing, intricate hairstyles and other details of the geometry. Recent advancements have explored the use of neural fields for modeling 3D human avatars [6–11], often using a parametric body models as a scaffold for modeling deformations. Neural fields excel in capturing details like clothing, accessories, and hair, surpassing the quality that can be achieved by rasterization of parametric models with texture and other properties. However, they come with trade-offs, notably being less efficient to train and render. Furthermore, deformation of neural fields in a versatile manner presents challenges, often requiring recourse to an inefficient rootfinding loop, which adversely affects both training and rendering durations [12–14]. To address these challenges, we introduce a novel avatar representation HUGS—Human Gaussian Splats. HUGS represents both the human and the scene as 3D Gaussians and utilizing the 3D Gaussian Splatting (3DGS) [15] for its improved training and rendering speeds as compared to implicit NeRF representations [7, 16]. While utilizing the 3D Gaussian representation allows explicit control of humanbody deformation, it also creates new problems. Specifically, a realistic animation of human motions requires a coordination of individual Gaussians to retain surface integrity (i.e., without generating holes or pop outs). To enable human-body deformation, we introduce a deformation model that represents the human body in a canonical space using the 3D Gaussians. The deformation model predicts the mean-shifts, rotations, and scale of the 3D Gaussians to fit the subject’s body shape (in the canonical pose). Moreover, the deformation model predicts the Linear Blend Skinning (LBS) weights [4] that are used to deform the canonical human into the final pose. We initialize HUGS from the parameteric SMPL body shape model [4] but allow the Gaussians to deviate, increase, and pruned from the SMPL model. This enables HUGS to model the geometry and appearance details (e.g., hair and clothing) beyond the SMPL model. The learned LBS weights also coordinate the movement of Gaussians during animation. HUGS is trained on a single monocular video with 50-100 frames and learns a disentangled representation of the human and scene, enabling versatile use of the avatars in different scenes. In summary, our main contributions are: • We propose Human Gaussian Splats (HUGS), a neural representation for a human embedded in the scene that enables novel pose synthesis of the human and novel view synthesis of the human and the scene. • We propose a forward deformation module that represents the target human in a canonical space using 3D Gaussians and learns to animate them using LBS to novel poses. • HUGS enables fast creation and rendering of animatable human avatars from in-the-wild monocular videos with a small number of (50-100) frames, taking 30 minutes to train, improving over baselines [6, 7] by ∼100×, while rendering at 60 frames per second (FPS) at HD resolution.1. • HUGS achieves state-of-the-art reconstruction quality over baselines such as NeuMan [7] and Vid2Avatar [6] on the NeuMan dataset and the ZJU-Mocap dataset."
      ]
    },
    {
      "section": "Related Work",
      "chunks": [
        "Early works on photorealistic rendering and animation employed traditional computer graphics pipelines which involved large multi-camera setups such as lightstages [17] to capture the detailed texture and material of the human body. The animation of human bodies involved the rigging of an artist-created template of a human body mesh [1, 2]. The introduction of statistical body shape models [3–5, 18, 19] enabled representation of diverse human shape and animation of the human body by a single model. This reduced the manual effort in creating template meshes and rigging them. However, these shape models do not account for many details such as clothing, hair, accessories etc. Follow up works such as DRAPE [20] or CAPE [21] augment the shape models to add an additional layer of clothing or altogether choose a different representation such as occupancy [12,22–25] to represent the details of the geometry. In recent years, Neural Radiance Fields (NeRF) [26] have enabled a joint representation of geometry and appearance for view-synthesis using multiview images without the need of a large capture setup. Although, a NeRF is designed for capturing static objects, recent work [6–9,16,27–33] has extended the NeRF to enable capturing a dynamic moving humans. Weng et al. [8] propose a method to model a NeRF representation of a human using a single monocular video enabling 360 degree view generation of a human. Furthermore, NeuMan [7] introduces a joint NeRF representation of human and the scene capable of view synthesis and animation of the human in the scene. However, a major limitation of NeRF-based methods is that NeRFs are slow to train and render. Several methods have emerged to speed up training and rendering of NeRFs. These include using an explicit representation such as learning a function at grid points [34, 35], using hash encoding [36] or altogether discarding the learnable component [37,38]. Recent work on 3D Gaussian Splatting [15, 39, 40] uses a set of 3D Gaussians to represent a scene and renders it by splatting and rasterizing the Gaussians. This approach significantly improves the training and rendering times over traditional NeRFs. Recent work has addressed the extension of 3DGS scenes to controlled dynamic scenes [41] and 1The train/rendering speed is thanks to 3DGS [15], our contribution is enabling it for deformable cases such as humans.",
        "multi-camera capture setup [42]. However, the 3D Gaussian Splatting framework is not trivial to extend to dynamic humans that allows for both novel-view and novel-pose synthesis of human and the scene. Our methods builds on the 3D Gaussian Splatting framework [15] and utilize the SMPL body shape model [4] as a prior and learns a deformation model for animation control. We use a triplane and three MLPs to coordinate the Gaussians (e.g., their rotation, scale, color, and LBS weights)."
      ]
    },
    {
      "section": "Method",
      "chunks": [
        "Given a monocular video containing camera motions, a moving human, and a static scene, our method automatically disentangles and represents the human and the static scene with 3D Gaussians. The human Gaussians are initialized using the SMPL body model and the scene Gaussians are initialized from the structure-from-motion point cloud from COLMAP [43, 44]. In the following, we first quickly review 3D Gaussian splatting and the SMPL body model. Then, we introduce the proposed method to address challenges when modeling and animating humans in the 3D Gaussian framework. 3.1. Preliminaries 3D Gaussian Splatting (3DGS) [15] represents a scene by arranging 3D Gaussians. The i-th Gaussian is defined as G( \\ ma th b f {p}) = o_ i \\, e^{-\\frac {1}{2} (\\mathbf {p} - \\bm \\mu _i)^T \\bm \\Sigma _{i}^{-1} (\\mathbf {p} - \\bm \\mu _i)}, \\label {eq:gauss3d} (1) where p ∈R3 is a xyz location, oi ∈[0, 1] is the opacity modeling the ratio of radiance the Gaussian absorbs, µi ∈R3 is the center/mean of the Gaussian, and the covariance matrix Σi is parameterized by the scale Si ∈R+ along each of the three Gaussian axes and the rotation Ri ∈SO(3) with Σi = RiSiS⊤ i R⊤ i . Each Gaussian is also paired with spherical harmonics [45] to model the radiance emit towards various directions. During rendering, the 3D Gaussians are projected onto the image plane and form 2D Gaussians [46] with the covariance matrix Σ2D i = JW ΣiW ⊤J⊤, where J is the Jacobian of the affine approximation of the projective transformation and W is the viewing transformation. The color of a pixel is calculated via alpha blending the N Gaussians contributing to a given pixel:",
        "C",
        "= \\ sum _{j",
        "= 1 }^ { N} c_j \\alpha _{j} \\prod _{k=1}^{j-1} (1 - \\alpha _{k}), (2) where the Gaussians are sorted from close to far, cj is the color obtained by evaluating the spherical harmonics given viewing transform W, and αj is calculated from the 2D Gaussian formulation (with the covariance Σ2D j ) multiplied by its opacity oj. The rendering process is differentiable, which we take advantage of to learn our human model. SMPL [4] is a parametric human body model which allows pose and shape control. The SMPL model comes with a template human mesh ( ¯T , F ) in the rest pose (i.e., T-pose) in the template coordinate space. ¯T ∈Rnv×3 are the nv vertices on the mesh, and F ∈Nnt×3 are the nt triangles with a fixed topology. Given the body shape parameters, β ∈R|β|, and the pose parameters, θ ∈R3nk+3, SMPL transforms the vertices ¯T from the template coordinate space to the shaped space via T_S (\\ s ha p ecoef f , \\posecoeff ) = \\template + B_{S}(\\shapecoeff ) + B_{P}(\\posecoeff ), \\label {eq:smpl vertex} (3) where TS(β, θ) are the vertex locations in the shaped space, BS(β) ∈Rnv×3 and BS(θ) ∈Rnv×3 are the xyz offsets to individual vertices. The mesh in the shaped space fits the identity (e.g., body type) of the human shape in the rest pose. To animate the human mesh to a certain pose (i.e., transforming the mesh to the posed space), SMPL utilizes nk predefined joints and Linear Blend Skinning (LBS). The LBS weights W ∈Rnk×nv are provided by the SMPL model. Given the i-th vertex location on the resting human mesh, pi ∈R3, and individual posed joints’ configuration (i.e., their rotation and translation in the world coordinate), G = [G1, . . . , Gnk], where Gk ∈SE(3), the posed vertex location vi is calculated as vi = (Pnk k=1 Wk,i Gk) pi, where Wk,i ∈R is the element in W corresponding to the k-th joint and the i-th vertex. While the SMPL model provides an animatable human body mesh, it does not model hair and clothing. Our method utilizes SMPL mesh and LBS only during the initialization phase and allows Gaussians to deviate from the human mesh to model details like hairs and clothing. 3.2. Human Gaussian Splats Given T captured images and their camera poses, we first use a pretrained SMPL regressor [47] to estimate the SMPL pose parameters for each image, θ1, . . . , θT , and the body shape parameters, β, that is shared across images.2 Our method represents the human with 3D Gaussians and drive the Gaussians using a learned LBS. Our method outputs the Gaussian locations, rotations, scales, spherical harmonics coefficients, and their LBS weights with respect to the nk joints. An overview of our method is illustrated in Fig. 2. The human Gaussians are constructed from their center locations in a canonical space, a feature triplane [48, 49] F ∈R3×h×w×d, and three Multi-Layer Perceptrons (MLPs) which predict properties of the Gaussians. All of 2We also obtain a coordinate transformation from SMPL’s posed space to the world coordinate (used by the camera poses) for each frame, following Jiang et al. [7]. For simplicity, we will ignore the coordinate transformation in the discussions.",
        "Figure 2. HUGS overview. Given a video with dynamic human and camera motions, HUGS recovers an animatable human avatar and synthesizes human and scene from novel view points. Our method represents both the human and the scene as 3D Gaussians. The human Gaussians are parameterized by their mean locations in a canonical space and the features from a triplane. Three MLPs are used to estimate their color, opacity, additional shift, rotation, scale, and LBS weights to animate the Gaussians with given joint configurations. The human and the scene Gaussians are combined and rendered together with splatting. them are optimized per person. The Human Gaussians live in a canonical space, which is a posed space of SMPL where the human mesh performs a predefined Da-pose. Rendering process. Given a joint configuration G, to render an image, for each Gaussian, we first interpolate the triplane at its center location µi and get feature vectors f i x, f i y, f i z ∈Rd. The feature f i representing the i-th Gaussians is the concatenation of f i x, f i y, f i z. Taking f i as input, an appearance MLP, DA, outputs the RGB color and the opacity of the i-th Gaussian; a geometry MLP, DG, outputs an offset to the center location, ∆µi, the rotation matrix Ri (parameterized by the first two columns), and the scale of three axes Si; a deformation MLP, DD, outputs the LBS weights, Wi ∈Rnk for this Gaussian. The LBS uses W and the joint transformation G to transform the Human Gaussians, which are then combined with the Scene Gaussians and splat onto the image plane. Here, we use the static 3D Gaussians for scene representation identical to [15]. The rendering process is end-to-end differentiable. Optimization. We optimize the center locations of the Gaussians µ, the feature triplane, and the parameters of the three MLPs.3 The rendered image is compared with the ground-truth captured image using L1 loss, the SSIM 3We also follow Jiang et al. [7] and adjust the per image SMPL pose parameters θ during the optimization, since θ are initialized by an off-theshelf SMPL regressor [47] and may contain errors (see the details in the supplementary material). loss [50] Lssim, and the perceptual loss [51] Lvgg. We also render a human-only image (using only the human Gaussians on a random solid background) and compare regions containing the human in the ground-truth image using the above losses. The human regions are obtained using a pretrained segmentation model [52]. We also regularize the learned LBS weights W to be close to those from SMPL with an ℓ2 loss. Specifically, to regularize the LBS weights W , for individual Gaussians we retrieve their k = 6 nearest vertices on the SMPL mesh and take a distance-weighted average of their LBS weights to get ˆ W . The loss is LLBS = ∥W −ˆ W ∥2 F. Specifically, our loss is composed of \\ math c al {L} = \\unde r br a ce {\\ l ambda _1 \\ m a thca l {L } _1 + \\l a mb d a _2 \\mathcal {L}_{\\text {ssim}} + \\lambda _3 \\mathcal {L}_{\\text {vgg}}}_{\\text {scene + human}} \\\\ + \\underbrace {\\lambda _1 \\mathcal {L}^h_1 + \\lambda _2 \\mathcal {L}^h_{\\text {ssim}} + \\lambda _3 \\mathcal {L}^h_{\\text {vgg}}}_{\\text {human}} + \\lambda _4 \\mathcal {L}_{\\text {LBS}}, (4) where λ1 = 0.8, λ2 = 0.2, λ3 = 1.0, λ4 = 1000 for all scenes in the experiments. We employ the Adam optimizer [53] with a learning rate of 10−3, coupled with a cosine learning rate schedule. We initialize the center of the Gaussians, µ, at the canonical-posed SMPL mesh vertices (so we have the same number of Gaussians as the SMPL vertices at the beginning of the optimization). We pretrain the feature triplane",
        "and the MLPs to output RGB color as [0.5, 0.5, 0.5], opacity o = 0.1, ∆µ = 0, rotation R so that z-axis of the Gaussians align with the corresponding SMPL vertex normal, scale S as the average incoming edges’ lengths, and LBS weights W as those from SMPL (since the Gaussians lie exactly on the SMPL vertices). The pretraining takes 5000 iterations (1 minute on a GeForce 3090Ti GPU). We use an upsampled version of SMPL with nv = 110, 210 vertices and nt = 220, 416 faces. Note that the SMPL mesh and LBS weights are only used in the initialization and regularization, i.e., they are not used during testing. During the optimization, similar to the standard 3DGS, we clone, split, and prune Gaussians, every 600 iterations, based on their loss gradient and opacity. They are important steps to avoid local minima during the optimization. To clone and split, we simply add additional entries in µ by repeating existing centers (cloning) and randomly sampling within the Gaussians with respect to their current shapes (i.e., R and S) (splitting). To prune a Gaussian, we remove it from µ. Since the new Gaussians’ centers are close to the original ones on the triplane, their features are similar and thus the new Gaussians have similar shape as the originals, allowing the optimization to proceed normally. To make the split Gaussians smaller, we record a base scale s ∈R+ for each Gaussian. The base scale is initialized as 1 and every time a Gaussian is split, we divide the base scale by 1.6. The actual scale of the Gaussian is s multiplied with the MLP estimate S. The entire optimization takes 12K iterations, and 30 minutes on a GeForce 3090Ti GPU. At the end of the optimization, the human is represented by 200K Gaussians on average. Test-time rendering. Importantly, after the optimization, the 3D Gaussians can be explicitly constructed, allowing direct animation of the human Gaussians using the LBS weights. In other words, we do not need to evaluate the triplane and MLPs to render new human poses. This is a big advantage compared to methods that represent human as implicit neural fields."
      ]
    },
    {
      "section": "Experiments",
      "chunks": [
        "4.1. Datasets NeuMan Dataset [7] consists of six videos, each lasting between 10 to 20 seconds, featuring a single individual captured using a mobile phone. The camera pans through the scenes, facilitating multi-view reconstruction. The sequences are denoted as Seattle, Citron, Parking, Bike, Jogging, and Lab. Following the approach outlined in [7], we split frames into 80% training frames, 10% validation frames, and 10% test frames. ZJU-MoCap Dataset [9] consists of videos of a human captured in a lab using multi-view capture setup. To align with the methodology in [8,14], we select six subjects (377, 386, 387, 392, 393, 394) showcasing diverse motions. We employed images captured by ”camera 1” as input and utilized the other 22 cameras for evaluation. The camera matrices, body pose, and segmentation provided by the dataset were directly applied in our evaluation process. Since the dataset is captured in a light stage studio, we only optimize the human Gaussians. 4.2. Qualitative Results State-of-the-art Comparison. We show the qualitative results of our method in Fig. 3 and compare it with Vid2Avatar [6] and NeuMan [7]. The results are shown from the test samples of the NeuMan dataset [7] that are not seen during training. In the scene background regions, HUGS shows better reconstruction quality than both Vid2Avatar and NeuMan. Vid2Avatar shows blurry scene reconstruction with several artifacts. In contrast, NeuMan shows better scene reconstruction quality but misses fine details such as the house numbers (zoomed-in) in the first row, the wooden plank (zoomed-in) in the second row and the cupboard (zoomed-in) in the third row. In comparison, HUGS shows better reconstruction quality and preserves these fine details as shown in the zoomed-in regions. In the human regions, Vid2Avatar shows artifacts in the hand region (row 1) and blurry reconstruction in the feet (row 2) and arm region (row 3). In contrast, NeuMan gets better details of the feet regions in some cases (row 2) and introduces artifacts in hands (row 2) and feet (row 3) regions in other cases. In comparison, our method preserves the details around hand and feet and shows better reconstruction quality. Furthermore, our method also preserves the structure around clothing (row 1) where the wrinkles are reconstructed well while preserving the structure of the zipper (zoomed-in) around it compared to previous work. In summary, we note that HUGS shows better reconstruction quality of both the scene and the human as compared to previous methods while being orders of magnitude faster to train and render (see §4.3 for speed comparison). We will provide additional qualitative results with videos in the Supp. Mat. Canonical Human Shapes. In Fig. 5, we show the reconstruction of the human in the canonical space. We note that our method captures fine details around the feet and hands of the human which look noisy in the case of NeuMan [7]. Furthermore, we note that our method preserves rich details on the face. This enables us to achieve high reconstruction quality during the animation phase. Disentanglement of the Human and the Scene. HUGS allows for a disentangled represenation of the human and the scene by storing their Gaussian features separately. This allows us to move the human to different scenes. In Fig. 4,",
        "Ground Truth HUGS (ours) NeuMan [7] Vid2Avatar [6] Figure 3. Qualitative results comparing HUGS (ours) with NeuMan and Vid2Avatar with full human (left) and zoomed-in regions (right) for each of the methods. HUGS shows better reconstruction quality especially around hands, feet and clothing wrinkles. Figure 4. Rendering obtained by transferring the Human Gaussians to a different scene. Top-left corner shows the original scene in which the human was captured. we show the composition of human captured in one scene into a different scene. We show additional video results in the supplemental material. 4.3. Quantitative Results We compare the performance of our method with NeRFT [54], HyperNeRF [55], NeuMan [7] and Vid2Avatar [6].",
        "Seattle Citron Parking Bike Jogging Lab PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ NeRF-T [54] 21.84 0.69 0.37 12.33 0.49 0.65 21.98 0.69 0.46 21.16 0.71 0.36 20.63 0.53 0.49 20.52 0.75 0.39 HyperNeRF [55] 16.43 0.43 0.40 16.81 0.41 0.56 16.04 0.38 0.62 17.64 0.42 0.43 18.52 0.39 0.52 16.75 0.51 0.23 Vid2Avatar [6] 17.41 0.56 0.60 14.32 0.62 0.65 21.56 0.69 0.50 14.86 0.51 0.69 15.04 0.41 0.70 13.96 0.60 0.68 NeuMan [7] 23.99 0.78 0.26 24.63 0.81 0.26 25.43 0.80 0.31 25.55 0.83 0.23 22.70 0.68 0.32 24.96 0.86 0.21 HUGS 25.94 0.85 0.13 25.54 0.86 0.15 26.86 0.85 0.22 25.46 0.84 0.13 23.75 0.78 0.22 26.00 0.92 0.09 Table 1. Comparison of HUGS (ours) with previous work on test images of the NeuMan dataset [7] using PSNR, SSIM and LPIPS metrics. HUGS achieves state-of-the-art performance across every category with the exception of PSNR on the Bike sequence. Seattle Citron Parking Bike Jogging Lab PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ Vid2Avatar [6] 16.90 0.51 0.27 15.96 0.59 0.28 18.51 0.65 0.26 12.44 0.39 0.54 16.36 0.46 0.30 15.99 0.62 0.34 NeuMan [7] 18.42 0.58 0.20 18.39 0.64 0.19 17.66 0.66 0.24 19.05 0.66 0.21 17.57 0.54 0.29 18.76 0.73 0.23 HUGS 19.06 0.67 0.15 19.16 0.71 0.16 19.44 0.73 0.17 19.48 0.67 0.18 17.45 0.59 0.27 18.79 0.76 0.18 Table 2. Comparison of HUGS (ours) with previous work on the NeuMan dataset [7] over human-only regions cropped using a tight bounding box. Performance is evaluated on PSNR, SSIM and LPIPS metrics. PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ NeuralBody [9] 29.11 0.97 0.04 30.54 0.97 0.05 27.00 0.95 0.06 30.10 0.96 0.05 28.61 0.96 0.06 29.10 0.96 0.05 HumanNerf [8] 30.41 0.97 0.02 33.20 0.98 0.03 28.18 0.96 0.04 31.04 0.97 0.03 28.31 0.96 0.04 30.31 0.96 0.03 InstantNVR [56] 31.36 0.98 0.03 33.53 0.98 0.03 28.11 0.96 0.05 32.03 0.97 0.04 29.55 0.96 0.05 31.46 0.97 0.04 MonoHuman [14] 30.77 0.98 0.02 32.97 0.97 0.03 27.93 0.96 0.03 31.24 0.97 0.03 28.46 0.96 0.03 28.94 0.96 0.04 HUGS 30.80 0.98 0.02 34.11 0.98 0.02 29.29 0.97 0.03 31.36 0.97 0.03 29.80 0.97 0.03 30.54 0.97 0.03 Table 3. Comparison of HUGS (ours) with the previous work on the ZJU Mocap dataset [9]. Performance is evaluated on PSNR, SSIM and LPIPS metric. HUGS achieves state-of-the-art performance across all scenes and all metrics. NeuMan [7] HUGS (ours) Figure 5. Visualization of Human in canonical Da-pose for HUGS (ours) showing qualitative improvements over NeuMan [7]. In Table 1, we evaluate the reconstruction quality on the NeuMan dataset [7] on three different metrics – Peak Signal-to-Noise Ration (PSNR), SSIM [50] and LPIPS [57]. NeRF-T and HyperNeRF are general dynamic scene reconstruction methods and do not specialize for humans. Therefore, they show poor reconstruction quality. On the other hand, NeuMan and Vid2Avatar employ specialized models for the human and the scene. NeuMan employs a NeRF-based [26] approach for both scene and human modeling. Vid2Avatar utilizes an implicit SDF model and volume rendering for scene and human representation. Therefore, both NeuMan and Vid2Avatar show improved reconstruction quality. In comparison, our method achieves state-of-the-art performance across all the scenes and metrics except PSNR on the Bike sequence where we show competitive performance. In Table 2, we further evaluate the reconstruction error but only on the regions containing the human. We first take a tight crop around the human region in the ground truth image. This crop is used over all the predictions, and the reconstruction error is evaluated over the cropped samples. It should be noted that we take rectangular crops of the region and do not use any segmentation mask since reconstruction metrics are highly sensitive to masks. Under this evaluation, we show state-of-the-art performance across all scenes and metrics except PSNR on the Jogging sequence where we show competitive performance. In addition, we evaluate our method using the ZJUMoCap dataset [9] in Table 3. We compare with recent previous work that report their evaluation on this dataset which include NeuralBody [9], HumanNerf [8], InstantNVR [56], and MonoHuman [14]. Speed. In Fig. 6, we compare the training and rendering time of our method with previous work. The use of 3DGS [15] speeds up our training and rendering times by a significant margin. We note that HUGS is 96× faster than Vid2Avatar and 336× faster than NeuMan training within 30 minutes. At rendering time, we do not rely on MLPs and only use the LBS weights, enabling higher frame rate. Our method achieves 60 FPS outperforming NeuMan by ∼7600× and Vid2Avatar by ∼3800×. For human-only op-",
        "Seattle Citron Parking Bike Jogging Lab PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ w/o LBS 18.47 0.66 0.16 19.00 0.70 0.16 19.13 0.72 0.19 19.73 0.68 0.18 17.29 0.58 0.27 18.80 0.76 0.18 w/o Densify 18.91 0.65 0.16 17.18 0.68 0.18 19.00 0.71 0.21 19.92 0.67 0.19 17.63 0.57 0.26 18.98 0.76 0.18 w/o Lh 18.87 0.67 0.16 17.31 0.67 0.19 17.76 0.70 0.23 19.63 0.68 0.19 18.23 0.60 0.26 18.75 0.76 0.19 w/o Triplane 18.70 0.65 0.15 17.12 0.67 0.17 18.78 0.69 0.19 19.52 0.66 0.18 17.60 0.56 0.25 18.91 0.74 0.19 HUGS 19.06 0.67 0.15 19.16 0.71 0.16 19.44 0.73 0.17 19.48 0.67 0.18 17.45 0.59 0.27 18.79 0.76 0.18 Table 4. Ablation study. The performance is evaluated over human-only bounding box regions using PSNR, SSIM and LPIPS metrics. Figure 6. Timing comparison for training in hours and rendering in milliseconds. y-axis is on log-scale. HUGS outperforms previous methods by an order of magnitude. a) HUGS b) w/o LBS c) w/o Densify d) w/o e) w/o Triplane Figure 7. Ablation study showing the visualization of details captured in the human canonical shape under different ablations of our method. timization performed in ZJU-MoCap dataset, our method takes 8min to train while InstantNVR reports 5min. Even though our method is slightly slower to train, it operates in realtime with 60 FPS during test time, whereas InstantNVR achieves 0.5 FPS. We benchmark all the methods on a single GeForce 3090Ti GPU. In summary, our model demonstrates efficiency in both training and rendering, delivering superior results compared to existing methods. Our model not only outperforms established NeRF and implicit-SDF based models but does so at orders of magnitude faster speeds. 4.4. Ablation Experiments We show the effect of ablating over our method in Fig. 7. We note that removing LBS from our full model results in floating artifacts that are mainly introduced in the corner region or the body. We also experiment by keeping the number of Human Gaussians to be fixed by disabling densification. This results in floaters around the edges (row 1, on the side of the shirt) since noisy Gaussians are not culled and large Gaussians are not split. Furthermore, we examine the effect of removing the loss on the human pixels Lh. This results in loss of fine details in the human region as evident from the reconstruction of the shoes (row 2). In addition, removing the triplane+MLP and directly optimizing the 3DGS parameters results in noisy estimates. Please refer to supplemental material for a detailed ablation and analysis of our contributions. In Table 4, we show quantitative results on the NeuMan dataset by evaluating over only the human-regions by cropping it using a tight bounding box. We evaluate rendering quality using PSNR, SSIM and LPIPS metrics."
      ]
    },
    {
      "section": "Limitations",
      "chunks": [
        "  Limitations and Future Work. HUGS is limited by the underlying shape model SMPL [4] and linear blend skinning that may not capture the general deformable structure of loose clothing such as dresses. In addition, HUGS is trained on in-the-wild videos that do not cover the posespace of the human body. Future work will aim to alleviate these problems by modeling non-linear clothing deformation. In addition, the lack of data maybe alleviated by learning an appearance prior on human-poses using generative approaches such as GNARF [10] and AG3D [11] or by distilling from image diffusion models [58, 59]. Furthermore, our model does not account for environment lighting that may effect the composition of the human in a different scene with a different environment light which can be addressed by factoring out an illumination representation [60,61]. Acknowledgements: We thank Angelos Katharopoulos, Thomas Merth, Raviteja Vemulapalli, Barry Theobald, and Skyler Seto for their feedback on the manuscript and Wei Jiang for providing the details on NeuMan experiments."

      ]
    },
    {
      "section": "Conclusion",
      "chunks": [
        "We have proposed HUGS, a new method for novel-view and novel-pose synthesis of a human embedded in the scene by bringing a deformable model into the Gaussian Splatting framework. The method is able to reconstruct human and scene representations from in-the-wild monocular videos containing a small number of frames (50-100). HUGS enables fast training (in 30 mins) and rendering (60 FPS), 100× faster than the previous methods [6, 7], while at the same time significantly improving rendering quality as measured by PSNR, SSIM and LPIPS metrics." 
      ]
    },
    {
      "section": "References",
      "chunks": [
        "[1] Oleg Alexander, Mike Rogers, William Lambeth, Jen-Yuan Chiang, Wan-Chun Ma, Chuan-Chang Wang, and Paul Debevec. The digital emily project: Achieving a photorealistic digital actor. IEEE Computer Graphics and Applications, 30(4):20–31, 2010. 1, 2 [2] Oleg Alexander, Graham Fyffe, Jay Busch, Xueming Yu, Ryosuke Ichikari, Andrew Jones, Paul Debevec, Jorge Jimenez, Etienne Danvoye, Bernardo Antionazzi, et al. Digital ira: Creating a real-time photoreal digital actor. In ACM SIGGRAPH 2013 Posters, pages 1–1. 2013. 1, 2 [3] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, and James Davis. Scape: shape completion and animation of people. In ACM SIGGRAPH 2005 Papers, pages 408–416. 2005. 1, 2 [4] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1–248:16, October 2015. 2, 3, 8 [5] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J Black. Expressive body capture: 3d hands, face, and body from a single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10975–10985, 2019. 2 [6] Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. Vid2avatar: 3d avatar reconstruction from videos in the wild via self-supervised scene decomposition. In Computer Vision and Pattern Recognition (CVPR), 2023. 2, 5, 6, 7, 8 [7] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, and Anurag Ranjan. Neuman: Neural human radiance field from a single video. In Proceedings of the European conference on computer vision (ECCV), 2022. 2, 3, 4, 5, 6, 7, [8] Chung-Yi Weng, Brian Curless, Pratul P Srinivasan, Jonathan T Barron, and Ira Kemelmacher-Shlizerman. Humannerf: Free-viewpoint rendering of moving people from monocular video. In Proceedings of the IEEE/CVF conference on computer vision and pattern Recognition, pages 16210–16220, 2022. 2, 5, 7 [9] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 5, 7 [10] Alexander Bergman, Petr Kellnhofer, Wang Yifan, Eric Chan, David Lindell, and Gordon Wetzstein. Generative neural articulated radiance fields. Advances in Neural Information Processing Systems, 35:19900–19916, 2022. 2, 8 [11] Zijian Dong, Xu Chen, Jinlong Yang, Michael J Black, Otmar Hilliges, and Andreas Geiger. AG3D: Learning to generate 3D avatars from 2D image collections. In International Conference on Computer Vision (ICCV), 2023. 2, 8 [12] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges, and Andreas Geiger. Snarf: Differentiable forward skinning for animating non-rigid neural implicit shapes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11594–11604, 2021. 2 [13] Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. Instantavatar: Learning avatars from monocular video in 60 seconds. In Computer Vision and Pattern Recognition (CVPR), 2023. 2 [14] Zhengming Yu, Wei Cheng, Xian Liu, Wayne Wu, and Kwan-Yee Lin. MonoHuman: Animatable human neural field from monocular video. In CVPR, 2023. 2, 5, 7 [15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk¨uhler, and George Drettakis. 3D gaussian splatting for real-time radiance field rendering. In ACM Transactions on Graphics, 2023. 2, 3, 4, 7 [16] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and Christian Theobalt. Neural actor: Neural free-view synthesis of human actors with pose control. ACM transactions on graphics (TOG), 40(6):1–16, 2021. 2 [17] Paul Debevec. The light stages and their applications to photoreal digital actors. SIGGRAPH Asia, 2(4):1–6, 2012. 2 [18] Ahmed A. A. Osman, Timo Bolkart, and Michael J. Black. STAR: Sparse trained articulated human body regressor. In Computer Vision - ECCV 2020, volume 6 of Lecture Notes in Computer Science, 12351, pages 598–613, Cham, August 2020. Springer. 2 [19] Ahmed A. A. Osman, Timo Bolkart, Dimitrios Tzionas, and Michael J. Black. SUPR: A sparse unified part-based human representation. In Computer Vision – ECCV 2022, volume 2 of Lecture Notes in Computer Science, 13662, pages 568– 585, Cham, October 2022. Springer. 2 [20] Peng Guan, Loretta Reiss, David A. Hirshberg, Alexander Weiss, and Michael J. Black. DRAPE: Dressing any person. ACM Trans. Graph., 31(4), jul 2012. 2 [21] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael J Black. Learning to dress 3d people in generative clothing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6469–6478, 2020. 2 [22] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. PiFU: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2304–2314, 2019. 2 [23] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84–93, 2020. 2",
        "[24] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J. Black. ECON: Explicit Clothed humans Optimized via Normal integration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023. 2 [25] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J. Black. ICON: Implicit Clothed humans Obtained from Normals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13296–13306, June 2022. 2 [26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision, pages 405–421. Springer, 2020. 2, 7 [27] Chung-Yi Weng, Brian Curless, and Ira KemelmacherShlizerman. Vid2Actor: Free-viewpoint Animatable Person Synthesis from Video in the Wild. arXiv preprint arXiv:2012.12884, 2020. 2 [28] Shih-Yang Su, Frank Yu, Michael Zollhoefer, and Helge Rhodin. A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering. https://arxiv.org/abs/2102.06199, 2021. 2 [29] Yao Feng, Jinlong Yang, Marc Pollefeys, Michael J. Black, and Timo Bolkart. Capturing and animation of body and clothing from monocular video. In SIGGRAPH Asia 2022 Conference Papers, SA ’22, 2022. 2 [30] Marko Mihajlovic, Aayush Bansal, Michael Zollhoefer, Siyu Tang, and Shunsuke Saito. KeypointNeRF: Generalizing image-based volumetric avatars using relative spatial encoding of keypoints. In European Conference on Computer Vision, 2022. 2 [31] Zicong Fan, Maria Parelli, Maria Eleni Kadoglou, Muhammed Kocabas, Xu Chen, Michael J Black, and Otmar Hilliges. HOLD: Category-agnostic 3d reconstruction of interacting hands and objects from video. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [32] Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed Kocabas, Manuel Kaufmann, Michael J. Black, and Otmar Hilliges. ARCTIC: A dataset for dexterous bimanual handobject manipulation. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2 [33] Yao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, and Michael J. Black. Learning disentangled avatars with hybrid 3d representations. arXiv, 2023. 2 [34] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision, pages 333–350. Springer, 2022. 2 [35] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14335– 14345, 2021. 2 [36] Thomas M¨uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):1–15, 2022. 2 [37] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5501–5510, 2022. 2 [38] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. NeurIPS, 2020. 2 [39] Leonid Keselman and Martial Hebert. Direct fitting of gaussian mixture models. In 2019 16th Conference on Computer and Robot Vision (CRV), pages 25–32, 2019. 2 [40] Leonid Keselman and Martial Hebert. Approximate differentiable rendering with algebraic surfaces. In 2022 European Conference on Computer Vision, page 596–614, Berlin, Heidelberg, 2022. Springer-Verlag. 2 [41] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023. 2 [42] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3D gaussians: Tracking by persistent dynamic view synthesis. In International Conference on 3D Vision (3DV), 2024. 3 [43] Johannes Lutz Sch¨onberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision (ECCV), 2016. 3 [44] Johannes Lutz Sch¨onberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 3 [45] Ravi Ramamoorthi and Pat Hanrahan. An efficient representation for irradiance environment maps. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 497–500, 2001. 3 [46] M. Zwicker, H. Pfister, J. van Baar, and M. Gross. Surface splatting. In ACM Transactions on Graphics (Proc. ACM SIGGRAPH), pages 371–378, 07/2001 2001. 3 [47] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4D: Reconstructing and tracking humans with transformers. In ICCV, 2023. 3, 4 [48] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In European Conference on Computer Vision (ECCV), 2020. 3 [49] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3D generative adversarial networks. In CVPR, 2022. 3",
        "[50] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image Quality Assessment: From Error Visibility to Structural Similarity. IEEE TIP, 2004. 4, 7 [51] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In 3rd International Conference on Learning Representations (ICLR 2015). Computational and Biological Learning Society, 2015. 4 [52] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4015–4026, October 2023. 4 [53] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), San Diega, CA, USA, 2015. 4 [54] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6498– 6508, 2021. 6, 7 [55] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo MartinBrualla, and Steven M Seitz. Hypernerf: a higherdimensional representation for topologically varying neural radiance fields. ACM Transactions on Graphics (TOG), 40(6):1–12, 2021. 6, 7 [56] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In CVPR, 2023. 7 [57] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586–595, 2018. 7 [58] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 8 [59] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300–309, 2023. 8 [60] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields. In CVPR, 2022. 8 [61] Anurag Ranjan, Kwang Moo Yi, Jen-Hao Rick Chang, and Oncel Tuzel. Facelit: Neural 3d relightable faces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8619–8628, 2023. 8"
      ]
    }
  ]
}