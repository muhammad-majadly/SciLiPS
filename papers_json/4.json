{
  "paper_id": "4",
  "paper_title": "Information Fusion in LLMs",
  "sections": [
    {
      "section": "FrontMatter",
      "chunks": [
        "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 267–278 August 11-16, 2024 ©2024 Association for Computational Linguistics SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs Yebowen Hu,† Kaiqiang Song,‡ Sangwoo Cho,‡ Xiaoyang Wang,‡ Hassan Foroosh,† Dong Yu,‡ Fei Liu§ †University of Central Florida ‡Tencent AI Lab, Bellevue, WA §Emory University {yebowen.hu, hassan.foroosh}@ucf.edu {riversong, swcho, shawnxywang, dyu}@global.tencent.com fei.liu@emory.edu"
      ]
    },
    {
      "section": "Abstract",
      "chunks": [
        "Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, SportsMetrics, introduces a new mechanism for assessing LLMs’ numerical reasoning and fusion skills. 1"
      ]
    },
    {
      "section": "Introduction",
      "chunks": [
        "Large language models (LLMs) are more powerful than ever. OpenAI’s GPT-4 Turbo (2023) features a 128k context window, allowing it to process over 300 pages of text in a single prompt. Claude v2.1 (2023) steps it up with a 200k token window, equivalent to roughly 150,000 words or more than"
      ]
    },
    {
      "section": "500 pages. Mistral AI (2023) has created a sparse",
      "chunks": [
        "mixture of experts model capable of processing up to 32k tokens. The developments suggest language models can now engage with vast amounts of text content and data, opening doors to exciting new applications in various domains. One of the most promising uses of LLMs is in handling a combination of unstructured texts and structured data. For example, determining if a patient can be discharged from the hospital may involve reviewing doctor notes, radiology and pathology reports, lab results, and other records that blend Cade Cunningham misses driving floating jump shot 77 11:02 Jalen Duren offensive rebound 77 11:00 Alec Burks makes 28-foot three point jumper (Jalen Duren assists) 77 10:59 Bulls Full timeout 77 10:59 Zach LaVine makes 27-foot three point jumper (Ayo Dosunmu assists) 80 10:42 Jalen Duren makes 5-foot dunk (Cade Cunningham assists) 80 10:26 Zach LaVine makes 27-foot three pointer (Ayo Dosunmu assists) 83 10:16 Alec Burks makes 28-foot three point jumper (Cade Cunningham assists) 10:01 Zach LaVine misses 27-foot three point pullup jump shot 83 9:46 Ayo Dosunmu offensive rebound 83 9:45 Alec Burks blocks Ayo Dosunmu's two point shot 83 83 87 87 90 90 90 92 92 95 95 95 95 9:47 Play-by-Play Descriptions Detroit Pistons Cade Cunningham Jalen Duren Alec Burks ... Chicago Bulls Zach LaVine Ayo Dosunmu Nikola Vucevic ... Jalen Duren had __ points and __ rebounds as the Detroit Pistons overcame a career-high __ points from Zach LaVine to beat the Chicago Bulls __-__ on Saturday night. Team-Player Data Game Recap Figure 1: Play-by-plays of an NBA game. We include timestamps, player actions, team affiliations and a game recap. Total points for both teams are indicated in dotted circles and are withheld from LLMs. text and structured data (Adams et al., 2021; Bardhan et al., 2022; Cai et al., 2022, 2023; Veen et al., 2023; Ben Abacha et al., 2023); LLM Assistants for online shopping need to process product catalogs, sales transactions, and customer queries (Brynjolfsson et al., 2023; Loten, 2023). Yet, summarizing key details from a mix of unstructured and structured sources remains a considerable challenge. An LLM must navigate text descriptions, link entities, aggregate numbers, handle discrepancies, and beyond. Information fusion focuses on synthesizing information from multiple textual sources to derive meaningful conclusions (Barzilay et al., 1999). Current approaches involve summarizing multiple text documents, providing concise answers to user queries, and integrating summarization with natural language inference to deduce information (Bhaskar et al., 2023; Caciularu et al., 2023; Sprague et al., 267 Chicago Bulls"
      ]
    },
    {
      "section": "2 Points",
      "chunks": [
        "Field Goal (Inside the Three-Point Line): A player makes a basket from inside the three-point line. Three-Point Field Goal: A player makes a basket from beyond the three-point line. Free Throw: After certain types of fouls, a player may be awarded one or more free throws. Scoring Events Before"
      ]
    },
    {
      "section": "1 Point",
      "chunks": [
        "After"
      ]
    },
    {
      "section": "1 Point",
      "chunks": [
        "Team Affiliations (Before) Team Affiliations (After) Patrick Williams Demar DeRozan Nikola Vucevic Coby White Zach LaVine … Detroit Pistons Ausar Thompson Isaiah Stewart Jalen Duren Cade Cunningham Killian Hayes … Cade Cunningham misses driving floating jump shot 11:02 Jalen Duren offensive rebound 11:00 Alec Burks makes 28-foot three point jumper (Jalen Duren assists) 10:59 Bulls Full timeout 10:59 Zach LaVine makes 27-foot three point jumper (Ayo Dosunmu assists) 10:42 Jalen Duren makes 5-foot dunk (Cade Cunningham assists) 10:26 Zach LaVine makes 27-foot three pointer (Ayo Dosunmu assists) 10:16 Alec Burks makes 28-foot three point jumper (Cade Cunningham assists) 10:01 Zach LaVine misses 27-foot three point pullup jump shot 9:46 Ayo Dosunmu offensive rebound 9:45 Alec Burks blocks Ayo Dosunmu's two point shot 9:47 Play-by-Play Descriptions Chicago Bulls Points: 77 Rebounds: 28 Assists: … Detroit Pistons Points: 87 Rebounds: 48 Assists: … Key Stats (Before) Chicago Bulls Demar DeRozan Nikola Vucevic Coby White Zach LaVine … Detroit Pistons Ausar Thompson Isaiah Stewart Jalen Duren Cade Cunningham Killian Hayes … Patrick Williams Chicago Bulls Points: 83 Rebounds: 29 Assists: … Detroit Pistons Points: 95 Rebounds: 49 Assists: … Key Stats (After) Figure 2: (TOP LEFT) We examine the impact of changing game rules on final scores. For basketball, scoring events such as free throws, three-pointers, field goals, vary from 1 to 3 points. We ask LLMs to maintain these scoring events but under a new rule where each is worth only 1 point. (BOTTOM LEFT) We randomly swapped player team affiliations in the table without altering the game’s play-by-play records. (RIGHT) LLMs are provided with detailed play-by-play descriptions of a sports game and player team affiliations. Their job is to use this information to update key game statistics in a JSON format. 2022; Bostrom et al., 2022). The output is often a short text summary, the quality of which is difficult to evaluate (Deutsch et al., 2021). In contrast, our approach emphasizes the numerical aspect of information fusion (Geva et al., 2020; Zhu et al., 2021; Zhao et al., 2023; Reddy et al., 2024). We enable the LLM to navigate through lengthy texts, gather crucial statistics, and develop a working memory to manage complex data queries. We introduce SportsMetrics, a benchmark designed to assess LLMs’ abilities in numerical reasoning and data fusion. This benchmark provides LLMs with detailed, play-by-play descriptions of sports games, including timestamps, player actions, and team affiliations, as illustrated in Figure 1. We focus on four novel tasks to evaluate LLMs in adversarial scenarios: (a) adapting to new game rules, (b) handling lengthy game descriptions, (c) managing scrambled game narratives, and (d) analyzing critical statistics in game summaries.",
        "E.g., an LLM might be asked to complete a basketball game recap by inserting missing key statistics, which requires the development of a working memory for game stats and reasoning skills. Our SportsMetrics benchmark presents three main benefits. First, it leverages sports data, including team-player affiliations and play-by-play details; they are dynamic narratives that LLMs cannot easily memorize. Second, it allows us to evaluate LLMs’ ability to track key statistics such as team points, assists, blocks, steals, and more, while also offering an overall game efficiency score for direct LLM comparison. Lastly, its use of widely understood sports terminology makes it more accessible to researchers than specialized medical language, making it an ideal benchmarking tool. While our current focus is on English, SportsMetrics also holds promise for multilingual applications. 2"
      ]
    },
    {
      "section": "Related Work",
      "chunks": [
        "There is a growing need for a benchmark to evaluate LLMs’ information fusion capabilities, which offers clear, quantitative scores for comparing various LLMs. For example, Chatbot Arena (Zheng et al., 2023) utilizes Elo ratings (Boubdir et al., 2023), MT-Bench comprises of 80 multi-turn questions, and MMLU focuses on a model’s multitask accuracy across 57 tasks (Hendrycks et al., 2021). Multi-document summarization offers a promising benchmark (Lebanoff et al., 2021; Huang et al., 2021; Wang et al., 2022; Xu et al., 2023). However, developing a summary scoring system poses challenges due to variables such as summary length, content coverage, and faithfulness (Cao et al., 2022; Liu et al., 2023c; Krishna et al., 2023; Hu et al., 2023; Li et al., 2023; Xu et al., 2024; Joseph et al., 2024). Sports data, which combines static knowledge with player dynamics, presents an untapped opportunity for benchmarking LLMs. Combining information from a blend of textual and numerical records poses a significant challenge. In traditional multi-document summarization, the system creates a concise summary from a set of topically related documents. Giorgi et al. (Giorgi et al., 2023) show that this task remains difficult in an “open-domain” setting, where the document set is generated by a retriever and may include irrelevant information. With the growing popularity of 268 Game Score = (Points) + 0.4 x (Field Goals Made) - 0.7 x (Field Goals Attempted) - 0.4 x (Free Throws Attempted - Free Throws Made) + 0.7 x (Offensive Rebounds) + 0.3 x (Defensive Rebounds) + (Steals) + 0.7 x (Assists) + 0.7 x (Blocks) - 0.4 x (Personal Fouls) - (Turnovers) Passing Efficiency = { 8.4 x (Yards) + 330 x (Touchdowns) - 200 x (Interceptions) + 100 x (Completions) } / Attempts Yards = Number of Yards Passing Touchdowns = Number of Passes for Touchdowns Interceptions = Number of Passes intercepted Completions / Attempts = Number of Passes Completed / Attempted The NBA Game Score, developed by former ESPN writer John Hollinger, provide a rough measure of a player's productivity in a basketball game. “It takes into account both positive contributions (such as points, rebounds, and assists) and negative ones (such as missed shots and turnovers). It's a useful tool for quickly comparing players' performances.” Source: https://www.nbastuffer.com/analytics101/game-score In 1979, the NCAA created the Passing Efficiency formula with specific scaling factors to ensure an average passer would have a rating of exactly 100 for yards-per-attempt and completion percentage. “The factors 330 (3.3 times touchdown percentage) and 200 (2.0 times interception percentage) were selected so that they would balance each other out for an average player.",
        "While the NCAA and NFL formulas are essentially similar, the NFL's use of \"caps\" makes its formula a bit more complex to calculate.” Source: https://stassen.com/football/pass-eff/ Passing Efficiency (PE) Game Score (GS) Figure 3: We adopt the NBA’s Game Score, originally designed for player evaluation, to measure a team’s overall efficiency. For American football, we apply NCAA’s Passing Efficiency formula. retrieval-augmented generation (RAG) (Karpukhin et al., 2020; Liu et al., 2022), there is an increasing need to accurately fuse information from various sources. We explore information fusion by examining how LLMs cross-reference players and actions, and aggregate data across play-by-play descriptions to compile key game statistics. Our work relates to numerical reasoning, which uses arithmetic reasoning to tackle mathematical word problems. Prior datasets in this area include MathQA (Amini et al., 2019), GSM8k (Cobbe et al., 2021), SVAMP (Patel et al., 2021), TATQA (Zhu et al., 2021), FinQA (Chen et al., 2022), MATH (Liu et al., 2023d), DocMath-Eval (Zhao et al., 2023), TABMWP (Lu et al., 2023) and more, many allowing models to generate answer explanations. The problems typically have brief descriptions, with the challenge lying in creating an expression tree and applying arithmetic knowledge. In contrast, our approach focuses on assessing LLMs’ ability to track key statistics across extremely long contexts. Sports data has been utilized in various natural language tasks, including data-to-text generation for sports games (Lareau et al., 2011; Zhang et al., 2016; Wiseman et al., 2017; van der Lee et al., 2017; Puduppully et al., 2019), real-time game summarization from live commentaries (Edouard et al., 2017; Huang et al., 2020); and other aspects such as sports commentator bias (Merullo et al., 2019). Beyond sports, there’s significant interest in annotating and analyzing large-scale game-related corpora, such as reviews and gameplay logs, and summarizing gameplay commentaries (Lukin, 2020; Kicikoglu et al., 2020; Gu et al., 2022; Furman et al., 2022). We anticipate that insights from our SportsMetrics benchmark will benefit these areas, enhancing our understanding of game narratives and player dynamics. 3 The SportsMetrics Benchmark We collect NBA and NFL play-by-play data from ESPN.com. The descriptions capture the essence of each game. They are typically written by ESPN’s sports journalists, who are experts in their respective sports. We reached out to ESPN as necessary to ensure adherence to their data policies. In Figure 1, we use “time” to indicate the exact moment of each action on the game clock, while “play” details the actions occurring at those times. Scoring actions, which change the game’s score, are identified but not disclosed to LLMs during our experiments, as are team points. Additionally, we collect data on players’ team affiliations and the game’s box scores for our analysis.",
        "Our task requires LLMs to track key stats across thousands of play-by-play records, which is a nontrivial effort. An ideal LLM needs to associate each action with the right player and their team in order to calculate team-level statistics. It must also monitor multiple key statistics simultaneously, such as field goals, free throws, rebounds, assists, blocks, steals, personal fouls, and turnovers in a basketball game. We believe an LLM’s ability to summarize key details and fill in the missing statistics in game summaries demonstrates its capabilities in data fusion and numerical reasoning. We need a comprehensive scoring metric to evaluate LLMs’ ability to monitor key game statistics. Simply reporting individual metrics such as team points, rebounds, assists, and blocks for each team is inefficient and does not provide a holistic view of game analysis. To address this, we employ expertdeveloped team statistics formulas, as illustrated in Figure 3. We adopt the NBA’s “Game Score” by John Hollinger, originally for player evaluation, to measure a team’s overall effectiveness in basketball. It considers both positive (points, rebounds, 269 You are a helpful assistant tasked with analyzing sports games. You have been given play-by-play breakdowns of a basketball game between two teams. The \"Time\" column shows the exact time on the game clock when each play took place. The game clock counts down, so this column displays times in a descending order. The \"Play\" column describes the action that happened at the respective times. It provides details of specific plays, movements, and outcomes on the court. Team players are listed in two rows, each row representing one of the two basketball teams involved in the game. Your task is to fill in the missing key statistics from a basketball game recap. Each missing statistic is marked with '___'. Game Recap: ```Jalen Duren had ___ points and ___ rebounds as the Detroit Pistons overcame a careerhigh ___ points from Zach LaVine to beat the Chicago Bulls ___-___ on Saturday night.``` First, create an internal memory as a JSON object. Initially, this JSON object only has placeholders for team points, like this: Initial Memory: {\"Chicago Bulls\": {\"points\": null}, \"Detroit Pistons\": {\"points\": null}} Next, add the necessary key game or player statistics to the working memory to complete the missing information. These statistics might include categories such as 'field goals made', 'field goals attempted', 'free throws made', 'free throws attempted', 'rebounds', 'assists', 'blocks', 'steals', 'points' and others. This JSON object will later be populated with relevant data that will be used to fill in the blanks. Working Memory Example: {\"Jalen Duren\": {\"points\": null, \"rebounds\": null}, \"Zach LaVine\": {\"points\": null}, \"Chicago Bulls\": {\"points\": null}, \"Detroit Pistons\": {\"points\": null}} Now, you will be given a new game recap.",
        "Your goal is to create a working memory as a JSON object, which can be used to fill in the missing key statistics in the Game Recap. Game Recap: ```{GAME-RECAP}``` Can this working memory, represented as a JSON object, effectively complete the missing key statistics for the given game recap? If yes, just respond with the working memory. If not, improve the memory structure. Do not take into account or modify the NULL value. Game Recap: ```Franz Wagner scored 24 of his ___ points in the second half, Paolo Banchero added ___ points, and the Orlando Magic overcame Nikola Jokic's triple-double Wednesday night to record their fifth straight victory, ___-___ over the Denver Nuggets.``` Initially, you are given a JSON object where all values are set to null. Based on the provided play-by-play breakdown and teamplayer data, you will update these key statistics in JSON format. {\"Jalen Duren\": {\"points\": null, \"rebounds\": null}, \"Zach LaVine\": {\"points\": null}, \"Chicago Bulls\": {\"points\": null}, \"Detroit Pistons\": {\"points\": null}} Your task is to complete the missing key statistics from a basketball game recap. You'll fill in the blanks using only information from the working memory, which is represented as a JSON object containing the essential game or player statistics. Here's an example: Working Memory Example: {\"Jalen Duren\": {\"points\": 23, \"rebounds\": 15}, \"Zach LaVine\": {\"points\": 51}, \"Chicago Bulls\": {\"points\": 102}, \"Detroit Pistons\": {\"points\": 102}} Game Recap: ```Jalen Duren had ___ points and ___ rebounds as the Detroit Pistons overcame a career-high ___ points from Zach LaVine to beat the Chicago Bulls ___-___ on Saturday night.``` Output: ```Jalen Duren had 23 points and 15 rebounds as the Detroit Pistons overcame a career-high 51 points from Zach LaVine to beat the Chicago Bulls 118-102 on Saturday night.``` Now, you will be given a new game recap and its working memory, represented as a JSON object. Working Memory: {WORKING-MEMORY} Game Recap: ```{GAME-RECAP}``` Output: SYSTEM BUILDING A WORKING MEMORY KEY STATS REFLECTING TRACKING Figure 4: An LLM fills in missing key statistics in game summaries through a three-step process. Initially, the LLM creates an internal JSON object as its memory. It then enriches this memory by adding necessary game or player statistics, where all values are set to null, and further reflects on whether this memory is sufficient to accomplish the task. Lastly, the LLM uses detailed play-by-play and team-player data to update the JSON object’s values; it finally utilizes this updated memory to fill in the blanks in the game summary. assists) and negative (missed shots, turnovers) factors. For American football, we apply NCAA’s “Passing Efficiency” formula, as the NFL rule is more complex. In the following sections, we evaluate LLMs under different adversarial scenarios to assess their robustness. 3.1 Long-Form Game Narratives We begin by examining LLMs’ ability to reason over long contexts.",
        "For example, Liu et al. (2023b) introduced two tasks, multi-document QA and keyvalue retrieval, which require the model to identify relevant information within long contexts. They found that LLMs’ performance significantly deteriorates when they have to access relevant information in the middle of long contexts. Our study goes a step further, requiring LLMs to not only identify relevant actions but also accurately track statistics throughout long-form game narratives. In this task, each LLM is provided detailed playby-play descriptions of a sports game, including timestamps and specific actions. The players’ team affiliations are listed in two rows, representing each team. The LLM’s task is to use the play-by-plays to update key game statistics within a JSON object, initially filled with null values. For long-context LLMs such as GPT-4 Turbo, Claude 2.1, and Gemini Pro (Anil et al., 2023), we provide the entire game’s data at once for processing. For LLMs with 4k or 8k tokens context, we break the game down into four quarters. The LLM gathers statistics quarter by quarter. It generates a JSON object that holds values from each quarter. These are then added up to derive game-level statistics. We use comprehensive, expert-devised formulas to evaluate LLMs in tracking game statistics. For NBA games, we monitor 11 key statistics: team points, field goals made, field goals attempted, free throws made, free throws attempted, offensive rebounds, defensive rebounds, steals, assists, blocks, and personal fouls.1 Moreover, we calculate ‘Game Score’ to measure a team’s overall effectiveness in basketball. For NFL games, we track passing yards, touchdowns, interceptions, and pass completions and attempts. These additional stats allow for the computation of ‘Passing Efficiency.’ 3.2 The Impact of Changing Game Rules It is important to understand LLMs’ ability to make decisions under changing world rules. LLMs possess extensive knowledge from pretraining on the Internet, books, and other texts. This knowledge, held in their parametric memory, might not always align with the external evidence given to the model. 1We exclude turnovers from tracking due to limitations in the data. Play-by-play descriptions may not capture every turnover, making it difficult for the model to track them accurately. When necessary, we rely on the ground-truth Turnover count from the box score to calculate the Game Score. 270 Model Release Date Context Len Input Output Organization Claude-2.1 11.21.2023 200,000 $.008 $.024 Anthropic GPT-4-1106-preview 11.06.2023 128,000 $.01 $.03 OpenAI Gemini-Pro 12.06.2023 32,000 $.001 $.002 Deepmind GPT-3.5-Turbo-1106 11.06.2023 16,385 $.001 $.002 OpenAI Mistral-7B-Instruct-v0.1 09.27.2023 8,000 — — Mistral GPT-3.5-Turbo-0613 06.13.2023 4,096 $.0015 $.0015 OpenAI Llama-2-13B-Chat 07.18.2023 4,096 — — Meta Table 1: LLMs used in this study. Prices are per 1,000 tokens. Llama-2 and Mistral-7B are free and open-source.",
        "Therefore, LLMs need to adjust to changing rules. Xie et al. (2023) highlight the importance of knowing when to trust a model’s own knowledge. Meng et al. (2023) explored finetuning LLMs to alter specific knowledge, but such changes are often irreversible. Here, we propose two tasks to evaluate LLMs’ abilities in adapting to new game rules. New Scoring Rules We examine the impact of changing game rules on final scores. For basketball, scoring events such as free throws, three-pointers, field goals, vary from 1 to 3 points. We ask LLMs to maintain these scoring events but under a new rule where each action is worth only 1 point. This contradicts LLMs’ existing knowledge, challenging them to recalibrate game scores accordingly. Ground-truth scores under this rule are obtained by counting the total number of scoring actions to determine each team’s total points. Player Swapping We randomly swapped player team affiliations in the table without changing the game’s play-by-play records, as illustrated in Figure 2. Ground-truth team scores for this task are calculated by summing individual player scores under their new affiliations. This task allows us to vary the degree of conflict between the model’s existing knowledge and the provided evidence. Swapping more players increases the task’s difficulty. 3.3 Robustness Against Noise Shuffling Play-by-Plays We present an adversarial challenge where we shuffle basketball game play-by-play descriptions and then ask LLMs to track the total points of each team. We choose basketball because adjacent actions in this context do not show strong causal relationships. Changing the sequence of scoring actions does not affect the teams’ total points. We anticipate that long-context LLMs will produce consistent or similar final game scores. To avoid confusing the model, we maintain the original order of timestamps. We can also adjust the frequency of scoring plays { \"Denver Nuggets\": { \"points\": \"victory_score\" }, \"New York Knicks\": { \"points\": \"defeat_score\" }, \"Ty Lawson\": { \"points\": \"Lawson_score\" }, \"Randy Foye\": { \"points\": \"Foye_score\" } } { \"Denver Nuggets\": { \"points\": null }, \"New York Knicks\": { \"points\": null }, \"Ty Lawson\": { \"points\": null }, \"Randy Foye\": { \"points\": null } } { \"Ty Lawson\": { \"points\": 25, \"assists\": 7 }, \"Randy Foye\": { \"points\": 18, \"rebounds\": 5 }, \"New York Knicks\": { \"points\": 98, \"rebounds\": 42 }, \"Denver Nuggets\": { \"points\": 103, \"rebounds\": 45 } } GPT-4 Turbo Claude-2.1 LLaMA-2 Figure 5: Effective working memory is key in this task. The variance in memory structure arises because we allowed each LLM to generate its JSON object as working memory, without enforcing a uniform schema. This step allows us to explore how each model organizes its memory to complete the task. Note that Claude’s ‘null’ values represent an initial state rather than an inability to aggregate information. in a game, making it more or less challenging for LLMs to process the narrative.",
        "By choosing a probability p from a set of values {-50%, -20%, 0, +20%, +50%}, we can either duplicate non-scoring plays (thereby decreasing scoring play density and extending the game narrative) or remove them (increasing scoring play density). Further, to test the LLM’s inherent knowledge, we randomly select players from each team in NFL games and assign them new names, such as characters from science fictions. This approach evaluates the model’s ability to adapt to changes in player identities. These alterations do not introduce new players or change the total points scored in the game; it simply varies the narrative’s complexity. 3.4 Planning for Complex Data Queries In this task, LLMs fill in missing key statistics from game summaries (e.g., from ESPN). The process unfolds in three steps, illustrated in Figure 4. First, the LLM creates an internal JSON object memory, initially with placeholders for team points. Next, it enriches this memory by adding crucial game or 271 System ∆GScore ∆Points ∆NewRule ∆Swap ∆Shuffle Long-Context GPT-3.5-Turbo-1106 33.50 9.45 14.10 13.53 9.89 (16k+ Tokens) Gemini-Pro 32.30 17.62 25.99 17.78 14.85 GPT-4-1106-preview 51.97 25.17 14.55 39.91 49.57 Claude-2.1 55.16 21.73 22.28 17.12 31.11 Standard GPT-3.5-Turbo-0613 114.28 94.34 18.22 88.25 89.11 (4k to 8k Tokens) Mistral-7B-Instuct 123.49 73.53 26.79 70.69 103.24 Llama-2-13B-Chat 110.69 70.77 83.04 53.98 81.09 Table 2: Average absolute difference between model predictions and the actual scores on NBA data for tracking a team’s total points (Points) and all key game statistics (GScore). Moreover, we evaluate LLMs’ performance in three adversarial scenarios: ∆NewRule, ∆Swap and ∆Shuffle. player statistics. During a self-reflection phase, the LLM evaluates if its JSON memory can accurately complete the missing statistics for the given game recap. If it can, it responds with this memory; if not, it further refines the memory structure. Finally, using the detailed play-by-play and team-player data, the LLM updates the key statistics in the JSON format, then uses this information to fill in the blanks in the game summary. Figure 5 illustrates various LLM attempts building a memory. Our task is inspired by several studies on LLM planning. Unlike LLM+P which uses the Planning Domain Definition Language (PDDL) for problemsolving (Liu et al., 2023a), we simplify the process by requiring only a valid JSON object for working memory. Relevant studies such as Reflexion (Shinn et al., 2023), ReAct (Yao et al., 2023b), and Treeof-thought (Yao et al., 2023a) have also influenced our approach. Sumers et al. (2023) have developed a framework for integrating planning into LLM agents. Prior studies have focused on ALFWorld’s interactive TextWorld environments. Our method are focused on sports, which involves masking key statistics in game recaps by sports journalists, then converting them into task data points for LLMs.",
        "We assess LLMs by their accuracy in filling in missing key statistics from game summaries. 4"
      ]
    },
    {
      "section": "Experiments",
      "chunks": [
        "We evaluate various LLMs in our SportsMetrics benchmark. These models are listed in Table 1 and split into two categories: long-context LLMs, capable of processing over 16k tokens, and standard LLMs, handling 4k to 8k tokens. Our evaluation focuses on their ability to accurately track a team’s total points (Points) and all key game statistics (GameScore). We measure the average absolute difference (deviation) between the models’ predictions and the actual box scores, denoted as ∆Points and ∆GScore, respectively.2 Our dataset comprises 28,492 NBA games and 5,867 NFL games spanning two decades from 2002 to 2023, available through ESPN’s archives. We randomly selected 100 games from each sport for our test set. On average, NBA games contain 466 plays and NFL games 173 plays. An average NBA game includes 6,229 tokens, while an NFL game has 6,166 tokens, with maximum lengths reaching 7,322 and 7,659 tokens, respectively. LLMs’ ability to integrate information is tested under three adversarial scenarios: (a) ‘NewRule,’ which assigns every scoring action just one point, regardless of the move, (b) ‘Swap’ which randomly selects two players from each team to swap their affiliations in the team-player table, (c) ‘Shuffle,’ which duplicates any non-scoring action with a 20% chance (p=0.2) before shuffling the play-byplays. We assess LLMs’ performance in these scenarios and report the deviation of predicted team points from actual scores as ∆NewRule, ∆Swap and ∆Shuffle. In Table 2, we present our findings from the NBA section of our dataset. With ∆representing the gap between predictions and actual scores, smaller values are preferable. We find that long-context LLMs significantly outperform standard LLMs across all tasks. GPT-3.5-Turbo-1106 leads in performance in every task except for ∆GScore, where GeminiPro has a slight edge. Long-context models have been released recently in late 2023. These results demonstrate their remarkable ability in identifying relevant actions from game play-by-plays, attribut2∆GScore consistently shows higher values compared to ∆Points because it goes beyond counting a team’s points. It offers a full game analysis by requiring the LLM to consolidate key statistics such as points, rebounds, steals, assists and more into an overall score. Considering only team points is insufficient, especially in sports like soccer where scoring is rare. When necessary, we can convert GameScore to points by zeroing out other stats.",
        "272 System ∆Yards ∆ATT ∆COMP ∆TD ∆INT ∆PE Long-Context GPT-4-1106-Preview 34.77 4.44 2.96 0.17 0.13 14.33 (16k+ Tokens) Claude-2.1 52.53 5.43 3.75 0.29 0.22 17.53 GPT-3.5-Turbo-1106 64.87 7.80 4.73 0.49 0.30 18.43 Gemini-Pro 85.14 12.68 6.87 0.83 0.52 26.17 Standard GPT-3.5-0613 105.68 24.11 15.80 1.09 0.60 89.56 (4k to 8k Tokens) Llama-2-13B-Chat 244.48 22.37 19.66 1.47 1.03 191.76 Mistral-7B-Instuct 119.31 17.64 9.05 1.23 0.69 202.07 Table 3: Discrepancies between model predictions and actual scores on NFL stats, including yards (Yards), attempts (ATT), completions (COMP), touchdowns (TD), interceptions (INT) and passing efficiency (PE). 0 20 40 60 80 100 Games: Sorted by Length 25 50 75 100 125 150 175 Game Score Deviation gpt-3.5-turbo-0613 gpt-3.5-turbo-1106 llama-2-13b-chat mistral-7b-instruct gemini-pro gpt-4-1106-preview claude-2.1 Figure 6: We organize games based on the length of their play-by-plays, with the x-axis showing the games and the y-axis the deviation scores; lower scores indicate better performance. GPT-3.5-Turbo-1106 and GeminiPro stand out here, maintaining nearly flat curves. ing each action to the right player and team, and aggregating numerical data to compute final team points and GameScore. This requires a level of numerical reasoning that humans are adept at but it is still new territory for LLMs. In Figure 6, we organize games based on the length (number of tokens) of their play-by-play descriptions, with the x-axis showing the games and the y-axis the deviation scores from various LLMs, where lower scores indicate better performance. We perform a regression analysis to demonstrate each LLM’s trend in handling games of increasing length. GPT-3.5-Turbo-1106 and Gemini-Pro stand out, maintaining nearly flat curves, which corresponds with their superior performance as shown in Table 2. By contrast, GPT-4-1106-Preview does well in shorter games but face difficulties in aggregating key statistics for longer games. Additionally, 79% its returned JSON objects contain zeros or null values, contributing to its unsatisfying performance on this task. We note that basketball teams typically score between 100 to 120 points. Our findings show that the smallest prediction gap for ∆Points is 9.45, while the largest can exceed 100. This indicates the difficulty in accurately tracking key game statistics over long contexts, as standard LLMs can produce predictions significantly off from actual scores due to hallucinations. Among the three adversarial scenarios, the New Rule is relatively simpler as it requires LLMs to assign one point to every scoring action, focusing on counting these actions instead of distinguishing between types (3-pointers vs. free throws) and adding them up for a team’s score. In this scenario, Llama-2-13B-Chat scores lower than all other LLMs. In Table 3, we present NFL data findings.",
        "American football’s play-by-plays have demonstrated a sequential nature, we cannot apply tests like New Rule, Swap, or Shuffle as with basketball games. Instead, we measure how model predictions deviate from actual scores on key game statistics, including yards (∆Yards), attempts (∆ATT), completions (∆COMP), touchdowns (∆TD), and interceptions (∆INT). We also combine them into Passing Efficiency (∆PE) for a holistic game analysis. Our results suggest that long-context LLMs greatly surpass standard models, with GPT-4-1106-Preview taking the lead, followed by Claude-2.1 and GPT3.5-Turbo-1106. Particularly, passing yards are vital in the NFL games, often leading to scoring opportunities like touchdowns and field goals. On average, NFL teams average 200 to 250 passing yards per game. We find that the top model, GPT-4-1106-Preview, exhibits a 34.77-yard discrepancy in passing yards prediction, while the open-source Llama-2-13BChat lags significantly in comparison. This highlights the difficulty of tracking passing yards, a task even more challenging than summarizing basketball points, with most models struggling to accurately aggregate such data. 273 0.5 0.2 0.0 0.2 0.5 Probability 20 40 60 80 100 120 Game Score Deviation 1 2 3 4 5 Number of Swapped Players 10 20 30 40 50 60 70 80 90 Game Score Deviation 1 2 3 4 5 Number of Replaced Players 50 100 150 200 250 Game Score Deviation gpt-3.5-turbo-0613 gpt-3.5-turbo-1106 llama-2-13b-chat mistral-7b-instruct gemini-pro gpt-4-1106-preview claude-2.1 Figure 7: (LEFT) We adjust the difficulty of identifying scoring events by either removing or duplicating non-scoring events. Moreover, we randomly swapped n players’ affiliations in the team-player table (MIDDLE) and replaced n players’ names with science fiction characters (RIGHT), all without changing the play-by-play texts. 0 2 4 6 8 10 12 14 16 Accuracy (%) 5.22 7.51 7.94 7.94 9.12 11.24 16.72 gpt-3.5-turbo-0613 gpt-3.5-turbo-1106 llama-2-13b-chat mistral-7b-instruct gemini-pro gpt-4-1106-preview claude-2.1 Figure 8: Accuracy of various LLMs in filling missing key statistics from basketball game recaps. Claude-2.1 shows strong performance, while Mistral-7B-Instruct achieves the highest accuracy among standard LLMs. Our results suggest that the difference in performance between GPT-3.5-Turbo-1106 and GPT-4 across basketball and football games stems from the scoring frequency in each sport. Basketball’s frequent scoring presents a challenge for GPT-41106-Preview to track all actions, while football, with less frequent scoring, is somewhat easier for the model to track. GPT-4-1106-Preview is optimized for handling extremely long contexts and it is less accurate in tracking frequent scoring. This distinct characteristic accounts for the varied performance of both models. In Figure 7, we test LLMs’ robustness against adversarial conditions.",
        "In the left subfigure, we vary the difficulty of identifying scoring events by either dropping or duplicating non-scoring events. E.g., at probability p=-0.5, we eliminate any non-scoring event with a 50% chance; at p=0.2, we duplicate any non-scoring event with a 20% chance, before shuffling the entire game description. The y-axis measures the deviation from the actual box score, with smaller values indicating better model performance. We observe that GPT-3.5-Turbo-1106 and Gemini-Pro perform the best, whose curves are quite flat, indicating their robustness to a varying level of noise in the play-by-plays. Overall, LLMs perform well when non-scoring events are removed, yet their performance drops as more non-scoring events are added, akin to searching for a needle in a larger haystack. Further, we randomly swapped n players’ affiliations in the team-player table and replaced n players’ names with science fiction characters, all without changing the play-by-play texts. Our findings are shown in the middle and right subfigures. We find that Claude-2.1, Gemini-Pro, and GPT-3.5Turbo-1106 are the top performers. Interestingly, renaming players significantly decreases all models’ performance. This suggests LLMs may use familiar basketball player names from their pretraining to guess team scores, rather than analyzing the actual play-by-plays. GPT-4-1106-Preview is the least adaptable to these adversarial conditions among the long-context LLMs. We also observe a notable performance disparity exists between opensource and proprietary LLMs. We assess the accuracy of various LLMs in completing missing key statistics from basketball game recaps. The types of missing data include a player’s total points, team scores, assists, rebounds, and other stats. An LLM must understand the recap’s context to precisely estimate the missing statistic. To do this, LLMs create a JSON object as its working memory. They then calculate the needed statistics using play-by-play and team-player data and use this memory object to fill in the blanks. Figure 8 presents the results of this task. Claude274 2.1 shows strong performance, while Mistral-7BInstruct achieves the highest accuracy among standard LLMs. This task requires that LLMs possess strong instruction-following capabilities to build an effective working memory. Figure 5 provides sample working memories from various LLMs. Although complex structures are possible, they increase the risk of errors when populating values. Models such as GPT-4-1106-Preview and Llama2-13B-Chat face difficulties in creating a working memory. They hallucinate field values or fail to accurately fill fields with aggregated values from play-by-play data. By contrast, Claude-2.1’s memory structure is the best in terms of efficiency, focusing on essential game statistics. Our task crucially evaluates LLMs’ memory management skills when handling complex data queries. 5"
      ]
    },
    {
      "section": "Conclusion",
      "chunks": [
        "We introduce SportsMetrics, a novel benchmark designed to evaluate LLMs in sports data analytics. It assess LLMs’ numerical reasoning and fusion abilities through challenges such as new game rules, lengthy descriptions, scrambled narratives and key stats analysis in game summaries. SportsMetrics highlights LLMs’ potential in fields such as multiplayer gaming and collaborative workspaces. "
      ]
    },
        {
      "section": "Limitations",
      "chunks": [
        "      Our research focuses on NBA and NFL games, which are major sports with rich datasets. We are interested in exploring the generalizability of our findings to other sports. For example, soccer and cricket have distinct play styles and rules, which might challenge LLMs in unique ways. Our study has explored multiple adversarial scenarios, such as new game rules and scrambled game narratives. Such drastic changes might be uncommon in realworld conditions, and the models’ ability to handle these scenarios might not translate to improved performance in other analytical tasks. Finally, our scoring system’s effectiveness in assessing LLMs’ numerical reasoning capabilities in different contexts, such as multiplayer online gaming or collaborative workspaces, remains to be validated. This study explores LLMs’ potential in sports analytics. It is important to recognize these limitations when applying our findings to broader contexts."
      ]
    },
    {
      "section": "Acknowledgements",
      "chunks": [
        "We are grateful to the reviewers for their insightful feedback, which has helped enhance the quality of our paper. This research has been partially supported by the NSF CAREER award, #2303655."
      ]
    }
  ]
}