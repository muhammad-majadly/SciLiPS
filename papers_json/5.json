{
  "paper_id": "5",
  "paper_title": "Enhancing Explainable Rating Prediction through Annotated Macro Concepts",
  "sections": [
    {
      "section": "FrontMatter",
      "chunks": [
        "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11736–11748 August 11-16, 2024 ©2024 Association for Computational Linguistics Enhancing Explainable Rating Prediction through Annotated Macro Concepts Huachi Zhou1, Shuang Zhou1, Hao Chen1, Ninghao Liu2, Fan Yang3, Xiao Huang1,∗ 1The Hong Kong Polytechnic University, Hong Kong, China 2University of Georgia, Georgia, USA 3University of Wake Forest, North Carolina, USA 1huachi.zhou@connect.polyu.hk, {csszhou,xiaohuang}@comp.polyu.edu.hk, 1sundaychenhao@gmail.com 2ninghao.liu@uga.edu 3yangfan@wfu.edu"
      ]
    },
    {
      "section": "Abstract",
      "chunks": [
        "Generating recommendation reasons for recommendation results is a long-standing problem because it is challenging to explain the underlying reasons for recommending an item based on user and item IDs. Existing models usually learn semantic embeddings for each user and item, and generate the reasons according to the embeddings of the user-item pair. However, user and item IDs do not carry inherent semantic meaning, thus the limited number of reviews cannot model users’ preferences and item characteristics effectively, negatively affecting the model generalization for unseen user-item pairs. To tackle the problem, we propose the Concept Enhanced Explainable Recommendation framework (CEER), which utilizes macro concepts as the intermediary to bridge the gap between the user/item embeddings and the recommendation reasons. Specifically, we maximize the information bottleneck to extract macro concepts from user-item reviews. Then, for recommended user-item pairs, we jointly train the concept embeddings with the user and item embeddings, and generate the explanation according to the concepts. Extensive experiments on three datasets verify the superiority of our CEER model. 1"
      ]
    },
    {
      "section": "Introduction",
      "chunks": [
        "In recommender systems, rating prediction is a crucial component for filtering user interest items from a vast pool of candidates. However, many recommender systems only offer rating prediction results without providing any explanations (Pan et al., 2022; Zhang et al., 2022). Personalized and appropriate explanations can greatly arouse user interest and enhance the overall user experience. Therefore, there is potential to deliver accurate rating predictions while generating personalized and suitable reasons for recommended items (Balog et al., 2019; Wang et al., 2022a). ∗Xiao Huang is the corresponding author. Current explainable rating prediction models typically involve learning user and item embeddings and then generating explanations based on these embeddings (Chen et al., 2019, 2020b). For instance, they attribute semantic meanings to each user and item ID by reconstructing the associated review sentence using user and item embeddings (Wang et al., 2023; Gao et al., 2019). CETP (Li et al., 2021a) goes a step further by incorporating knowledge graphs as auxiliary information and leveraging graph neural networks to enhance the item embeddings. Similarly, MMCT utilizes multi-modal information to aid in training the user and item embeddings (Liu et al., 2023). However, existing models still highly rely on the embeddings of user and item. Nevertheless, training user and item embeddings of explainable rate prediction face the following limitations: (i). training sparsity: The observed user-item pairs are sparse in comparison to all possible combinations. This sparsity issue prevents users and item embeddings from capturing sufficient semantic information by solely reconstructing the available reviews, thus negatively affecting the model’s generalization in generating explanations for unseen user-item pairs. (ii). Inference sparsity: The absence of available information in the inference stage makes it challenging to generate appropriate explanations. There are some efforts that explicitly put the collected item features as prompting information to alleviate this issue (Li et al., 2020b; Cheng et al., 2023). However, it is hard to predict in advance which specific features will appear in the explanations. It is challenging to solve the sparsity problems due to two reasons. (i) ID insufficient semantics: The initial embeddings for users and items lack semantic meanings, and are not effectively recognized by language models. The insufficient initial semantics exacerbate the learning of a detailed and accurate mapping to the semantic space. 11736 (ii) Mixed high-level semantics: The user reviews are complicated and may involve multiple highlevel semantics. For example, a review that “I hate this guitar since the quality of the guitar is bad.\" could be identified by three high-level semantics: “negative emotion”, “instrument”, and “quality”. It is difficult to assist language models in recognizing user focus by providing an item feature.",
        "To this end, we propose a framework named Concept Enhanced Explanation Recommendation (CEER). Our framework explicitly annotates highlevel semantics, i.e., macro concepts that abstract item characteristics with similar semantics in reviews and embeds the potentially matched macro concepts into the user and item embeddings. The mined macro concepts enrich the semantic meaning of user and item embeddings without requiring additional data to learn. Our key contributions are summarized below: • To improve the explanation generation, we leverage the macro concepts to address the issue of inadequate semantic information in user and item embeddings. • To build macro concepts, we discover the micro characteristics of the item attended by the user from the reviews by maximizing the information bottleneck. • We devise three tasks to enrich user and item embeddings motivated by integrating the macro concept into representations space. • Extensive experiments on three real-world datasets prove the superiority of the proposed framework CEER. 2 Preliminary Problem Formulation. We denote the user set and item set as U = {u1, u2, ..., u|U|} and V = {v1, v2, ..., v|V|}, respectively. The rating score rij assigned towards the interaction (ui, vj) characterizes the degree of user preference. The observed reviews are organized within a matrix X, where each element in the matrix represents a review and Xij represents the review for the rating rij. Xij is defined as a sentence denoted as [x1..., xm, ..., xM], where xm is the mth word in Xij. During inference, the available information for each interaction is limited to the user and item ID. In this work, we define explanation generation as a sequence-tosequence prediction task. Specifically, given a useritem pair, we apply the user and item embeddings as soft prompts, asking the transformer to generate explanations, where each word in the explanation is predicted given the preceding words and soft prompt. For example, given the soft prompt [ui, vj], the transformer is trained to predict the next word x1. After multiple generation steps, the output from the transformer forms the sequence [x1..., xm, ..., xM]. The notations are put into Appendix A.1 3"
      ]
    },
    {
      "section": "Methodology",
      "chunks": [
        "This section introduces two primary components of our framework: a macro concept annotator and an explanation generator. The first component involves two steps: identifying the informative micro characteristics of items in the reviews and using LLMs to annotate macro concepts. The second component utilizes three tasks based on the annotated concepts to enhance user and item embeddings and facilitate explanation generation. The architecture sketch is provided in Figure 1. 3.1 Macro Concept Annotation Micro Characteristics Identification. We represent each macro concept as a group of semantically similar micro characteristics of items that are carefully curated from the reviews. To serve as the micro characteristics, the selected words are required to be informative and capable of justifying the rationale behind recommendations. To measure the informativeness of the words, we need to first quantify the informative level of the words in the corpus. Pre-trained language models (PLMs) are trained on vast amounts of text data and can recognize which words are more informative in conveying the explanation. To learn the informative level quantification, we maximize the following objective, i.e., information bottleneck (TISHBY,"
      ]
    },
    {
      "section": "2000) M(Xij, rij):",
      "chunks": [
        "max M(Xij, rij) = max I(Tij, rij)−τ·I(Tij, Xij), (1) where I(·, ·) represents mutual information between two variables; τ is a trade-off hyperparameter; Tij ∈RM×d signifies the intermediate word representation from PLMs whose informative levels can be measured. Here, d denotes the word dimension in the frozen PLM, which, for instance, is 768 when using BERT (Devlin et al., 11737 Joint Learning I hate this guitar hate this guitar since Transformer Concept Emb. User Emb. Item Emb. Word Emb. ij X Noise minimize error ... ... user-item pair Macro Concept Annotation + ... ... s p m r          Seq2seq Training LLM Word Selection Positive Emotion Macro Concept Micro Characteristics like love favor prefer ... Negative Quality bad poor inferior deficient ... guitar violin piano cello ... I hate this guitar since the quality of the guitar is bad. I like this guitar. It is quite cheap for me. Reviews Users Items Encoder Macro Concept Annotation like guitar cheap hate guitar bad guitar Instrument bad inferior dislike hate Instrument Instrument Figure 1: The figure shows the proposed CEER framework. The lower part demonstrates how we annotate the macro concepts. The upper part shows how to leverage the annotated concepts to enrich user and item embeddings."
      ]
    },
    {
      "section": "2019) as the frozen PLM. In Eq. (1), the first term",
      "chunks": [
        "maximally preserves the information related to rating prediction while the second term (Alemi et al.,"
      ]
    },
    {
      "section": "2016) serves as the regularization term that constrains the amount of information in intermediate",
      "chunks": [
        "representations encoded from Xij. The variational approximation of the information bottleneck of traditional approaches applies restrictive constraints to the pre-trained language models as the second term (Tu and Li, 2022). The posterior distribution of the intermediate representations may not be standard Gaussian distribution (Tu et al., 2022). Some recent efforts have tried more flexible distributions, e.g., sample-based representations of variational distributions (Fang et al., 2019, 2022). They could learn expressive intermediate representations but are unable to explicitly quantify the informative level of the words. To achieve this goal, we inject noise into word representations (Guan et al., 2019). The extent to which these representations tolerate noise can serve as a reflection of their informativeness, as informative words play an important role in understanding the corpus and are sensitive to the introduced noise. To facilitate the second-term calculation, we synthesize the word representations with noise as Tij: Tij = (1 −σ) ⊙PLM(Xij) + σ ⊙ϵ, (2) where PLM(·) provides the original word representations from PLMs, and ϵ ∈RM×d denotes the random noise independently sampled from standard Gaussian distribution while the learnable vector σ ∈RM [0,1] controls the magnitude of noise. The minimization of I(Tij, Xij) is calculated as: min H(Xij) −H(Xij|Tij) ≈max log σ, (3) where we assume P(Xij|Tij) ≈P(Tij) and follows the multivariate Gaussian distribution with zero covariance. Next we could use 1 −σxm to identify the informative level of word xm in Xij. Then the informative level for each word in Xij is: [1 −σx1, ..., 1 −σxm, ..., 1 −σxM ] . (4) To calculate the first term, we transform word representations into ratings and minimize the difference between predictions and labels. In this process, we employ a rating-specific message-passing encoder for transformation (Shuai et al., 2022). After obtaining σ, we exploit the LLMs to perform the macro concept annotation for top k important micro characteristics. Here we use LLM ChatGPT, i.e., gpt-3.5-turbo and take one demonstration following (Lou et al., 2023; Zhang et al.,"
      ]
    },
    {
      "section": "2023) as an example:",
      "chunks": [
        "[Task Description] User: Organize the following keywords into groups based on their semantic relationships, and give a concept name to each group. [Demonstrations] Input: [Expensive, Cheap]. Output: {Economy: [Expensive, Cheap]} [Test Instance] Input: [Guitar, Hate, Quality]. Output: {Instrument: [Guitar], Negative Emotion: [Hate], Quality: [Quality]}. In this way, we obtain the macro concept labels for k informative micro characteristics from the historical reviews and create a padded concept label for the remaining words. The words in Xij are labeled as {cx1, ..., cxm, ..., cxM }, where cxm is the macro concept ID for word xm. 11738 3.2 Explanation Generator In this component, our goal is to utilize the annotated macro concepts to enrich user/item embeddings. Specifically, we embed the relationships between users/items and concepts, as well as concepts and reviews, in representations through three tasks. These three tasks collaborate to utilize macro concepts as an intermediary to align user and item embeddings with the review embeddings. 3.2.1 User/item-concept Relationship Modeling The first task user/item-concept relationship modeling utilizes user and item embeddings to aid in predicting macro concepts in the associated review. To train the prediction of macro concept distribution, we need to craft macro concept labels Wij = \u0002 w1, ..., wq, ..., w|C| \u0003 ∈R|C| for review Xij. The macro concept label wq is determined by the cumulative informative level of the words in Xij associated with that concept cq: M X m=1 I(cxm = cq)(1 −σxm), (5) where the indicator I(·) take value 1 if the condition holds otherwise 0. After obtaining the macro concept labels for review Xij, we come to train the user/item embeddings to predict the macro concept distribution ˆ Wij in the associated review, defined as: ˆ Wij = MLP(eui, evj), (6) where eui, evj denotes the user and item embeddings respectively. Next, we use Kullback-Leibler divergence loss to penalize the divergence between the predicted macro concept distribution and the labels, defined as: Lp = LKL( ˆ Wij||Wij) = |C| X q=1 wq · log wq ˆwq . (7) After training, the user and item embeddings could be utilized to recognize the potential macro concepts in the inference. Then, we obtain a composite concept embedding by the weighted sum of all macro concept embeddings with the predicted importance: P|C| q=1 ˆwqecq/ P|C| q=1 ˆwq. The composite macro concept embedding is appended behind the user and item embedding to instruct the transformer to generate relevant explanations. 3.2.2 Concept-review Relationship Modeling The second task concept-review relationship modeling brings the words in reviews and associated macro concept embeddings closer together. Through this task, we proceed to enhance the words in reviews’ awareness of the associated macro concept within the representation space. To reach this objective, we reduce the uncertainty about each word mapping to its associated macro concept.",
        "For simplicity, we use the inner product to compute the uncertainty and optimize it with the cross-entropy loss, driving the movement of word embeddings to the macro concept embedding: Lm = − M X m=1 |C| X q=1 I(cxm = cq) · log(pxm,cq), (8) where log(pxm,cq) is the uncertainty between the word xm and the macro concept cq, computed by inner product of embedding: pxm,cq = e⊤ xm · ecq. It encourages words and associated concepts to exhibit proximate representations. 3.2.3 User/item-review Relationship Modeling The third task user/item-review relationship modeling assists the transformer in considering words belonging to the same macro concepts when generating the next word. Traditional approaches employ the maximum likelihood function to reconstruct the original reviews. However, it ignores the diversity of the language expression. In practice, each position in the sentence could have multiple word choices, e.g., semantically similar words. To achieve this, we substitute the word in the review with multiple alternatives in the same macro concept. The loss can be reformulated as: Ls = −1 M M X m=1 (log p(xm) + α log p(x′ m)), (9) where x′ m denotes a randomly selected word within the same macro concept as xm and α denotes a hyper-parameter to control the loss term. Here, we only incorporate one substitution. More substitutions may take the risk of introducing noise and generating contextually inappropriate explanations. 3.2.4 Training Objective Besides the above tasks, we jointly perform rating prediction following previous efforts (Li et al., 2023b) using the loss function defined as: Lr = \u0000MLP(eui, evj) −rij \u00012 . (10) 11739 Finally, we unify these tasks in a linear combination form to train the transformer. The joint learning objective is indicated by: L = Ls + βLm + γLp + Lr, (11) where β, γ are trade-off hyper-parameters to balance the effect of different objectives of the model. 4"
      ]
    },
    {
      "section": "Experiments",
      "chunks": [
        "In this section, we perform comprehensive experiments to validate the effectiveness of CEER and gain insights into its behavior. We aim to answer the following research questions: RQ1: How well do the explanations and ratings generated by CEER and baselines align with the actual labels in terms of text quality? RQ2: How does the quality of explanations generated by all models in terms of interpretability? RQ3: How do different tasks contribute to the whole model performance? RQ4: What impact do different hyper-parameter settings of the proposed tasks have on CEER? RQ5: How does information bottleneck perform compared with other micro characteristics identification methods for macro concept annotation? Table 1: Detailed datasets statistics. Datasets # Users # Items # Interactions # Reviews # Concepts Instruments 8,292 695 22,831 7,380 425 Home 66,519 27,679 539,403 538,204 551 Automotive 20,886 1,573 57,722 16,866 477 4.1 Dataset Processing In our experiments, we use three real-world Amazon datasets (He and McAuley, 2016), i.e., Musical Instruments, Home and Kitchen, and Automotive, to evaluate the overall performance. The dataset statistics are shown in Table 1. The detailed description of these datasets is illustrated in Appendix A.2. 4.2 Experimental Setup We implement our framework by PyTorch using NLG4RS ecosystem (Li et al., 2020a) and run it on NVIDIA Tesla V100S PCIe, 32GB memory. The learning rate is set as 0.001. All models are optimized by the Adam optimizer and the embedding dimension is set as 64 for a fair comparison. The batch size equals 256. We truncate explanations with a maximum length of 20 and the budget k is set as 5000 for each dataset. We search the number of transformer layers and attention heads in each layer for the base explanation generator from {1, 2, 3, 4, 5}. And we also search the trade-off factor α, β, γ from {1e −5, 1e −4, 1e − 3, 1e −2, 1e −1, 1} and the dropout rate from {0, 0.2, 0.4, 0.6, 0.8}. The search strategy is grid search. We also set the patience threshold on the validation set to stop model training earlier and the optimal performance on the validation set is selected to report performance on the test set. We run each model five times and report the average performance. 4.3 Evaluation Metrics To evaluate the explanations, we categorize all the adopted metrics into two groups, which evaluate the text quality and the quality of interpretation, respectively. In the first text quality evaluation group, we have BLEU-1, BLEU-4, MAE, and RMSE. In the second interpretability evaluation group, we have USR, FCR, FMR, Entail, and Consistency. The detailed introduction of these metrics is illustrated in Appendix A.3.",
        "4.4 Baseline Methods We integrate the following baselines to conduct the comparison: RGCL (Shuai et al., 2022) applies a graph contrastive learning framework to learn useritem rating prediction; NRT (Li et al., 2017) learns explanation generation via gated recurrent units; NETE (Li et al., 2020b) generates explanations with controlled neural templates; PETER (Li et al., 2021b) introduces contextualization loss and item features to enhance the quality of the explanation; SAER (Yang et al., 2022) adopts an extract-andrefine architecture to effectively generate explanations; PEPLER (Li et al., 2022) fuses user/item IDs into soft prompt and fine-tunes GPT-2. To demonstrate the effectiveness of different components, we also introduce three variants of the CEER: 1) P-CEER: it removes all the proposed tasks and adopts the pure transformer to generate explanations. 2) C-CEER: it adds the conceptreview relationship modeling task to the P-CEER."
      ]
    },
    {
      "section": "3) CW-CEER: it further adds the user/item-review",
      "chunks": [
        "relationship modeling task to the C-CEER. To examine the usefulness of the information bottleneck in micro characteristics identification, we incorporate the following variants: (1) GPT3.5: Directly annotate the macro concepts in the historical reviews with ChatGPT and remove the information bottleneck. (2) β-VAE: Replace the information bottleneck with the β-VAE (Higgins et al., 2017) and use the learned variance as the informative level. 11740 Instrument Home Automotive 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Performance USR NRT PETER PEPLER NETE CEER (Ours) Instrument Home Automotive 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 Performance FCR NRT PETER PEPLER NETE CEER (Ours) Instrument Home Automotive 0.000 0.001 0.002 0.003 0.004 0.005 Performance FMR NRT PETER PEPLER NETE CEER (Ours) Figure 2: Explanation quality in terms of USR, FCR, FMR. Table 2: Text quality comparison of all methods in terms of BLEU-1 and BLEU-4."
      ]
    },
    {
      "section": "Methods",
      "chunks": [
        "Instruments Home Automotive BLEU-1 BLEU-4 BLEU-1 BLEU-4 BLEU-1 BLEU-4 NRT (2017) 12.8507 0.7109 12.1058 0.6764 15.3298 0.8062 PETER (2021) 14.9794 0.8345 11.5656 0.5737 15.9109 0.8116 PEPLER (2023) 4.5601 0.2289 8.8215 0.4074 2.4393 0.0706 NETE (2020) 13.8053 0.8949 10.6288 0.6059 14.6157 0.6916 SAER (2022) 13.7924 0.1210 12.5891 0.2763 13.9785 0.0000 CEER 16.1862 1.0489 12.6839 0.6887 16.1159 0.9010 Table 3: Rating prediction of all methods in terms of RMSE and MAE."
      ]
    },
    {
      "section": "Methods",
      "chunks": [
        "Instruments Home Automotive RMSE MAE RMSE MAE RMSE MAE RGCL (2022) 0.9224 0.6425 1.0612 0.7703 0.9883 0.6842 NRT (2017) 1.0930 0.6751 1.3312 0.8508 1.2184 0.7312 PETER (2021) 0.9412 0.7229 1.1073 0.8487 1.0259 0.7801 PEPLER (2023) 1.0183 0.6556 1.0792 0.7610 1.2007 0.7006 NETE (2020) 0.9400 0.7139 1.1207 0.8691 1.0261 0.7740 SAER (2022) 1.0970 0.9156 1.1731 0.9409 1.0429 0.8131 CEER 0.9210 0.6680 1.0789 0.7490 0.9693 0.6771 4.5 Text Quality Analysis (RQ1) 4.5.1 Word-level Relevance First, we evaluate the text quality of explanations in terms of BLEU-1 and BLEU-4 and exhibit the results in Table 2. We have several observations."
      ]
    },
    {
      "section": "1) The performance of baselines does not consistently improve with increasing complexity",
      "chunks": [
        "across the three datasets. The NRT model is simple and designed for generating abstractive tips, while the PETER, NETE, and SAER have more complicated architectures. Surprisingly, we find that the complex models do not consistently outperform the simpler NRT model in all cases. Sparse data during inference and training hinders the effective learning of user and item embeddings and limits the ability of advanced language models to generate accurate explanations. Although PEPLER, which fine-tunes GPT-2, fails to yield satisfactory results. This may be because they do not have harvest knowledge about ID during pre-training. 2) The proposed CEER outperforms all baselines across the three datasets. Compared to the baseline methods, the CEER framework consistently produces better results. We infer that this improveInstrument Home Automotive variable 0.075 0.050 0.025 0.000 0.025 0.050 0.075 0.100 0.125 Performance Entail"
      ]
    },
    {
      "section": "Method",
      "chunks": [
        "NRT PETER PEPLER NETE CEER(Ours) Instrument Home Automotive variable Consistency"
      ]
    },
    {
      "section": "Method",
      "chunks": [
        "NRT PETER PEPLER NETE CEER(Ours) Figure 3: Explanation quality w.r.t Entail and Consistency. ment is due to the inclusion of macro concepts, which offer higher-level semantics that enhance the learning of ID embeddings and provide additional information for generating explanations. 4.5.2 Rating Prediction We also examine the recommendation performance and present the results in Table 3. We find that the proposed CEER performs consistently better than the explainable recommendation baselines. And it performs even better than the dedicated rating prediction baseline RGCL in most cases. We infer that incorporating the high-level semantics, i.e., macro concepts refines the user and item representations, leading to improved recommendation performance. As a result, our model exhibits strong potential for real-world applications, as it not only offers explanations with high text quality but also delivers accurate rating predictions, addressing the specific requirements of the system. 4.6 Interpretability Quality Analysis (RQ2) 4.6.1 Personalization We also evaluate the degree of informativeness and diversity, i.e., personalization of explanations, and report the results in Figure 2. Notably, in Subsection 4.6, we only select the explainable baselines with superior performance for the RQ1 (i.e., PEPLER, PETER, NETE, and NRT) for comparison. From the results, we have several observations. First, the higher diversity of explanations does not indicate that the features are accurately 11741 included in each explanation. It indicates that improving informativeness and diversity simultaneously is a challenging task. Second, the proposed model generally outperforms the selected baselines in most cases, especially on the Instrument and Automotive datasets. It indicates that the proposed model can not only generate more diverse explanations but also match the corresponding features for the explanations. 4.6.2 Entail & Consistency We evaluate the interpretability from the sentiment perspective and display the results in Figure 3. We notice that the CEER achieves relatively better performance in producing emotionally coherent and factually accurate explanations in most cases. During training, words become associated with the relevant concepts they belong to. This alignment between positive or negative concepts and the corresponding words enables CEER to consistently generate emotionally coherent explanations. Additionally, user and item representations tend to align with the representations of related concepts. It facilitates the incorporation of related facts into the explanation generation process. 4.7 Ablation Study (RQ3) To explore the individual contributions of different tasks to the overall model performance, we analyze three variants and report the results in Table 4. Notably, in Subsections 4.7 and 4.8, we select a subset of metrics for evaluating text quality and interpretability rather than including all available metrics to save layout space.",
        "We have several observations: 1) The P-CEER has the worst performance. It does not include any of the designed tasks. The bad performance underscores the necessity of integrating the designated task to enhance model performance. 2) By taking each designed task gradually, CEER, CWCEER, and C-CEER demonstrate performance improvements compared with its predecessor CW-CEER, and C-CEER, P-CEER correspondingly. It demonstrates the effectiveness of each task in addressing the issue of limited generalization caused by sparsity. The overall results validate the effectiveness of the proposed model. 4.8 Hyper-parameter Sensitivity (RQ4) We further analyze the hyper-parameters α, β, γ in the proposed regularization terms. We conduct experiments on the Automotive dataset and present the results in Figure 4. We have several observations. 1) Across all selected metrics, the model is sensitive to extreme hyper-parameter values. Hyper-parameters that are too large, such as 1, or too small, such as 1e −5, do not lead to the best performance of the model. 2) Different evaluation metrics require different hyper-parameters to reach their optimal values. This observation implies that it is difficult to have a hyper-parameter setting that could achieve optimal results considering multiple evaluation metrics together for the whole model. 3) Compared with β and γ, α generally prefers relatively small values. We infer that the user/item-review relationship modeling task may act as noise and hinder the modeling of the language pattern in the explanations when α is large. All the results imply that the CEER is sensitive to the hyper-parameter values and it is important to choose appropriate values. 4.9 Characteristics Identification Study (RQ5) In this subsection, we first conduct the comparison between different micro characteristics identification methods and display the results in Table 5. Then we present an example from the Amazon Instrument dataset to intuitively show the identified micro characteristics, annotated macro concept, and the generated explanation in Figure 5. Performance Comparison. We compare the information bottleneck method with two other choices using the same annotation prompt, and explanation generator. The two choices are compared on the Amazon Instrument dataset and the Amazon Home dataset, respectively. From the results, we find that direct annotation without information bottleneck negatively affects the model performance. We infer that although LLMs are proficient at text comprehension, they are not trained to assign consistent macro concepts to similar words, which leads to poor performance. And the β-VAE also performs worse than the information bottleneck in most cases. β-VAE does not quantitatively define the importance of the words. And the learned variance may not be a good indicator to differentiate word importance although the small variance may mean that the meaning of the word does not change a lot across contexts, such as stop words.",
        "These results jointly demonstrate the effectiveness of the information bottleneck method. Case Study. We showcase the informative level of each word in the historical reviews associated with 11742 Table 4: Ablation study results on three datasets."
      ]
    },
    {
      "section": "Methods",
      "chunks": [
        "Instruments Home Automotive BLEU-1 BLEU-4 USR FCR FMR BLEU-1 BLEU-4 USR FCR FMR BLEU-1 BLEU-4 USR FCR FMR P-CEER 12.8870 0.7512 0.2138 0.0158 0.0038 11.6297 0.6568 0.0577 0.0299 0.0002 14.5324 0.7367 0.4688 0.0311 0.0012 C-CEER 14.6413 0.9538 0.3303 0.0250 0.0051 12.3248 0.6598 0.0652 0.0330 0.0003 14.9229 0.8626 0.5402 0.0424 0.0030 CW-CEER 15.8464 0.8565 0.4904 0.0261 0.0038 12.6099 0.6344 0.0791 0.0475 0.0002 15.4576 0.7608 0.6217 0.0473 0.0027 CEER 16.1862 1.0489 0.5552 0.0281 0.0048 12.6839 0.6887 0.0890 0.0431 0.0003 16.1159 0.9010 0.6120 0.0435 0.0044 1e-05 0.0001 0.001 0.01 0.1 1.0 value 15.0 15.5 16.0 16.5 BLEU-1 Automotive 1e-05 0.0001 0.001 0.01 0.1 1.0 value 0.80 0.85 0.90 0.95 BLEU-4 Automotive 1e-05 0.0001 0.001 0.01 0.1 1.0 value 0.4 0.5 0.6 0.7 0.8 USR Automotive 1e-05 0.0001 0.001 0.01 0.1 1.0 value 0.03 0.04 0.05 0.06 FCR Automotive 1e-05 0.0001 0.001 0.01 0.1 1.0 value 0.002 0.004 0.006 FMR Automotive Figure 4: Hyper-parameter sensitivity results of different regularization terms on Automotive dataset. Figure 5: Examples from the Instrument dataset. the given user-item pair. Then, we demonstrate the annotated concepts that drive the model to generate the explanation involved by the user and item in this example. First, we visualize the informative level of each word representation of two historical reviews associated with user 543 and item 504, respectively. We observe that those keywords that explain the reason for the rating are assigned significantly lower amounts of noise. For example, in the first review of subfigure (a), the words “costly” and “price” describe the reason for user dissatisfaction and their informative levels are very high. It demonstrates that the first component could select the informative words. That facilitates the cost reduction for LLMs to annotate concepts. Second, we compare the explanations generated Table 5: Model performance comparison with different micro characteristics identification methods."
      ]
    },
    {
      "section": "Methods",
      "chunks": [
        "BLEU-1 BLEU-4 USR FCR FMR GPT-3.5 15.9572 0.8701 0.4763 0.0233 0.0038 CEER 16.1862 1.0489 0.5552 0.0281 0.0048 β-VAE 12.3631 0.7028 0.0819 0.0389 0.0003 CEER 12.6839 0.6887 0.0890 0.0431 0.0003 by the CEER and the P-CEER, i.e., the transformer model for the interaction between user 543 and item 504, and show it in subfigure (b). The target rating is 5. We observe that the P-CEER only describes the user’s positive sentiment towards the item and does not contain any useful information. The generated explanation from CEER reveals the reason for the high rating is due to the price and the positive quality. We also exhibit the top two prioritized concepts and words associated with them in the subfigure (c). It shows to a certain extent that the high-level concept does drive the transformer to generate semantically relevant sentences. And the associated words for the concept demonstrate that the LLMs could appropriately assign a concept label to semantic similar words. 5"
      ]
    },
    {
      "section": "Related Work",
      "chunks": [
        "5.1 Sentence-based Explanation Generation Compared with examining which feature plays a more important role in driving the interaction (Wang et al., 2018a, 2022b), sentence-based explanations for rating prediction offer natural language explainable information about why the user assigns particular ratings (Hua et al., 2023; Wu et al., 2024). Some efforts only manipulate the historical reviews to generate the explanations (Le and 11743 Lauw, 2020; Pugoy and Kao, 2021), other efforts attempt to integrate varied auxiliary information into explanation generation. Knowledge graph provides an organized way to access facts (Chen et al., 2024; Shengyuan et al., 2024). CETP (Li et al., 2021a) selects the triples from the knowledge graph and incorporates them into explanation generation. Further, METER (Geng et al., 2022; Liu et al., 2023) incorporates multi-modality information to encourage the model to write more faithful and diverse explanations. LLMs have strong text manipulation abilities (Dong et al., 2024; Zhang et al., 2024). PRAG (Xie et al., 2023) leverages LLMs and reformulates the explanation generation as a questionanswering task. Additionally, the tips written by users show how they feel about the interaction and can also be utilized to generate a comprehensive explanation (Zhu et al., 2023). To generate coherent explanations, AESG tries to incorporate the syntax graph dependency tree to improve the quality of explanations (Hu et al., 2023). 5.2 Review-based Rating Prediction Review-based rating prediction involves predicting a numerical rating for a user-item pair by modeling the text of a review provided by a user (Harrag et al., 2019). Early effort introduces the reviewlevel attention mechanism to model the review for rating prediction (Chen et al., 2018). It ignores the fine-grained information in the review. NPA (Wu et al., 2019) leverages user ID to incorporate wordlevel and review-level information to improve performance attentively. Further, DAML (Liu et al.,"
      ]
    },
    {
      "section": "2019) exploits the mutual information between the",
      "chunks": [
        "user and the item extracted from information that the convolutional neural network attends to. Similarly, CARP (Li et al., 2019) models mutual information and uses the capsule network to extract high-level information. EDMF (Liu et al., 2022) extracts the useful feature from the review text to further improve performance. Graph neural networks (GNNs) capture global relationships between nodes that are indirectly connected (Dong et al., 2023; Chen et al., 2020a). Efforts adopt GNNs to model user and item relationships and the review features can be regarded as edge features (Qiao et al., 2022; Shuai et al., 2022). 6 Conclusion and Future Work In this paper, we present the CEER framework, designed to enrich user and item embeddings with high-level semantics, i.e., macro concepts to improve the explanation generation. The framework identifies micro characteristics of items from associated reviews and utilizes LLMs to annotate macro concepts. These macro concepts act as intermediaries to align the user and item embeddings with review embeddings to obtain more semantic information. And we achieve this through three tasks that focus on establishing relationships between users/items, macro concepts, and reviews. We conduct extensive experiments, and the results confirm the effectiveness of CEER. Our future work will involve adapting our method to annotate concepts from various data sources, such as images, and enhancing the explanation generation process (Li et al., 2023a)."
      ]
    },
    {
      "section": "Limitations",
      "chunks": [
        "In this study, we annotate the macro concept from the reviews to assist the explanation generation. The existing framework is constrained to annotating macro concepts solely from text and lacks the capability to expand to additional data sources like images. And we annotate macro concepts from a series of micro item characteristics, i.e., words. However, the semantic meaning of words differs under different contexts. Moreover, we employ LLMs for annotating macro concepts and training a small language model for explanation generation instead of fine-tuning LLMs. While fine-tuned GPT-2 may outperform the suggested framework when available reviews are rich, our framework demonstrates relatively better effectiveness in situations with extremely sparse reviews and does not incur significant computational costs. Ethics Statement In this study, all the Amazon datasets are publicly available and have been extensively used in research related to recommendation systems. All the baseline codes are open-sourced. And we adhere to the ACL Code of Ethics."
      ]
    },
    {
      "section": "Acknowledgments",
      "chunks": [
        "The work described in this paper was fully supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. PolyU 25208322)."
      ]
    }

  ]
}