{
  "paper_id": "131",
  "paper_title": "131",
  "sections": [
    {
      "section": "FrontMatter",
      "chunks": [
        "BiM-VFI: Bidirectional Motion Field-Guided Frame Interpolation for Video with Non-uniform Motions Wonyong Seo KAIST* wyong0122@kaist.ac.kr Jihyong Ohâ€  Chung-Ang University jihyongoh@cau.ac.kr Munchurl Kimâ€  KAIST* mkimee@kaist.ac.kr https://kaist-viclab.github.io/BiM-VFI_site/ Input [D,R]-IFRNet [ECCV`24] [D,R]-AMT-S [ECCV`24] GT [D,R]-EMA-VFI [ECCV`24] AMT-S [CVPR`23] UPRNet [CVPR`23] EMA-VFI [CVPR`23] Ours t=0.25 t=0.5 t=0.75 t=0.25 t=0.5 t=0.75 t=0.25 t=0.5 t=0.75 GT Input [D,R]-IFRNet [ECCV`24] [D,R]-AMT-S [ECCV`24] [D,R]-EMA-VFI [ECCV`24] AMT-S [CVPR`23] UPRNet [CVPR`23] EMA-VFI [CVPR`23] Ours GT Figure 1. Qualitative comparison of our proposed BiM-VFI and SOTA models at arbitrary time instances (t = 0.25, 0.5 and 0.75) for video frame interpolation. The previous SOTA methods yield blurry interpolated frames while our BiM-VFI model generates clear ones."
      ]
    },
    {
      "section": "Abstract",
      "chunks": [
        "Existing Video Frame interpolation (VFI) models tend to suffer from time-to-location ambiguity when trained with video of non-uniform motions, such as accelerating, decelerating, and changing directions, which often yield blurred interpolated frames. In this paper, we propose (i) a novel motion description map, Bidirectional Motion field (BiM), to effectively describe non-uniform motions; (ii) a BiMguided Flow Net (BiMFN) with Content-Aware Upsampling Network (CAUN) for precise optical flow estimation; and *Korea Advanced Institute of Science and Technology. â€ Co-corresponding authors. (iii) Knowledge Distillation for VFI-centric Flow supervision (KDVCF) to supervise the motion estimation of VFI model with VFI-centric teacher flows. The proposed VFI is called a Bidirectional Motion field-guided VFI (BiMVFI) model. Extensive experiments show that our BiMVFI model significantly surpasses the recent state-of-the-art VFI methods by 26% and 45% improvements in LPIPS and STLPIPS respectively, yielding interpolated frames with much fewer blurs at arbitrary time instances. arXiv:2412.11365v4 [cs.CV] 24 Mar 2025",
        "0.5 0.5 ğœ‹ 0.4 0.6 ğœ‹ t=1 t=0.5 t=0 t=1 t=0.5 t=0 t=1 t=0.5 t=0 t=1 t=0.5 t=0 t=1 t=0.5 t=0 0.4 0.6 0.8ğœ‹ 0.6 0.4 ğœ‹ 1.2ğœ‹ 0.4 0.6 r0 r1 BiM ğ‘€ğ‘¡â†’0,1 â‰”áˆº ğ‘Ÿ0 ğ‘Ÿ0 àµ…ğ‘Ÿ1 , ğœ™áˆ» ğœ™ ğ‘£ğ‘¡â†’1 ğ‘£ğ‘¡â†’0 (0.5) (0.4) ï€¨0.4ï€¬ï€ ï€±ï€®ï€²ï°) (0.5) (0.4) ï€¨0.4ï€¬ï€ ï€°ï€®ï€¸ï°) (0.5) (0.6) ï€¨0.6ï€¬ï€ ï°) (0.5) (0.4) ï€¨0.4ï€¬ï€ ï°) (0.5) (0.5) ï€¨0.5ï€¬ï€ ï°) Distance Indexing BiM (Ours) Time Indexing CASE 1 CASE 2 CASE 3 Ambiguous Distinct Distinct Ambiguous Ambiguous Distinct Figure 2. Time-to-location (TTL) ambiguity comparison of motion descriptors (time indexing, distance indexing, our BiM)."
      ]
    },
    {
      "section": "Introduction",
      "chunks": [
        "Video frame interpolation (VFI) is a fundamental low-level vision task that aims to synthesize intermediate frames between temporally adjacent input frames. VFI enables the conversion of low-frame-rate videos into high-frame-rate sequences, enhancing visual fluidity and realism. VFI has broad applications, including the restoration and enhancement of low frame rate videos, the creation of slow-motion videos [12, 41], and the improvement of animation workflows in the cartoon industry [2, 32, 36]. VFI is an ill-posed problem due to the time-to-location (TTL) ambiguity between two input frames [22, 34, 44]. The TTL ambiguity stems from infinitely many possible trajectories between the corresponding pixels of the two source input frames for video sequences with non-uniform motions (CASE1, CASE2, and CASE3 in Fig. 2). It is well known that TTL ambiguity complicates the prediction of the actual target frame during inference. However, it also can pose challenges during training. VFI learning based only on target timesteps t between the two source input frames can cause VFI networks to learn the average of all the possibilities as final interpolation results which often turn out to be very blurred [44]. Solving the TTL ambiguity problem is challenging, especially for inference because of its high illposedness. Instead, we propose an alternative solution not to solve the TTL ambiguity problem at inference time but to resolve ambiguity in the training phase to obtain clean interpolated frames at target time instances. To resolve TTL ambiguity during training, we propose a novel motion description map based on BidirectionalMotion Fields (BiM), inspired by the distance indexing [44]. The distance indexing relies only on relative distances on the line between two corresponding pixels in source frames, so it is limited in describing the directional changes along motion trajectories, and thus cannot fully resolve the ambiguity. However, our BiM can fully describe any non-uniform motions including accelerations, decelerations, or directional changes by incorporating both magnitude and angular information of bidirectional flows between a target frame and each of two source frames. The BiM is used as a description map for VFI learning to generate clean interpolated frames by limiting the solution space of the possible motion trajectories during training. Also, we design (i) a BiM-guided FlowNet (BiMFN), and (ii) a Content-Aware Upsample Network (CAUN) to precisely estimate the motions based on input BiM. Lastly, we propose a Knowledge Distillation for VFI-Centric Flow supervision (KDVCF) as a new training strategy with the help of the target frame to generate both BiM as input to student process and accurate flows for student process supervision during the training. Our proposed VFI model with KDVCF, BiMFN, and CAUN is called Bidirectional Motion fieldguided VFI (BiM-VFI). 2. Related Works 2.1. Video frame interpolation VFI methods can be divided into two categories: flowbased and kernel-based approaches. The flow-based methods [9, 13, 16, 18, 42] utilize optical flows in interpolating a target frame. The kernel-based methods construct various types of kernels, such as the adaptive kernels [27, 28], the deformable kernels [17, 39], or the attention maps [22, 34] to interpolate the target frames by applying these kernels to the source frames. While flow-based methods can interpolate at any arbitrary time frame, kernel-based methods are limited to interpolating center frames. Consequently, flowbased methods have dominated the recent VFI works. Flow-based methods focus on improving the performance of their motion estimators to enhance the interpolation quality. For the methods [7, 26] employing forward warping [26], pre-trained optical flow models [8, 37, 38] have been directly utilized to estimate the motion to improve the motion estimation accuracy. For the methods employing backward warping [11], flows have to be estimated from the target frame to each of the two source frames, while target frames are unavailable. Therefore, recent methods tried to design their own motion estimators for accurate flow estimations. Park et al. [29â€“31] and Jin et al. [13] have",
        "tailored local cost volumes in a bilateral manner to estimate motions between target frame and each of two source frame. Li et al. [18] also adopted â€˜all pair cost volumeâ€™ [38], to enhance the motion estimation capabilities of their model. Recent studies [16, 18] have demonstrated that supervising the flow estimation using pre-trained optical flow models [10] can benefit motion estimation learning, especially in large motions or motions in homogeneous regions, which are not adequately captured by photometric supervision. However, the pre-trained flow models resulted in degraded performance for VFI in cases of motion in certain regions such as shadows, or blurs, because flows estimated from supervised optical flow models and flows for VFI have distinct roles [16]. Huang et al. [9] introduced the â€œprivileged blockâ€, which utilizes the target frame to generate a more accurate optical flow from the target to the source frame. They supervised the flows estimated solely from source frames with these privileged flows to enhance motion estimation performance. However, the privileged block only consists of a few convolution layers, thus limiting the full utilization of target frames to enhance motion estimation accuracy. 2.2. Non-uniform motions for VFI There are studies focused on reducing ambiguity caused by non-uniform motions in VFI. Xu et al. [40] utilized four neighboring consecutive input source frames around each target frame to model motion as a quadratic equation. Several studies [22, 34] also use four neighboring frames with transformer architectures [4, 19] to implicitly capture nonuniform motion and interpolate the target frames with selfattention operation. However, while VFI methods using 4 input frames can reduce TTL ambiguity during training, they still cannot fully resolve it. Recently, Zhong et al. [44] proposed a novel paradigm to address TTL ambiguity during training. Zhong et al. [44] introduced a novel motion descriptor called â€˜distance indexingâ€™, which describes the relative magnitude between the motion from I0 to I1 and the motion from I0 to It, using a pixel-wise motion magnitude ratio map. It has shown that distance indexing can effectively resolve velocity ambiguity but cannot resolve directional ambiguity, because it only includes the motion magnitudes and ignores the directional components. 3. Proposed Method Fig. 3 depicts the overall network architecture of our BiMVFI based on a recurrent pyramid architecture, operates from (L âˆ’1)-th level to 0-th level (where L is a number pyramid level), and the procedure of proposed KDVCF. Every pyramid level consists of a pair of student and teacher processes, PS and PT , where the weights are shared between the processes as well as across all pyramid levels. A preceding source frame, a following source frame, and a target frame are denoted as I0, I1, and It, respectively. In the BiM-VFI, I0, It, and I1 are downsampled by a factor 1/2 at each hierarchical level. The Motion Feature Extractor (MFE) extracts motion features F l,m , F l,m and F l,m t for l-th level input, while the Context Feature Extractor (CFE) extracts context features F l,c and F l,c 1 . In the student process, F l,m and F l,m are fed in Bidirectional Motion Field FlowNet (BiMFN) that outputs bidirectional optical flows, ËœV l,PS tâ†’0 and ËœV l,PS tâ†’1. The Content-Aware Upsampling Network (CAUN) takes ËœV l,PS tâ†’0, ËœV l,PS tâ†’1, F l,c 0 , and F l,c as input and yields upsampled optical flows Vl,PS tâ†’0 and Vl,PS tâ†’1 in a adaptive manner. In the synthesis network (SN), Il 0, Il 1, F l,c 0 , and F l,c are backwarped [11] by Vl,PS tâ†’0 and Vl,PS tâ†’1. The warped frames and context features then finally yield a blending mask for two warped images, Ol,PS and corresponding interpolated frame Ë†Il,PS t . Simple U-net [33] architecture is employed as SN for our BiM-VFI. The teacher process operates almost in the same manner as the student process except using ground truth target frame Il t as input pair with each of Il 0 and Il 1. Since Il t is used as part of input, the resulting ËœV l,PT tâ†’0 and ËœV l,PT tâ†’1 are more precise than ËœV l,PS tâ†’0 and ËœV l,PS tâ†’1 so they are used to supervise the learning of ËœV l,PS tâ†’0 and ËœV l,PS tâ†’1, as well as to compute the BiM for student process M l,PS tâ†’0,1. In the rest of this section, we will explain our motion description map BiM (Sec. 3.1), specific modules in our BiM-VFI (Secs. 3.2 and 3.3), and the proposed knowledge distillation strategy KDVCF (Sec. 3.4) in detail. 3.1. Bidirectional Motion Field (BiM) Various non-uniform motions including accelerating, decelerating, and changing directions are contained in realworld videos. Such non-uniform motions not only make illposedness at inference but also cause VFI learning to suffer from the TTL ambiguity at training time, thus resulting in severe blur artifacts in interpolated frames. It is very challenging to resolve such ambiguity problems directly in inference time because it is difficult to predict the actual trajectory when only the first and last frames of the motion are given. Instead, we take an alternative approach to solving the blur problem caused by TTL ambiguity at training time. To resolve the TTL ambiguity during training, we introduce a Bidirectional Motion Field (BiM), Mtâ†’0,1 = [R, Î¦]T , as a novel motion descriptor that consists of pixelwise motion magnitude ratios R and angles Î¦ between bidirectional optical flows Vtâ†’0 and Vtâ†’1 from It to each of I0 and I1. The BiM at pixel location (x, y) is defined as: Mtâ†’0,1(x, y) = [R(x, y), Î¦(x, y)]T = [ r0 r0 + r1 , Ï•]T , (1)",
        "Image Pyramid Teacher Process PT (Training only) Student Process PS â‹® BiMFN BiMFN BiMFN CAUN CAUN SN SN Pyr. level L-1 ` : Training only : Stop gradient â‹® MFE CFE Pyr. level l Pyr. level 0 lI l tI lI lI l tI lI lI lI , l m F ,l m tF , l m F , l c F , l c F , 0,1 l tï‚® M S P , 0, l t t ï‚® M T P , ,1 l t t ï‚® M T P ï‚°, l tï‚® V S P ï‚°, l tï‚® V S P ï‚°, l tï‚® V T P ï‚°, l tï‚® V T P , l tï‚® V S P , l tï‚® V S P , l tï‚® V T P , l tï‚® V T P ,Ë†lI S P ,Ë†lI T P ,l O S P ,l O T P , , l m l m F F ïƒ¦ ïƒ¶ ïƒ§ ïƒ· ïƒ¨ ïƒ¸ , , l m l m t F F ïƒ¦ ïƒ¶ ïƒ§ ïƒ· ïƒ¨ ïƒ¸ , , l m t l m F F ïƒ¦ ïƒ¶ ïƒ§ ïƒ· ïƒ¨ ïƒ¸ ï‚°, |0 l t t t ï‚® V T P ï‚°, | 1 l t t t ï‚® V T P Eq.1 ï€¨ ï€© l l I I ï€¨ ï€© l l I I ï€¨ ï€© , , l c l c F F ï€¨ ï€© , , l c l c F F ï€¨ ï€© , , l c l c F F ï€¨ ï€© , , l c l c F F ,uni 0,1 l tï‚® M 1, l t ï€« ï‚® V S P 1, l t ï€« ï‚® V S P 1, l O ï€« S P 1, l O ï€« S P 1, l t ï€« ï‚® V S P 1, l t ï€« ï‚® V S P 1, l O ï€« S P Figure 3. Our Bidirectional Motion Field-guided VFI (BiM-VFI) with Knowledge Distillation for VFI-Centric Flow supervision (KDVCF). where r0 = ||Vtâ†’0(x, y)||, r1 = ||Vtâ†’1(x, y)||, and Ï• = âˆ Vtâ†’1(x, y) âˆ’âˆ Vtâ†’0(x, y) (top left of Fig. 2). Fig. 2 depicts a TTL ambiguity comparison of different motion descriptors, time indexing [9, 16, 18, 45], distance indexing [44] and our BiM. CASE 1 illustrates a car at timestep t = 0.5 is exactly at the center between the cars at timestep t = 0 and t = 1. All motion descriptors can avoid TTL ambiguity if all training triplets have uniform motions as depicted in CASE1. In CASE 2, the blue car is placed at a relative distance of 0.4, while the green car is placed at a relative distance of 0.6. In this case, the blue and green cars are described by the same time indexing of 0.5 but different distance indexings and BiMs of 0.4 and 0.6, which incurs the TTL ambiguity only with time indexing. That is, time indexing cannot distinguish the blue and green carâ€™s position, where both cars are captured at t = 0.5. Lastly, CASE 3 shows the case where the blue and green cars have different changes in motion directions at an accelerating speed. Both time and distance indexings fail to distinguish the two cars, but our BiM can describe the two cars differently in terms of Ï• = 1.2Ï€ and 0.8Ï€. Due to this TTL ambiguity problem, recent VFI models trained with time indexing [9, 16, 18, 45] or distance indexing [44] tend to produce blurry interpolated frames at target times in the sense of averaging all possible answers (blue and green cars) to minimize the objectives such as L1 losses. For inference, It has to be restored, but the flows Vtâ†’0, Vtâ†’1, or even the BiM Mtâ†’0,1 is not available except for the target time t. Moreover, the motion types (uniform or non-uniform) are not known between I0 and I1. Nevertheless, it is known that uniform motion assumption reasonably works well [44]. We extend this uniform assumption, Vtâ†’0/t + Vtâ†’1/(1 âˆ’t) = 0. by adding angle information Ï• = Ï€. So, the uniform BiM used for inference is given as: Muni tâ†’0,1 = \u0002 t Â· 1HÃ—W Ï€ Â· 1HÃ—W \u0003T , (2) where H and W are the height and width of desired bidirectional optical flows, Vtâ†’0 and Vtâ†’1. We demonstrate that our BiM, under the uniform motion assumption, can interpolate frames with a similar sense of time indexing as described in Sec. 4.5. We point out that our BiM guides the BiM-VFI model to yield relatively cleaner interpolated frames at t than other VFI models with time indexing and distance indexing (Fig. 1). With the distinct motion describability of BiM, our BiM-VFI is trained without TTL ambiguity and can infer much cleaner target frames under the uniform motion assumption, although the motion of the interpolated target frames may not aligned with their real motions. 3.2. BiM-guided FlowNet (BiMFN) We now design a bidirectional flow estimation network that utilizes the BiM. Fig. 4 shows our BiM-guided FlowNet (BiMFN) that estimates ËœV l tâ†’0 and ËœV l tâ†’1 from Il t to each of Il 0 and Il 1 at l-th pyramid level. It is noted that Vl+1,d tâ†’0 and Vl+1,d tâ†’1 are bilinearly downsampled versions of Vl+1 tâ†’0 and Vl+1 tâ†’1 by a factor of 2 and its magnitude is also divided by",
        "Conv Conv BiM-M Conv w ğ¹ğ‘‰ ğ¹Î¦ ğ¹ğ‘… ResBlock ResBlock ResBlock * + * ğ›·IN Backward warping Residual connection Elementwise multiplication BiM-MConv cv Cost volume construction + ğ•à·©ğ‘¡â†’0 ğ‘™,ğ‘Ÿğ‘’ğ‘  ğ•à·©ğ‘¡â†’1 ğ‘™,ğ‘Ÿğ‘’ğ‘  w w ğ•ğ‘¡â†’0 ğ‘™àµ…1 ğ¹0 ğ‘™,ğ‘š ğ¹1 ğ‘™,ğ‘š ğ•ğ‘¡â†’1 ğ‘™àµ…1 cv cv ğ‘‚ğ‘™àµ…1 Conv ğ¹ğ‘‰ Conv ğ•ğ‘¡â†’0 ğ‘™àµ…1,ğ‘‘ ğ•ğ‘¡â†’1 ğ‘™àµ…1,ğ‘‘ ğ‘… ğ•à·©ğ‘¡â†’0 ğ‘™",
        "ğ•à·©ğ‘¡â†’1 ğ‘™",
        "d Bilinear downsampling d d ğ•ğ‘¡â†’0 ğ‘™àµ…1,ğ‘‘ ğ•ğ‘¡â†’1 ğ‘™àµ…1,ğ‘‘ ğ‘‚ğ‘™àµ…1,ğ‘‘ d ... + DEM AEM N ğ¹Î¦ ğ¹ğ‘… Figure 4. Proposed BiM-Guided FlowNet (BiMFN) at l-th pyramid level. factor of 2 to match the spatial sizes and magnitude of ËœV l tâ†’0 and ËœV l tâ†’1. The blending mask estimated from the previous pyramid level, Ol+1, is also downsampled as Ol+1,d in the same sense. The BiMFN first warps F l,m and F l,m using Vl+1 tâ†’0 and Vl+1 tâ†’1, resulting in F l,m 0â†’t and F l,m 1â†’t. F l,m 0â†’t and F l,m 1â†’t are then used to construct local cost volumes [37, 38] to find precise correspondences between two features. Local cost volumes from F l,m 0â†’t to F l,m 1â†’t and vice versa are constructed to encapsulate asymmetric correspondence information. Ol+1,d is encoded through separate convolution layers and then concatenated with F l,m 0â†’t, F l,m 1â†’t, and two cost volumes before being passed to the next module. Then, the BiM Modulation Convolution (BiM-MConv) in Fig. 4 takes three inputs with FV , FR, and FÎ¦ where (i) FV is the output of the cascaded eight convolution layers; (ii) FR is the encoded output from the Distance Embedding Module (DEM) with a one-channel motion ratio component input R in Eq. (1); and (iii) FÎ¦ is the feature output of the Angle Embedding Module (AEM) for a two-channel angular component input Î¦IN = (sin(Î¦), cos(Î¦)) from Eq. (1). Finally, for FR, FÎ¦, and FV input, the BiM-MConv integrates them with elementwise multiplication and produces the refined residual flow fields, ËœV l,res tâ†’0 and ËœV l,res tâ†’1 . The final flow estimations at l-th pyramid level are computed as: ËœV l tâ†’0 = Vl+1,d tâ†’0 + ËœV l,res tâ†’0 , ËœV l tâ†’1 = Vl+1,d tâ†’1 + ËœV l,res tâ†’1 . (3) 3.3. Content-Aware Upsampling Network (CAUN) Since ËœV l tâ†’0 and ËœV l tâ†’1 are of the same size as F l 0,m and F l 1,m, they must be upsampled by a scale factor of 4 to match the image dimensions, H/2l and W/2l. In general, the usage of bilinear or bicubic upsampling can incur flow leakages along object boundaries and diminish small objects by blending external flows [23, 38]. To avoid this, adaptive upsampling is commonly employed in optical flow estimation models such as [10, 23, 38], but still is not widely used in VFI models. We adopt â€˜Convex upsampleâ€™ layer, proposed by Teed et al. [38], and extend it for VFI to upsample ËœV l tâ†’0 and ËœV l tâ†’1 to Vl tâ†’0 and Vl tâ†’1 using pixelwise adaptive kernels. The detailed structure of CAUN is presented in Supplementals. The adaptive upsampling of the CAUN not only aesthetically enhances the upsampling of flows but it also more effectively captures small objects and complex boundaries in interpolated frames, thanks to its ability to maintain the flows of small objects and prevent mixing the flows at object boundaries. Our extensive experiments show its effectiveness, which is presented in Sec. 4.4. 3.4. Knowledge Distillation for VFI-centric Flow Supervision (KDVCF) As discussed in Sec. 2.1, the flow estimation of pre-trained optical flow models and VFI models operates differently in certain areas, such as blurs and shadows. Therefore, instead of using pre-trained optical flow models for BiM and flow supervision, we propose Knowledge Distillation for VFIcentric Flow supervision (KDVCF) that provides BiM and flow supervision more suitable for VFI. KDVCF consists of student process PS and teacher process PT . The two processes sequentially run but share the same architecture with the same weights. First, as shown in Fig. 3, PT takes two input pairs, (Il 0, Il t) and (Il t, Il 1), and produce precise flow estimations ËœV l,PT tâ†’0 and ËœV l,PT tâ†’1 . Then the BiM is computed based on ËœV l,PT tâ†’0 and ËœV l,PT tâ†’1 according to Eq. (1), and is inputted to the BiMFN of PS. So, the knowledge distillation can be made from PT to PS for flow estimations during training. Note that PS only remains at inference. In PT , the BiMFN yields ËœV l,PT tâ†’0 and ËœV l,PT tâ†’t|0t for input frame pair Il 0 and Il t. In this case, ËœV PT tâ†’0 can be an accurate flow field estimate, and ËœV l,PT tâ†’t|0t is ideally a vector field of all 0â€™s. Since our BiM is formatted in terms of a relative motion ratio and an angle between bidirectional flows, the resulting motion ratio R is constructed as a uniform map of 1â€™s but the angles map Î¦ is undefined because ËœV l,PT tâ†’t has zero vectors. So, Î¦ is filled with angles randomly sampled from a uniform distribution U(0, 2Ï€) to avoid a bias to any specific angle value. Thus, the BiM to be inputted to the BiMFN in PT for input pair (Il 0, Il t) is defined as: Ml,PT tâ†’0,t = \u00021H/2l+2Ã—W/2l+2 Ï•0 Â· 1H/2l+2Ã—W/2l+2\u0003T (4) where Ï•0 âˆ¼U(0, 2Ï€). Like for another input pair (Il t, Il 1) in PT , the BiMFN yields ËœV l,PT tâ†’t|t1 and ËœV l,PT tâ†’1 . So, the BiM",
        "Overlayed AMT [CVPR`23] UPRNet [CVPR`23] EMA-VFI [CVPR`23] Ours GT Figure 5. Qualitative comparison for fixed-time interpolation datasets, XTest [35]. as input to the BiMFN for input pair (Il t,Il 1) is defined as: Ml,PT tâ†’t,1 = \u00020H/2l+2Ã—W/2l+2 Ï•1 Â· 1H/2l+2Ã—W/2l+2\u0003T (5) where Ï•1 âˆ¼U(0, 2Ï€). To ensure the VFI-centric estimation of ËœV l,PT tâ†’0 and ËœV l,PT tâ†’1 , these flows are further employed to reconstruct the target image Ë†Il,PT t in PT , along with CAUN and SN, and are trained with a photometric reconstruction loss. Note that ËœV l,PT tâ†’0 and ËœV l,PT tâ†’1 are also employed to construct the BiM in Eq. (1) for PS that operates for source frame pairs, Il 0 and Il 1, as well as to supervise the output flow fields, ËœV l,PS tâ†’0 and ËœV l,PS tâ†’1, of the BiMFN in PS. Unlike estimated flows from pre-trained supervised optical flow models, that are trained to minimize end-point error with GT flows, our ËœV l,PT tâ†’0 and ËœV l,PT tâ†’1 align precisely with the objectives of VFI. Consequently, the distillation is fully beneficial and effectively tailored to the PSâ€™s purpose. Extensive experiments showed that our KDVCF is more beneficial than the distillation from pre-trained supervised optical flow models. During PS, our BiM-VFI learns various uniform and non-uniform motions with a distinct motion descriptor (BiM) and a precise VFI-centric flow supervision produced by PT . It is worth menting that ËœV l,PS tâ†’0 and ËœV l,PS tâ†’1 in PS can be precisely learned in the help of our BiM based on accurate flow fields ËœV l,PT tâ†’0 and ËœV l,PT tâ†’1 obtained in PT , with the availability of target frame Il t. For inference, the BiM for uniform motions is fed into the BiMFN, and our BiM-VFI can correspondingly construct clean interpolated frames with uniform motions although they might not be well aligned with ground truth target frames with nonuniform motions."
      ]
    },
    {
      "section": "Experiments",
      "chunks": [
        "4.1. Experiments details Benchmarks. We tested our BiM-VFI for both fixed-time (t=0.5) and arbitrary time interpolation datasets. For fixedtime interpolation, we used Vimeo90K triplet [41], SNUFILM [3], and XTest [35] datasets. For arbitrary-time interpolation, we conducted experiments on Vimeo90K septuplet [41] and SNU-FILM-arb [6] datasets. Metrics. We measured both pixel-centric metrics (PSNR and SSIM) and perceptual metrics (LPIPS [43], STLPIPS [5], and NIQE [25]) for quantitative comparisons between our BiM-VFI and SOTA methods. Both LPIPS and STLPIPS are full-reference perceptual metrics, while NIQE is a no-reference perceptual metric. LPIPS and STLPIPS compute the similarity between features of the input and reference images using a pre-trained network. However, while LPIPS exhibits a significant drop in metric performance in the presence of minor misalignments, STLPIPS is more tolerant of such misalignments. 4.2. Qualitative results We compared our BiM-VFI with both State-of-the-art (SOTA) arbitrary-time and fixed-time VFI models for various datasets. Fig. 1 compares arbitrary-time interpolation results at t = 0.25, 0.5, and 0.75 for SNU-FILM-arb ex-"
      ]
    },
    {
      "section": "Method",
      "chunks": [
        "Vimeo90K-septuplet SNU-FILM-arb Medium Hard Extreme psnr ssim lpips stlpips niqe lpips stlpips niqe lpips stlpips niqe lpips stlpips niqe RIFE [9] 28.22 0.912 0.105 0.084 6.663 0.038 0.021 4.975 0.072 0.054 5.177 0.134 0.116 5.358 IFRNet [16] 28.26 0.915 0.088 0.094 6.422 0.046 0.037 4.921 0.066 0.054 4.870 0.115 0.094 4.793 M2M-PWC [7] 27.43 0.901 0.081 0.055 6.026 0.030 0.014 4.806 0.049 0.025 4.758 0.089 0.055 4.657 AMT-S [18] 28.52 0.920 0.101 0.105 6.866 0.072 0.046 5.443 0.089 0.060 5.444 0.135 0.098 5.500 UPRNet [13] 27.23 0.900 0.087 0.061 6.280 0.031 0.014 4.837 0.054 0.028 4.909 0.092 0.056 4.923 EMA-VFI [45] 29.41 0.928 0.086 0.079 6.736 0.041 0.025 4.984 0.072 0.054 5.236 0.125 0.106 5.522 [D,R]-RIFE [44] 27.41 0.901 0.086 0.059 6.220 0.027 0.011 4.751 0.050 0.026 4.829 0.101 0.072 4.898 [D,R]-IFRNet [44] 27.13 0.899 0.078 0.053 6.167 0.026 0.010 4.757 0.048 0.023 4.798 0.095 0.062 4.821 [D,R]-AMT-S [44] 27.17 0.902 0.081 0.053 6.326 0.029 0.013 4.747 0.054 0.028 4.849 0.107 0.071 5.017 [D,R]-EMA-VFI [44] 24.73 0.851 0.081 0.046 6.227 0.032 0.013 4.864 0.055 0.027 4.978 0.106 0.071 5.120 Ours 26.83 0.893 0.070 0.043 6.009 0.023 0.008 4.693 0.039 0.015 4.725 0.070 0.034 4.751 Table 1. Quantitative comparisons on arbitrary-time interpolation datasets. treme datasets [3]. While the objects (dogâ€™s heads and balls in the upper figures, legs in the lower figures) with very fast motions are blurrily reconstructed by all the SOTA models, including the models plugged with â€˜distance indexingâ€™ and â€˜iterative reference-based estimationâ€™ [44] (denoted as [D,R]), our BiM-VFI successfully restored much cleaner frames than other methods. Also, we compared our BiM-VFI with other SOTA models (Fig. 5) for a fixed-time VFI dataset, XTest-set [35]. Even though other SOTA VFI models are trained on fixedtime datasets, our BiM-VFI outperforms them qualitatively. As shown in Fig. 5, the small objects (streetlight pole in the 1st row, power lines in the 3rd row, car plate numbers in the 5th row) and the complex boundaries between a car wheel and a rear bumper in the 2nd row are well constructed in the interpolated frame by our BiM-VFI, while the other SOTA models fail to interpolate repeated patterns (building wall with vertical strips in the 1st row) or incurred blurs in object boundaries. It is worth noting for the estimated flows in the 3rd, 5th, 7th, and 9th columns that our BiM-VFI can estimate sharper flows even in object boundaries and repeated patterns compared to other SOTA models. 4.3. Quantitative results Tab. 1 compares our BiM-VFI and SOTA methods for arbitrary time interpolation test datasets. While our BiMVFI underperforms in terms of pixel-centric metrics for Vimeo90K-septuplet [41], it demonstrated significantly higher performance by a large margin in terms of the perceptual metrics for Vimeo90K-septuplet and SNU-FILMarb [6]. For the Vimeo90K-septuplet and the SNU-FILMarb (Medium, Hard and Extreme) data sets, our BiM-VFI outperformed all other VFI models in terms of perceptual metrics (LPIPS, STLPIPS, and NIQE) except M2MPWC [7] with 4.657 only in NIQE metric for SNU-FILMarb Extreme data set. As shown in Tab. 1, there are large metric gaps in pixel-centric metrics (PSNR and SSIM) between our BiM-VFI and most of the other SOTA methods because our BiM-VFI assumes uniform motion in interpolated frame reconstruction at inference. In spite of relatively large values of pixel-centric metrics for the other SOTA methods, their reconstructed interpolated frames are very blurry as shown in Fig. 1. These pixel-centric metrics conducted on test datasets containing non-uniform motions do not match the perceptual qualities as reported in [15, 44]. Tab. 2 shows the frame interpolation results for fixedtime interpolation data sets. As shown, our BiM-VFI also achieved comparable or superior performance to other SOTA models across most datasets although it was not trained for frame interpolation tasks at fixed target times. 4.4. Ablation studies We conducted ablation studies on the proposed BiM, KDVCF, and model components. First, for the BiM, we performed ablations by replacing R with time indexing T and by removing the Î¦ component, where we supervised the experiments using our proposed KDVCF on Vimeo90K septuplet datasets [41]. When the Î¦ component was removed, we excluded corresponding network components from the BiMFN. As shown in Tab. 3 (a), our proposed BiM achieved the best perceptual metric, which indicates BiM can resolve ambiguity at training, thus interpolating frames with much fewer blurs under uniform-motion scenarios. In Tab. 3 (b), we compared our proposed KDVCF with the supervision using FlowFormer [8]-extracted flows and the training without any flow supervision. As shown, our proposed KDVCF yielded higher perceptual metrics than the pre-trained flow supervision, confirming that our flow estimations from PT provide more suitable BiM and supervision for training our BiM-VFI. Lastly, in Tab. 3 (c), we compared replacing elementwise multiplication in BiM-MConv with a module that concatenates FV , FR, and FÎ¦ followed by a convolution layer, and removing the adaptive upsampling (CAUN). Our BiMFN",
        "Vimeo 90K-triplet SNU-FILM XTest Easy Medium Hard Extreme AMT-G [18] 0.019/0.012/5.327 0.022/0.008/4.822 0.035/0.015/4.924 0.060/0.028/4.993 0.112/0.068/4.993 0.134/0.097/6.883 M2M-PWC [7] 0.025/0.017/5.346 0.021/0.009/4.765 0.036/0.016/4.824 0.063/0.033/4.773 0.212/0.057/6.082 0.211/0.135/6.005 UPRNet [13] 0.022/0.015/5.367 0.018/0.008/4.703 0.034/0.015/4.853 0.062/0.030/4.975 0.110/0.067/5.052 0.095/0.059/6.372 RIFE [9] 0.022/0.014/5.240 0.018/0.007/4.709 0.032/0.014/4.813 0.066/0.037/4.872 0.138/0.099/4.935 0.295/0.209/6.419 XVFI [35] 0.028/0.019/5.236 0.027/0.015/4.829 0.040/0.024/4.847 0.068/0.043/4.780 0.120/0.083/4.618 0.109/0.072/6.041 IFRNet [16] 0.019/0.013/5.267 0.020/0.008/4.820 0.032/0.013/4.889 0.056/0.027/4.890 0.113/0.073/4.856 0.190/0.134/5.892 EMA-VFI [45] 0.020/0.013/5.350 0.019/0.008/4.704 0.033/0.015/4.847 0.059/0.030/4.979 0.113/0.073/5.087 0.139/0.099/7.008 Ours 0.020/0.012/5.283 0.017/0.006/4.678 0.029/0.011/4.773 0.052/0.022/4.863 0.097/0.052/4.942 0.089/0.060/6.717 Table 2. Quantitative comparisons on fixed time interpolation datasets. Ablation",
        "lpips stlpips niqe (a) BiM (T) 0.098 0.077 6.838 (R) Train failed (T, Î¦) 0.074 0.045 6.222 (b) KDVCF w/o KDVCF 0.076 0.050 6.358 w Flow loss 0.076 0.049 6.334 (c) Modules BiM concat 0.071 0.044 6.059 w/o CAUN 0.076 0.045 6.124 Full Ours 0.070 0.043 6.009 Table 3. Ablation studies on BiM, KDVCF, and modules. design was found to better leverage the BiM, and the CAUN not only effectively upsamples the flow but also improves the perceptual quality of frame interpolation results. 4.5. Limitation on Uniform Motion Assumption Our BiM-VFI is limited for frame interpolation under a uniform motion assumption at inference time, due to the unavailability of the target BiM, which is an inherit limitation as for all other VFI methods. Fig. 6 demonstrates that other VFI methods, such as those employing time indexing (EMA-VFI [45]) and distance indexing ([D,R]- EMA-VFI [44]), also fail to adequately interpolate the target frame with non-uniform motions, where the boundary of the tree at the target frame (indicated by the blue line) does not align with the interpolation results from EMA-VFI, [D,R]-EMA-VFI, or our BiM-VFI (indicated by the green line). Moreover, the boundary of the tree from the interpolated frame using EMA-VFI that employs time indexing is well aligned with those of models using distance indexing or BiM under the uniform motion assumption. This suggests that the time-indexing-based method implicitly tends to comply with the uniform motion assumption from training where uniform motion is dominant, thereby supporting that the uniform motion assumption at inference is more likely enforced for all the methods in VFI. Ours [D,R]-EMA-VFI EMA-VFI GT Figure 6. Visual comparisons of interpolating non-uniform motion target frame. 2nd column is zoom-in version of the red boxes in 1st column."
      ]
    },
    {
      "section": "Conclusion",
      "chunks": [
        "We proposed Bidirectional Motion field-guided VFI (BiMVFI), which consists of (i) a distinct motion descriptor, named Bidirectional Motion Field (BiM); (ii) a BiM-guided FlowNet (BiMFN) and Context-Aware Upsampling Network (CAUN); and (iii) a Knowledge Distillation for VFIcentric Flow supervision (KDVCF). Our BiM-VFI trained with the BiM can resolve the time-to-location ambiguity during training and interpolate clear frames by not averaging all the possible interpolation results. In inference, our BiM-VFI can interpolate frames with very clean frames under uniform motion assumptions. Extensive experiments have verified the effectiveness of our BiM-VFI, perceptually outperforming the recent SOTA models significantly."
      ]
    },
    {
      "section": "Limitations",
      "chunks": [
        "C. Limitation Our KDVCF requires approximately twice the training time compared to training solely with the student process, as both the teacher and student processes are trained simultaneously. However, the model trained with KDVCF demonstrated its effectiveness in perceptual metrics compared to models supervised with pre-trained flow models or without flow supervision. It is also noteworthy that only the student process remains during inference, so the inference runtime is the same as that of models trained without KDVCF."
      ]
    },
    {
      "section": "Acknowledgments",
      "chunks": [
        "This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korean Government [Ministry of Science and ICT (Information and Communications Technology)] (Project Number: RS-2022-00144444, Project Title: Deep Learning Based Visual Representational Learning and Rendering of Static and Dynamic Scenes, 100%)."
      ]
    },
    {
      "section": "References",
      "chunks": [
        "[1] Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert, and Michel Barlaud. Two deterministic half-quadratic regularization algorithms for computed imaging. In IEEE Int. Conf. Image Process., pages 168â€“172. IEEE, 1994. 1 [2] Shuhong Chen and Matthias Zwicker. Improving the perceptual quality of 2d animation interpolation. In Eur. Conf. Comput. Vis., pages 271â€“287. Springer, 2022. 2 [3] Myungsub Choi, Heewon Kim, Bohyung Han, Ning Xu, and Kyoung Mu Lee. Channel attention is all you need for video frame interpolation. In AAAI, pages 10663â€“10671, 2020. 6, 7, 1, 3, 4 [4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. Learn. Represent., 2021. 3 [5] Abhijay Ghildyal and Feng Liu. Shift-tolerant perceptual similarity metric. In Eur. Conf. Comput. Vis., pages 91â€“107. Springer, 2022. 6 [6] Zujin Guo, Wei Li, and Chen Change Loy. Generalizable implicit motion modeling for video frame interpolation. arXiv preprint arXiv:2407.08680, 2024. 6, 7, 1, 3, 4, 5, 8, 9 [7] Ping Hu, Simon Niklaus, Stan Sclaroff, and Kate Saenko. Many-to-many splatting for efficient video frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 3553â€“3562, 2022. 2, 7, 8, 3, 4 [8] Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang, Ka Chun Cheung, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer: A transformer architecture for optical flow. In Eur. Conf. Comput. Vis., pages 668â€“685. Springer, 2022. 2, 7 [9] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate flow estimation for video frame interpolation. In Eur. Conf. Comput. Vis., pages 624â€“642. Springer, 2022. 2, 3, 4, 7, 8 [10] Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Liteflownet: A lightweight convolutional neural network for optical flow estimation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8981â€“8989, 2018. 3, 5 [11] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Adv. Neural Inform. Process. Syst., 28, 2015. 2, 3 [12] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik Learned-Miller, and Jan Kautz. Super slomo: High quality estimation of multiple intermediate frames for video interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 9000â€“9008, 2018. 2 [13] Xin Jin, Longhai Wu, Jie Chen, Youxin Chen, Jayoon Koo, and Cheul-hee Hahm. A unified pyramid recurrent network for video frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1578â€“1587, 2023. 2, 7, 8, 3, 4, 5 [14] Rico Jonschkowski, Austin Stone, Jonathan T Barron, Ariel Gordon, Kurt Konolige, and Anelia Angelova. What matters in unsupervised optical flow. In Eur. Conf. Comput. Vis., pages 557â€“572. Springer, 2020. 1 [15] Simon Kiefhaber, Simon Niklaus, Feng Liu, and Simone Schaub-Meyer. Benchmarking video frame interpolation, 2024. 7 [16] Lingtong Kong, Boyuan Jiang, Donghao Luo, Wenqing Chu, Xiaoming Huang, Ying Tai, Chengjie Wang, and Jie Yang. Ifrnet: Intermediate feature refine network for efficient frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1969â€“1978, 2022. 2, 3, 4, 7, 8 [17] Hyeongmin Lee, Taeoh Kim, Tae-young Chung, Daehyun Pak, Yuseok Ban, and Sangyoun Lee. Adacof: Adaptive collaboration of flows for video frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 5316â€“5325, 2020. [18] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, ChunLe Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 9801â€“9810, 2023. 2, 3, 4, 7, 8, 5 [19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Int. Conf. Comput. Vis., pages 10012â€“10022, 2021. 3 [20] I Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 3 [21] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 3 [22] Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, and Jiaya Jia. Video frame interpolation with transformer. In IEEE Conf. Comput. Vis. Pattern Recog., pages 3532â€“3542, 2022. 2, 3 [23] Kunming Luo, Chuan Wang, Shuaicheng Liu, Haoqiang Fan, Jue Wang, and Jian Sun. Upflow: Upsampling pyramid for unsupervised optical flow learning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1045â€“1054, 2021. 5 [24] Simon Meister, Junhwa Hur, and Stefan Roth. Unflow: Unsupervised learning of optical flow with a bidirectional census loss. In AAAI, 2018. 1 [25] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a â€œcompletely blindâ€ image quality analyzer. IEEE Sign. Process. Letters, 20(3):209â€“212, 2012. 6, 4 [26] Simon Niklaus and Feng Liu. Softmax splatting for video frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 5437â€“5446, 2020. 2 [27] Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive convolution. In IEEE Conf. Comput. Vis. Pattern Recog., pages 670â€“679, 2017. 2",
        "[28] Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive separable convolution. In Int. Conf. Comput. Vis., pages 261â€“270, 2017. 2 [29] Junheum Park, Keunsoo Ko, Chul Lee, and Chang-Su Kim. Bmbc: Bilateral motion estimation with bilateral cost volume for video interpolation. In Eur. Conf. Comput. Vis., pages 109â€“125. Springer, 2020. 2 [30] Junheum Park, Chul Lee, and Chang-Su Kim. Asymmetric bilateral motion estimation for video frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 14539â€“ 14548, 2021. [31] Junheum Park, Jintae Kim, and Chang-Su Kim. Biformer: Learning bilateral motion estimation via bilateral transformer for 4k video frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1568â€“1577, 2023. 2 [32] Markus Plack, Karlis Martins Briedis, Abdelaziz Djelouah, Matthias B Hullin, Markus Gross, and Christopher Schroers. Frame interpolation transformer and uncertainty guidance. In IEEE Conf. Comput. Vis. Pattern Recog., pages 9811â€“ 9821, 2023. 2 [33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234â€“241. Springer, 2015. 3, 1 [34] Zhihao Shi, Xiangyu Xu, Xiaohong Liu, Jun Chen, and Ming-Hsuan Yang. Video frame interpolation transformer. In IEEE Conf. Comput. Vis. Pattern Recog., pages 17482â€“ 17491, 2022. 2, 3 [35] Hyeonjun Sim, Jihyong Oh, and Munchurl Kim. Xvfi: extreme video frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 14489â€“14498, 2021. 6, 7, 8, 1, 3, 4 [36] Li Siyao, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris Metaxas, Chen Change Loy, and Ziwei Liu. Deep animation video interpolation in the wild. In IEEE Conf. Comput. Vis. Pattern Recog., pages 6587â€“6595, 2021. 2 [37] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8934â€“8943, 2018. 2, 5 [38] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Eur. Conf. Comput. Vis., pages 402â€“419. Springer, 2020. 2, 3, 5 [39] Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P Allebach, and Chenliang Xu. Zooming slow-mo: Fast and accurate one-stage space-time video super-resolution. In IEEE Conf. Comput. Vis. Pattern Recog., pages 3370â€“3379, 2020. 2 [40] Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, and MingHsuan Yang. Quadratic video interpolation. Adv. Neural Inform. Process. Syst., 32, 2019. 3 [41] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. Video enhancement with task-oriented flow. Int. J. Comput. Vis., 127:1106â€“1125, 2019. 2, 6, 7, 1, 3, 4 [42] Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen, Gangshan Wu, and Limin Wang. Extracting motion and appearance via inter-frame attention for efficient video frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 5682â€“5692, 2023. 2 [43] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE Conf. Comput. Vis. Pattern Recog., pages 586â€“595, 2018. 6 [44] Zhihang Zhong, Gurunandan Krishnan, Xiao Sun, Yu Qiao, Sizhuo Ma, and Jian Wang. Clearer frames, anytime: Resolving velocity ambiguity in video frame interpolation. In Eur. Conf. Comput. Vis., 2024. 2, 3, 4, 7, 8, 5 [45] Kun Zhou, Wenbo Li, Xiaoguang Han, and Jiangbo Lu. Exploring motion ambiguity and alignment for high-quality video frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 22169â€“22179, 2023. 4, 7, 8, 3, 5",
        "BiM-VFI: Bidirectional Motion Field-Guided Frame Interpolation for Video with Non-uniform Motions"
      ]
    },
    {
      "section": "Appendix",
      "chunks": [
        "In this supplementary material, we first provide additional details of our proposed BiM-VFI. Especially, the detailed network structure of CAUN and SN, loss functions, implementation details, and the proof of how BiM can describe bidirectional motion distinctly are explained in Appendix A. Subsequently, in Appendix B, we provided additional experimental results that could not be included in the main paper due to the page limitation. In Appendix B.1, pixel-centric metrics (PSNR and SSIM) in SNU-FILMarb [6], Vimeo90K-triplet [41], SNU-FILM [3], and XTestsingle [35], and Ã—8 interpolation on XTest dataset are provided. Also, in Appendix B.4, we provided the results of the user study we conducted for interpolated videos from various VFI methods. Lastly, in Appendix B.5, we provided additional qualitative comparisons on SNU-FILM-arb [6] datasets. A. Additional Details A.1. Structure of Content-Aware Upsampling Network (CAUN) Fig. 7 depicts the detailed architecture of our proposed Content-Aware Upsampling Network, CAUN (Sec. 3.3). CAUN is designed to construct adaptive upsampling kernels that upsample flows while preserving high-frequency details, especially sharp boundaries and small objects. For this, CAUN effectively utilizes and integrates multiscale features. Context features F l,c and F l,c are consists of multi-scale features (F l,c,0 , F l,c,1 , F l,c,2 ) and (F l,c,0 , F l,c,1 , F l,c,2 ), respectively, where F l,c,j i is H/2j Ã— W/2j-sized context feature map of Il i for i âˆˆ{0, 1} and j âˆˆ{0, 1, 2}. Note that F l,c,2 and F l,c,2 are of the same spatial sizes as ËœV l tâ†’0 and ËœV l tâ†’1. So, the context features F l,c,2 and F l,c,2 can be directly aligned to target time t by warping via ËœV l tâ†’0 and ËœV l tâ†’1, respectively. However, to warp F l,c,1 and F l,c,1 , the two flows ËœV l tâ†’0 and ËœV l tâ†’1 must be bilinearly upsampled by a factor of 2 and their magnitudes are scaled by a factor of 2 to match with the spatial size of the features F l,c,1 and F l,c,1 . In this sense, ËœV l tâ†’0 and ËœV l tâ†’1 are further upsampled by a factor of 4, and their magnitudes are scaled by a factor of 4 to warp the features F l,c,0 and F l,c,0 . Then, the warped features are concatenated and further passed through several convolution layers and PixelShuffle layers to integrate multi-scale features. Finally, adaptive kernels Kl,Ã—2 tâ†’0, Kl,Ã—2 tâ†’1, Kl,Ã—4 tâ†’0, and Kl,Ã—4 tâ†’0 are obtained for input with the integrated multiscale features, where Kl,Ã—2 tâ†’0, Kl,Ã—2 tâ†’1 âˆˆR H 4 Ã— W 4 Ã—9Ã—4 and Kl,Ã—4 tâ†’0, Kl,Ã—4 tâ†’1 âˆˆR H 4 Ã— W 4 Ã—9Ã—16. Kl,Ã—2 tâ†’0 and Kl,Ã—2 tâ†’1 are pixelwise convolved with 3 Ã— 3 neighboring pixels of ËœV l tâ†’0 and ËœV l tâ†’1, respectively, to adaptively upsample ËœV l tâ†’0 and ËœV l tâ†’1 by a factor of 2, thus yielding Vl,Ã—0.5 tâ†’0 and Vl,Ã—0.5 tâ†’1 . Similarly, Kl,Ã—4 tâ†’0 and Kl,Ã—4 tâ†’0 are pixel-wise convolved with 3 Ã— 3 neighboring pixels of ËœV l tâ†’0 and ËœV l tâ†’1, then yielding Vl tâ†’0 and Vl tâ†’1, respectively, where Vl tâ†’0 and Vl tâ†’1 are of the same sizes as those of the source images at l-th level, Il and Il 1. Vl,Ã—0.5 tâ†’0 , Vl,Ã—0.5 tâ†’1 , Vl tâ†’0, and Vl tâ†’1 are further utilized in Synthesis Network (SN) to warp the source images and their context features with more precise flows. A.2. Structure of Synthesis Network (SN) We employed a simple U-Net [33] for our Synthesis Network (SN) as depicted in Fig. 8. The multiscale flows from CAUN, which include (ËœV l tâ†’0, ËœV l tâ†’1), (V0,Ã—0.5 tâ†’1 , Vl,Ã—0.5 tâ†’1 ) and (Vl tâ†’0, Vl tâ†’1), are used to warp the multi-scale context features (F l,c,2 , F l,c,2 ), (F l,c,1 , F l,c,1 ), and (F l,c,0 , F l,c,0 ). As depicted in Fig. 8, the warped multiscale context features are passed through the U-Net, finally yielding a blending mask Ol and a residual image Ë†Il,res t at l-th level. The resulting Ë†Il,res t and Ol from the U-Net are employed to construct final interpolation result at l-th level, Ë†Il t, as follow: Ë†Il t = bw(Il 0, Vl tâ†’0) âˆ—Ïƒ(Ol)+ bw(Il 1, Vl tâ†’1) âˆ—(1 âˆ’Ïƒ(Ol)) + Ë†Il,res t , (6) where bw(Â·, Â·) is a backward warping function and Ïƒ(Â·) is a sigmoid function. A.3. Loss Functions Our training objectives consist of student loss LPS, and teacher loss LPT . The teacher loss will be discussed first, and followed by the student loss. To supervise photometric reconstruction of the teacher process, the charbonnier loss [1] Lchar and the census loss [24] Lcss are used as follow: Ll char,PT = Î»char,PT Lchar(Ë†Il,PT , Il), Ll css,PT = Î»css,PT Lcss(Ë†Il,PT , Il), Ll pho,PT = Ll char,PT + Ll css,PT , (7) where l is the current pyramid level, Î»char,PT and Î»css,PT are weights for each loss. Furthermore, the first-order edgeaware smoothness loss [14] Ls1 is used to ensure smooth",
        "W W W W W W ğ¹ğ¹0 ğ‘™ğ‘™,ğ‘ğ‘,1 ğ¹ğ¹1 ğ‘™ğ‘™,ğ‘ğ‘,1 ğ¹ğ¹0 ğ‘™ğ‘™,ğ‘ğ‘,0 ğ¹ğ¹1 ğ‘™ğ‘™,ğ‘ğ‘,0 à·©ğ•ğ•ğ‘¡ğ‘¡â†’1 ğ‘™ğ‘™ à·©ğ•ğ•ğ‘¡ğ‘¡â†’0 ğ‘™ğ‘™ à·©ğ•ğ•ğ‘¡ğ‘¡â†’1 ğ‘™ğ‘™ Conv Bilinear Upsample Ã—2 Bilinear Upsample Ã—4 Bilinear Upsample Ã—4 Bilinear Upsample Ã—2 à·©ğ•ğ•ğ‘¡ğ‘¡â†’0 ğ‘™ğ‘™ Conv PixelShuffle Ã—2 ğ¹ğ¹0 ğ‘™ğ‘™,ğ‘ğ‘,2 ğ¹ğ¹1 ğ‘™ğ‘™,ğ‘ğ‘,2 à·©ğ•ğ•ğ‘¡ğ‘¡â†’0 ğ‘™ğ‘™ à·©ğ•ğ•ğ‘¡ğ‘¡â†’1 ğ‘™ğ‘™ Conv PixelShuffle Ã—2 Conv Conv ğ¾ğ¾ğ‘¡ğ‘¡â†’0 ğ‘™ğ‘™,Ã—2 ğ¾ğ¾ğ‘¡ğ‘¡â†’1 ğ‘™ğ‘™,Ã—2 ğ¾ğ¾ğ‘¡ğ‘¡â†’0 ğ‘™ğ‘™,Ã—4 ğ¾ğ¾ğ‘¡ğ‘¡â†’1 ğ‘™ğ‘™,Ã—4 Reshape Reshape [H/4,W/4,C1] [H/4,W/4,2] [H/4,W/4,2] [H/2,W/2,2] [H/2,W/2,C1] [H/2,W/2,C1] [H/2,W/2,2] [H/4,W/4,C1] [H,W,2] [H,W,2] [H/4,W/4,9,16] [H/4,W/4,9,16] [H/4,W/4,9,4] [H/4,W/4,9,4] [H,W,C0] [H,w,C0] W Backwarp. Adaptive Upsample Ã—2 Adaptive Upsample Ã—2 Adaptive Upsample Ã—4 Adaptive Upsample Ã—4 à·©ğ•ğ•ğ‘¡ğ‘¡â†’0 ğ‘™ğ‘™ à·©ğ•ğ•ğ‘¡ğ‘¡â†’1 ğ‘™ğ‘™ à·©ğ•ğ•ğ‘¡ğ‘¡â†’0 ğ‘™ğ‘™ à·©ğ•ğ•ğ‘¡ğ‘¡â†’1 ğ‘™ğ‘™ ğ•ğ•ğ‘¡ğ‘¡â†’0 ğ‘™ğ‘™ ğ•ğ•ğ‘¡ğ‘¡â†’1 ğ‘™ğ‘™ ğ•ğ•ğ‘¡ğ‘¡â†’0 ğ‘™ğ‘™,Ã—0.5 ğ•ğ•ğ‘¡ğ‘¡â†’0 ğ‘™ğ‘™,Ã—0.5 Figure 7. Detailed architecture of our Content-Aware Upsampling Network (CAUN). W ğ¹ğ¹1 ğ‘™ğ‘™,ğ‘ğ‘,0, ğ¼ğ¼1 ğ‘™ğ‘™ ğ•ğ•ğ‘¡ğ‘¡â†’1 ğ‘™ğ‘™ ğ•ğ•ğ‘¡ğ‘¡â†’0 ğ‘™ğ‘™ W ğ¹ğ¹0 ğ‘™ğ‘™,ğ‘ğ‘,0, ğ¼ğ¼0 ğ‘™ğ‘™ ConvBlock DownBlock DownBlock Conv 3Ã—3 PReLU Conv 3Ã—3 PReLU Conv 2Ã—2 stride 2 PReLU Conv 3Ã—3 PReLU Conv 3Ã—3 PixelShuffle Ã—2 PReLU Conv 3Ã—3 PReLU W ğ¹ğ¹1 ğ‘™ğ‘™,ğ‘ğ‘,1 ğ•ğ•ğ‘¡ğ‘¡â†’1 ğ‘™ğ‘™,Ã—0.5 ğ•ğ•ğ‘¡ğ‘¡â†’0 ğ‘™ğ‘™,Ã—0.5 W ğ¹ğ¹0 ğ‘™ğ‘™,ğ‘ğ‘,1 W ğ¹ğ¹1 ğ‘™ğ‘™,ğ‘ğ‘,2 à·©ğ•ğ•ğ‘¡ğ‘¡â†’1 ğ‘™ğ‘™ à·©ğ•ğ•ğ‘¡ğ‘¡â†’0 ğ‘™ğ‘™ W ğ¹ğ¹0 ğ‘™ğ‘™,ğ‘ğ‘,2 ConvBlock UpBlock UpBlock ğ‘‚ğ‘‚ğ‘™ğ‘™âˆ’1 C C C C C Conv 3x3 W C ConvBlock DownBlock UpBlock Backwarp. Concat. ğ‘‚ğ‘‚ğ‘™ğ‘™ Ì‚ğ¼ğ¼ğ‘™ğ‘™,res Bilinear Upsample Ã—2 Figure 8. Detailed architecture of our Synthesis Network (SN). teacher flows excluding object boundaries, and the regularization loss Lreg is utilized to force V l,PT tâ†’t|0t and V l,PT tâ†’t|t1 to be uniform vector fields of all zeros, where the input BiM of teacher process (Eq. (4), Eq. (5)) is used to guide the two flows to be zero flows: Ll s1 = Î»s1(Ls1(V l,PT tâ†’0 ) + Ls1(V l,PT tâ†’1 )), Ll reg = Î»reg(L2(V l,PT tâ†’t|0t) + L2(V l,PT tâ†’t|t1)), Ll flo,PT = Ll s1 + Ll reg, (8) where Î»s1 and Î»reg are weights for each loss. The photometric loss for the student process is constructed in the same manner as the teacher process, which is given by: Ll char,PS = Î»char,PSLchar(Ë†Il,PS, Il), Ll css,PS = Î»css,PSLcss(Ë†Il,PS, Il), Ll pho,PS = Ll char,PS + Ll css,PS, (9) where Î»char,PS and Î»css,PS are the weights for their respective losses. The flows of the student process will be supervised by a flow distillation loss that enforces the flows of the student process to get closer to those of the teacher process, which is given by: Ll flo,PS = Î»distill(L2(V l,PS tâ†’0 âˆ’sg(V l,PT tâ†’0 )) +L2(V l,PS tâ†’1 âˆ’sg(V PT tâ†’1))), (10) where Î»distill is a weighting factor, and sg(Â·) is a stop gradient function that is used to force the gradients to be only"
      ]
    },
    {
      "section": "Method",
      "chunks": [
        "SNU-FILM-arb XTest medium hard extreme Ã—8 psnr ssim psnr ssim psnr ssim psnr ssim lpips stlpips niqe RIFE [9] 36.31 0.981 31.86 0.952 27.20 0.895 30.58 0.904 0.153 0.114 7.393 IFRNet [16] 34.82 0.976 31.11 0.947 26.29 0.882 26.36 0.826 0.198 0.147 5.842 M2M-PWC [7] 36.54 0.982 31.92 0.951 27.13 0.892 30.81 0.912 0.080 0.047 6.521 AMT-S [18] 34.42 0.974 30.98 0.947 26.42 0.887 28.16 0.873 0.187 0.134 7.082 UPRNet [13] 36.70 0.982 31.9 0.951 27.08 0.893 30.50 0.905 0.093 0.058 6.148 EMA-VFI [45] 36.85 0.982 32.7 0.957 28.15 0.906 31.36 0.914 0.165 0.130 7.77 RIFE[D,R] [44] 36.17 0.981 31.59 0.949 27.05 0.891 26.93 0.839 0.232 0.169 6.477 IFRNet[D,R] [44] 35.92 0.981 31.18 0.947 26.54 0.886 28.76 0.891 0.147 0.096 7.054 AMT-S[D,R] [44] 34.78 0.978 30.48 0.944 26.15 0.886 29.27 0.886 0.098 0.055 6.409 EMA-VFI[D,R] [44] 35.75 0.980 31.02 0.946 26.37 0.885 25.75 0.833 0.258 0.192 6.928 ours 36.57 0.982 31.92 0.949 27.22 0.891 30.80 0.914 0.068 0.045 6.449 Table 4. Additional quantitative comparisons on arbitrary time interpolation datasets. activated for the student process. The overall loss for our BiM-VFI with KDVCF is defined as: L = Lâˆ’1 X l=0 Î³l pho(Ll pho,PT + Lpho,PS)+ Î³l flo(Lflo,PT + Lflo,PS), (11) where L is the total number of pyramid levels used in training, and Î³pho, and Î³flo are exponential weights for the photometric loss and the flow-centric loss, respectively, which are employed to weigh more supervision on larger-sized image resolutions. A.4. Implementation details We trained our BiM-VFI with a training split of Vimeo90k septuplet datasets [41]. We randomly crop the images to a resolution of 256 Ã— 256, flip horizontally and vertically, rotate, reverse temporally, and permute the color channels to augment the training data. We set the batch size to 32, and train the model for 400 epochs with an initial learning rate of 1 Ã—10âˆ’4. We gradually decay the initial learning rate using a Cosine annealing scheduler [21] and optimize our model using the AdamW optimizer [20]. Also, because the architecture of our BiM-VFI is based on a recurrent pyramid architecture, we employed resolution-aware adaptation for the pyramid hierarchy depth proposed by Jin et al. [13]. For training on Vimeo90K, we used 3 pyramid levels, while 5 pyramid levels are used for SNU-FILM [3] and SNU-FILM-arb [6], and 7 pyramid levels for Xtest [35]. As mentioned, our proposed KDVCF computes the BiM during training, and for inference time, the BiM is represented according to Eq. (2) corresponding to a uniform motion scenario. A.5. Distinct Description of BiM As discussed in Sec. 3.1 of the main paper, we proposed BiM as a distinct motion descriptor for non-uniform mo- ï± (a) Unique intersection of loci in case of k = 1 ï± (b) Unique intersection of loci in case of k Ì¸= 1 Figure 9. Visualization of intersection by two loci. tions, including accelerations, decelerations, and changing directions. To ensure the distinct descriptive power of BiM, we provide a mathematical analysis of how our BiM can explain the position of the intermediate pixel between given two corresponding pixels. Theorem 1. Let A and B be two fixed points, and let k be a positive real number. The point X such that the distance ratio AX BX = k and the angle âˆ AXB = Î¸ is unique. Proof. We start by describing the locus of points Xâ€² where âˆ AXâ€²B = Î¸. This locus forms an arc âŒ¢ AB where any point Xâ€² on the arc âŒ¢ AB satisfying âˆ AXâ€²B = Î¸. The locus of points Xâ€²â€² where AXâ€²â€² BXâ€²â€² = k varies inshape depending on the value of k. I) If k = 1 (Fig. 9a), this locus forms a perpendicular bisector of AB. In this case, the intersection of the arc âŒ¢ AB and the perpendicular bisector of AB is unique, thus the",
        "Vimeo 90K-triplet SNU-FILM XTest Complexity easy medium hard extreme single FLOPs Params psnr ssim psnr ssim psnr ssim psnr ssim psnr ssim psnr ssim (T) (M) AMT-G [18] 36.53 0.982 39.88 0.991 36.12 0.981 30.78 0.981 25.43 0.865 30.34 0.904 2.07 30.6 M2M-PWC [7] 35.49 0.978 39.66 0.991 35.74 0.980 30.32 0.980 25.07 0.863 30.81 0.900 0.26 7.6 UPRNet [13] 36.42 0.982 40.44 0.991 36.29 0.980 30.86 0.938 25.63 0.864 30.27 0.897 1.59 6.6 RIFE [9] 35.61 0.978 40.02 0.990 35.72 0.979 30.08 0.933 24.84 0.853 23.57 0.778 0.20 9.8 XVFI [35] 33.99 0.968 38.37 0.987 34.42 0.973 29.52 0.928 24.88 0.854 28.96 0.887 0.21 5.7 IFRNet [16] 36.16 0.980 40.10 0.991 36.12 0.978 30.63 0.936 25.27 0.861 27.53 0.847 0.79 19.7 EMA-VFI [45] 36.64 0.982 39.98 0.991 36.09 0.980 30.94 0.939 25.69 0.866 29.89 0.896 0.91 66.0 Ours 35.01 0.977 40.09 0.990 35.89 0.979 30.54 0.935 25.33 0.860 29.90 0.901 0.91 6.0 Table 5. Additional quantitative comparisons on fixed time interpolation datasets and the complexity of SOTA models. point satisfying the distance ratio AX BX = k and the angle âˆ AXB = Î¸ is unique. II) If k Ì¸= 1 (Fig. 9b), this locus forms an Apollonian circle [25]. In this case, if k > 1, point A is outside the circle, and point B is inside the circle. Conversely, if k < 1, point B is outside the circle, and point A is inside the circle. In any case, the resulting circle has only one intersection with the arc âŒ¢ AB, thus the point satisfying the distance ratio AX BX = k and the angle âˆ AXB = Î¸ is unique. By I) and II), for given two fixed points A and B, a positive real number k, it is concluded that the point X satisfying the distance ratio AX BX = k and the angle âˆ AXB = Î¸ is unique. B. Additional Experimental Results B.1. Quantitative Results We provided pixel-centric metrics (PSNR and SSIM) measured on SNU-FILM-arb [6] datasets and additional arbitrary time interpolation on XTest [35] to interpolate Ã—8 frames, which are tabulated in Tab. 4. As discussed in Sec. 4.3 of the main paper, while our BiM-VFI underperforms in pixel-centric metrics, it consistently outperforms the other SOTA methods on XTest [35] Ã—8 interpolation in terms of perceptual metrics, such as LPIPS and STLPIPS. In Tab. 5, we also provided pixel-centric metrics measured on fixed-time datasets (Vimeo 90K-triplet [41], SNUFILM [3], and XTest [35] single) and complexity comparisons between other SOTA methods. B.2. Computational complexity We additionally provide below #â€™s of parameters and FLOPs on each component for 256Ã—256-sized images. MFE CFE BiMFN CAUN SN Total #Params(M) 0.58 0.58 3.41 0.61 1.7 6.88 FLOPs(G) 12.02 12.02 18.81 24.64 78.49 We measured FLOPs for interpolating 1280Ã—720-sized source images and the total parameters used in the methods. As shown in Tab. 5, our BiM-VFI effectively reduced the number of parameters by employing a recurrent pyramid architecture, while having moderate computational complexity among the other SOTA methods in terms of FLOPs. GIMM-VFI-R EMA-VFI UPR AMT Ours #Params(M) 19.73 65.66 6.56 30.64 6.88 FLOPs(G) Runtime(ms) B.3. Additional ablation study We provide below the detection performance of small objects and object boundaries with and without CAUN module. As shown, the CAUN can help capture well small toes (top) and detect tight boundaries of the windmill blade (bottom), while failing without it. GT w/ CAUN w/o CAUN Also, as mentioned in Suppl., while our KDVCF increases the training time from 2.5 to 4 days using 4 A100 GPUs due to the PT process, it does not increase the inference time. We also compared the #â€™s of parameters, FLOPs, and runtime (measured on an A100 GPU with 1280Ã—768-sized images) in the below table. It can be noted that our BiM-VFI has a low number of parameters, showing moderate runtime. B.4. User Study We conducted a user study to show that our BiM-VFI with uniform motion BiM perceptually outperforms other SOTA methods. 21 participants were asked to choose",
        "Figure 10. Preference of interpolated videos between our BiMVFI and the other SOTA models measured by user study. the best-interpolated videos among AMT-S [18], UPRNet [13], EMA-VFI [45], [D,R]-AMT-S, and [D,R]-EMAVFI, where [D,R] indicates that distance indexing and iterative reference-based estimation, proposed by Zhong et al. [44], are plugged into the method. We used 9 test videos for blind subjective tests where the six 8Ã—-interpolated videos for the six VFI methods including our BiM-VFI are displayed simultaneously on the same screens for each test video. In order to remove any subjective bias to specific VFI methods, the six 8Ã—-interpolated videos for each test video are randomly ordered and presented to the participants in the blind subjective test. As shown in Fig. 10, our BiM-VFI dominantly outperforms the other SOTA methods in the subjective tests, by 61% preference against the other six VFI methods. B.5. Qualitative Results We provided additional qualitative comparisons with the SOTA methods in SNU-FILM-arb [6] extreme datasets. ",
        "Figure 11. Additional qualitative comparisons on SNU-FILM-arb [6] extreme datasets.",
        "Figure 12. Additional qualitative comparisons on SNU-FILM-arb [6] extreme datasets.",
        "Figure 13. Additional qualitative comparisons on SNU-FILM-arb [6] extreme datasets.",
        "Figure 14. Additional qualitative comparisons on SNU-FILM-arb [6] extreme datasets."
      ]
    }
  ]
}