{
  "paper_id": "9",
  "paper_title": "CRAFT: Extracting and Tuning Cultural Instructions from the Wild",
  "sections": [
    {
      "section": "FrontMatter",
      "chunks": [
        "Proceedings of the 2nd Workshop on Cross-Cultural Considerations in NLP, pages 42–47 August 16, 2024 ©2024 Association for Computational Linguistics CRAFT: Extracting and Tuning Cultural Instructions from the Wild Bin Wang♡, Geyu Lin♡, Zhengyuan Liu♡, Chengwei Wei§, Nancy F. Chen♡, † ♡Institute for Infocomm Research (I2R), A*STAR, Singapore §University of Southern California, USA †Centre for Frontier AI Research (CFAR), A*STAR, Singapore wang_bin@i2r.a-star.edu.sg"
      ]
    },
    {
      "section": "Abstract",
      "chunks": [
        "Large language models (LLMs) have rapidly evolved as the foundation of various natural language processing (NLP) applications. Despite their wide use cases, their understanding of culturally-related concepts and reasoning remains limited. Meantime, there is a significant need to enhance these models’ cultural reasoning capabilities, especially concerning underrepresented regions. This paper introduces a novel pipeline for extracting high-quality, culturally-related instruction tuning datasets from vast unstructured corpora. We utilize a self-instruction generation pipeline to identify cultural concepts and trigger instruction. By integrating with a general-purpose instruction tuning dataset, our model demonstrates enhanced capabilities in recognizing and understanding regional cultural nuances, thereby enhancing its reasoning capabilities. We conduct experiments across three regions: Singapore, the Philippines, and the United States, achieving performance improvement of up to 6%. Our research opens new avenues for extracting cultural instruction tuning sets directly from unstructured data, setting a precedent for future innovations in the field.1 1"
      ]
    },
    {
      "section": "Introduction",
      "chunks": [
        "Large language models (LLMs) like ChatGPT (Achiam et al., 2023), Claude, and Gemini (Reid et al., 2024) have demonstrated their proficiency in managing diverse tasks related to semantic understanding and text generation. Beyond acting as general task-solvers, the ability of LLMs to understand and reason with cultural nuances could play a crucial role in generating precise and personalized responses to benefit broader communities (Tao et al., 2023; Wang et al., 2024; Adilazuarda et al., 2024). Culture is a comprehensive concept encompassing traditions, customs, beliefs, values, and social 1Our models and datasets are available for future research at https://github.com/SeaEval/CRAFT. norms, all deeply rooted in historical contexts and continuously evolving over time. It is also intrinsically linked to languages and dialects, which can be sparsely represented in available resources. In the domain of LLMs, which initially train on vast amounts of unlabeled data, knowledge is systematically captured and structured through data-driven techniques. With limited model sizes, knowledge that occurs infrequently is often less effectively captured compared to more frequently occurring information (Kaplan et al., 2020). Additionally, the predominance of English in the pretraining corpus inherently biases these models towards Western perspectives, a consequence of the over-representation of English-language sources. This bias means that cultural concepts may be inadequately captured, especially for under-represented regions (Masoud et al., 2023). Consequently, LLMs struggle to effectively adapt to and represent diverse cultural concepts due to these inherent limitations in their training data. Expanding the cultural reasoning capabilities of LLMs could potentially be achieved by pre-training them on corpora from diverse languages. However, this approach is still expansive and challenging due to the difficulty in obtaining high-quality multilingual datasets (Bai et al., 2023; Singapore, 2023). Meanwhile, instruction fine-tuning could more directly impact end-user applications. However, the development of cultural instruction tuning sets is limited due to the high costs associated with collecting culturally relevant instruction sets, along with challenges in ensuring their quality and diversity. In this study, we pioneer the study of deriving instruction tuning sets from unlabeled corpora. Initially, we use keyword filtering to isolate culturally relevant concepts from a vast corpus containing over 600 billion English tokens. Subsequently, we then utilized these selected regional text segments to prompt LLMs for both questions and answers. Our evaluations focus on the context of Singapore 42 Massive Unstructured Text Regional Filtering Unstructured Regional Text Chunking Open/Closed Source LLM Question Mining What historical colonial landmarks can still be found in Singapore, and how have they contributed to the city's unique cultural blend?",
        "One of the historical colonial landmarks that can still be found in Singapore is the Rafﬂes Hotel, which was founded in 1887 by Sir Stamford Rafﬂes. This iconic hotel … Context Context-Dependent Answer Generation Figure 1: The CRAFT method involves creating instruction datasets tailored for culturally rich instruction by processing extensive unstructured data with large language models (LLMs). These specialized cultural instructions are then employed to improve the ability of LLMs to reason within cultural contexts through instruction fine-tuning. and extend to the Philippines and the US. Our experiments utilize the Mistral-7B model, combining general instructions with our specifically curated cultural instruction set. We observe performance improvement of up to 6% while maintaining intelligence in general subject knowledge as assessed by the MMLU dataset. Additionally, we also analyzed the impact of answer sources and the ratio of cultural instructions. Both the model and the curated cultural instruction-tuning dataset are made available for future research. 2"
      ]
    },
    {
      "section": "Related Work",
      "chunks": [
        "Our work aims to improve the cultural reasoning capability of LLMs. The capability of LLMs can be refined in their training schemes, such as pre-training, instruction tuning, and RLHF preference optimization. Current efforts often concentrate on fine-tuning using specific datasets derived from diverse multilingual corpus sources (Lin and Chen, 2023; Abbasi et al., 2023; Pipatanakul et al., 2023). Yet, this approach is costly in training and lacks transparency regarding the extent of cultural concepts incorporated into the model. Li et al. (2024) propose to leverage a set of opinion questions to gather views towards different cultural groups, which is then used to finetune a model. However, despite the efforts at data augmentation, the scope of these questions remains limited and fails to encompass a wide range of cultural dimensions. Therefore, in our research, we focus on extracting a wide variety of cultural instructions from large unlabeled corpora, ensuring guaranteed diversity. Given the complexity of sourcing high-quality instructional data, LLMs are employed to create synthetic question-answering pairs (Wang et al., 2022, 2023) and dialogue data through iterative processes (Ding et al., 2023). However, these approaches tend to concentrate on generating general instructions and dialogues, lacking the capability to produce culturally rich instructions. Prompting LLMs to directly generate cultural concepts is also challenging, as these concepts are sparsely distributed across various resources. 3"
      ]
    },
    {
      "section": "Methodology",
      "chunks": [
        "We introduce the CRAFT (Cultural ReAsoning with Instruction Fine-Tuning) method, designed to synthesize cultural instructions from a massive, unlabeled English corpus. The methodology is detailed in the following steps, as illustrated in Figure 1. Selective Data Extraction. We utilize SlimPajama (Soboleva et al., 2023) as our primary data source, which comprises an English corpus containing over 600 billion tokens. Given the sparse distribution of culturally relevant concepts within this vast dataset, and to manage the processing burden effectively, we propose an efficient filtering process using keywords to identify and extract culture-related concepts. Specifically, we curate a collection comprising a minimum of 150 words to represent each region. We then segment the documents into chunks no larger than 512 tokens each. From these segments, we retain only those text chunks that include at least two regional keywords, such as \"National Day Parade\" and \"Merlion\" for Singapore. Through analyzing a subset of over 200 billion English tokens, we successfully extracted 35,000 text segments for Singapore, 25,000 for the Philippines, and 35,000 43 Models SG-Eval PH-Eval US-Eval MMLU General LLMs ChatGPT-3.5 64.4 58.4 74.9 67.5 LLaMA-3-8B-Instruct (Meta_AI, 2024) 62.1 54.6 69.8 62.5 LLaMA-2-7B-Chat (Touvron et al., 2023) 39.8 35.4 50.1 44.5 Mistral-7B-Instruct-v0.2 62.1 48.6 60.9 58.1 Base Model: Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) Tuning w/ OpenHermes-2.5 62.6 46.4 65.5 58.5 CRAFTsg 68.3 / 64.2 46.2 / 44.8 65.2 / 64.6 59.8 / 58.4 CRAFTph 64.3 / 63.7 49.2 / 44.6 65.4 / 64.7 60.0 / 59.4 CRAFTus 65.3 / 63.2 48.4 / 44.8 67.1 / 63.5 60.3 / 59.7 Table 1: The main results for general LLMs and our model across three cultural evaluation datasets and the MMLU dataset, which assesses general knowledge. For our results, \"-/-\" denotes the scores for context-dependent answers and context-free answers, respectively. for the US. Automated Question Creation. Given the text chunks rich in cultural and local content, we prompt an off-the-shelf LLM to generate questions specifically related to each chunk, focusing on the cultural and regional concepts mentioned. Answer Production. To collect responses for the generated questions, we employ two approaches:"
      ]
    },
    {
      "section": "1) context-dependent answer generation, where the",
      "chunks": [
        "given context is provided to LLMs when forming answers, as shown in Figure 1, and 2) contextfree answer generation, allowing for responses that are more creative and less tailored to the immediate context. For automatic question creation and context-dependent answer generation, we utilize Zephyr-7B-Beta (Tunstall et al., 2023). For contextfree answer generation, we employ ChatGPT-3.5. Hybrid Instruction Tuning. After developing the cultural instructions, we compiled at least 20,000 instructions for each specified region. To ensure a balanced capability, we incorporated 50,000 single-round instructions from the OpenHermes2.5 (Teknium, 2023) dataset alongside random sampled 20,000 cultural instructions to fine-tune the Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) model. We adapt LoRA (Hu et al., 2021) training with a rank of 16 to effectively integrate our instruction tuning knowledge into the model. 4"
      ]
    },
    {
      "section": "Experiments",
      "chunks": [
        "Settings. For evaluation, we selected four datasets, including three culturally-focused ones: SG-Eval, Ph-Eval, and US-Eval (Wang et al., 2024). Each of these datasets is a human-crafted collection of multiple-choice questions and answers that probe cultural or regional knowledge. To assess knowledge capabilities in general subjects, we utilize a subset of the MMLU dataset (Hendrycks et al., 2020; Wang et al., 2024). To minimize the influence of randomness, we report the average results from five runs, using paraphrased prompt templates. Results and Discussion. Table 1 shows the main result where we listed four competitive LLMs and our baseline model tuned with 50,000 OpenHermes instructions. First, we witness that ChatGPT3.5 and recently released LLaMA-3-8B-Instruct are strong in capturing cultural concepts from different regions. Compared to Mistral-7B models, LLaMA3’s larger training data and multilingual tokens could help improve the model’s capability. Compared to our baseline, our CRAFT model demonstrates a clear performance improvement across all three regions. Notably, we observe more significant enhancements for Singapore-related questions, improving from 62.6% to 68.3%. We attribute this to two main reasons: 1) It is a culturally rich region with mixed and unique cultural origins, and 2) Singaporean concepts are well-documented in English corpora, even though they constitute a small portion. In contrast, cultural knowledge of the US appears more frequently during the pretraining stage, leading to better learning outcomes. Conversely, cultural knowledge from the Philippines is less documented and more dispersed. We encountered similar challenges when collecting culturally related text chunks for it. We found that identifying 44 Region Instruction and Answer SG Q: What historical significance does the Mandarin Orchard Singapore hold, and how has it been preserved while undergoing a major refurbishment? A1: The Mandarin Orchard Singapore holds historical significance as it is a landmark building on Orchard Road, with its two distinctive towers built in 1971 and 1973. ... A2: The Mandarin Orchard Singapore, previously known as the Mandarin Singapore, holds significant historical and cultural importance as one of Singapore’s ... SG Q: How has Kuala Lumpur’s mix of Malay, Indian, and Chinese cultures contributed to its unique experience and creative options for event planners, as mentioned in the context about Kuala Lumpur being a city of vibrant contrasts? A2: Kuala Lumpur’s blend of Malay, ... PH Q: How is the Araquio festival celebrated in Nueva Ecija, and what is its traditional month of celebration? A1: The Araquio festival is traditionally celebrated in the month of May in Nueva ... US Q: How did the discovery of gold in California in 1848 impact the transition from Mexican ranching to American farms and towns in the East Bay area, and what were the consequences for the Estudillo and Peralta families who owned land in the region?",
        "A2: The discovery of gold in California ... Table 2: Synthesized cultural instructions and answers. A1 and A2 refer to context-dependent and context-free answers, respectively. 54 57 60 63 66 69 0 2500 5000 7500 10000 12500 15000 17500 20000 SG-Eval MMLU Figure 2: Performance on SG-Eval and MMLU dataset. The CRAFT method with different ratios of Singapore cultural instruction data. US text chunks required the least data, followed by Singapore, with the Philippines proving to be the most challenging due to its scattered documentation. Lastly, when comparing context-dependent answers with context-free answers, we found that responses derived from context are consistently more reliable and informative. Consequently, instructions that utilize context-dependent answers consistently yield higher performance gains. While context-free answers could potentially become more informative using advanced models like GPT4, this enhancement would also benefit contextdependent answers. Cultural Instruction Ratios. In Figure 2, we investigate the effects of adding varying amounts of cultural instructions to a base of 50,000 general instructions. Cultural instructions are incrementally introduced into the training data at intervals of 2,500 samples. The results indicate that performance on the SG-Eval improves as more culturally related samples are added, suggesting that an increased number of cultural concepts are activated from the pre-training phase and further improved from the instructions. Concurrently, our analysis shows that the general performance as measured by the MMLU datasets remains consistent. Instruction Samples. In this section, we present a series of synthesized cultural instructions and their corresponding responses generated by various methods, as illustrated in Table 2. The samples demonstrate that culturally related questions can be effectively derived from culturally specific content. Context-dependent responses tend to incorporate more factual knowledge from the provided context compared to context-free answers. However, this can also lead to biases based on the limited facts presented. We observe that context-free responses often lack depth in knowledge-intensive instructions due to the model’s inherent limitations. 5"
      ]
    },
    {
      "section": "Conclusion",
      "chunks": [
        "In this paper, we introduce the CRAFT method, designed to synthesize cultural instructions from a vast, unlabeled corpus. We conduct experiments across three regions, with the potential for expansion to additional regions. Our pioneering selfinstruction techniques facilitate effective mining from unstructured data sources, enhancing both the diversity and quality of the synthesized instructions compared to previous studies. "
      ]
    },
        {
      "section": "Limitations",
      "chunks": [
        "In this study, we concentrate on mining cultural instructions from English-language corpora. However, it is important to recognize that cultural concepts are often deeply integrated with their respective languages, including those that are primarily spoken. Therefore, to effectively synthesize cultural concepts and instructions, adopting multilingual approaches (Liu et al., 2023; Lin et al., 2024) is essential to accommodate a broader range of cultural contexts."
      ]
    },
    {
      "section": "Acknowledgement",
      "chunks": [
        "This work is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-GC-2022-005). This research is also supported by the National Research Foundation, Singapore and Infocomm Media Development Authority, Singapore under its National Large Language Models Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore and Infocomm Media Development Authority, Singapore."
      ]
    }
  ]
}