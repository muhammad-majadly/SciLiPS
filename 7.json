{
  "paper_id": "7",
  "paper_title": "Conformity, Confabulation, and Impersonation: Persona Inconstancy in Multi-Agent LLM Collaboration",
  "sections": [
    {
      "section": "FrontMatter",
      "chunks": [
        "Proceedings of the 2nd Workshop on Cross-Cultural Considerations in NLP, pages 17–31 August 16, 2024 ©2024 Association for Computational Linguistics Conformity, Confabulation, and Impersonation: Persona Inconstancy in Multi-Agent LLM Collaboration Razan Baltaji1, Babak Hemmatian2, Lav R. Varshney1,2 1Department of Electrical and Computer Engineering 2Beckman Institute for Advanced Science and Technology University of Illinois Urbana-Champaign {baltaji, babak2, varshney}@illinois.edu"
      ]
    },
    {
      "section": "Abstract",
      "chunks": [
        "This study explores the sources of instability in maintaining cultural personas and opinions within multi-agent LLM systems. Drawing on simulations of inter-cultural collaboration and debate, we analyze agents’ pre- and post-discussion private responses alongside chat transcripts to assess the stability of cultural personas and the impact of opinion diversity on group outcomes. Our ﬁndings suggest that multi-agent discussions can encourage collective decisions that reﬂect diverse perspectives, yet this beneﬁt is tempered by the agents’ susceptibility to conformity due to perceived peer pressure and challenges in maintaining consistent personas and opinions. Counterintuitively, instructions that encourage debate in support of one’s opinions increase the rate of inconstancy. Without addressing the factors we identify, the full potential of multi-agent frameworks for producing more culturally diverse AI outputs will remain untapped. Warning: Contains potentially unsafe LLM responses. 1"
      ]
    },
    {
      "section": "Introduction",
      "chunks": [
        "A common ﬁnding in cognition research is that interactions between agents with varying opinions, such as those that arise in culturally diverse groups, can induce positive change, especially on multifaceted issues with no clear correct answer (Sulik et al., 2022). This change often takes the form of collective decisions that deviate from the group’s dominant initial response, reﬂecting in part the novel contributions of diverse members. While this research is traditionally done with human groups, advances in large language models (LLMs) allow cultural personas to be imposed on AI models through role prompting, such that the eﬀects of culture-induced diﬀerences in perspective Onboarding Reflection Debate Figure 1: An illustration of our experimental setup for a debate: a) Onboarding stage where agents are asked to report their opinions independently, b) Debate stage where agents participate in a debate moderated by a chat manager, c) Reﬂection stage where agents are asked to report their opinions independently based on the previous discussion. A similar setup is used for collaboration. on discussion outcomes can be simulated and interrogated in silico. Developments in multi-agent collaboration allow culture-sensitive AI instances to engage in debate about poignant issues, enabling more faithful simulations of diverse human interac17 tions. However, whether the outcomes would show the eﬀects of opinion diversity seen in humans depends on the models’ ability to fully adopt and reliably maintain the induced personas, as well as their use of human-like discourse dynamics that support the generation and spread of diverse ideas. Although prior work on multi-agent collaboration has demonstrated its beneﬁts in applications such as mathematical reasoning (Du et al., 2023), code generation (Hong et al., 2024) and common sense reasoning (Xiong et al., 2023), the stability and quality of discourse dynamics remain largely unstudied. It is particularly important to ﬁll this gap in cultural domains, as cultural personas tend to be more complex, less explicit in natural language, and subject to widespread model biases (Deshpande et al., 2023; Salewski et al., 2024). We speciﬁcally examine the ability of OpenAI’s GPT-3.5-Turbo model to simulate intercultural collaboration and debate using an experimental framework grounded in largescale polls about international relations opinions (Durmus et al., 2023). Using pre- and post-discussion private responses in conjunction with multi-agent chat transcripts, we test the stability of national personas and their individual opinions as well as the eﬀects of either on group outcomes.1 To preview, we ﬁnd multi-agent discussions to be eﬀective in producing collective decisions that more often reﬂect diverse perspectives. The beneﬁts, however, are reduced by the AI agents’ susceptibility to conformity during discussions, along with their imperfect ability to maintain consistent personas and opinions.",
        "These problems persist (and often amplify) even with instructions that emphasize debating in support of one’s opinion. Our results have implications for the use of multi-agent frameworks to reduce cultural bias in LLMs. The mere inclusion of diverse personas may not mitigate biases unless the sources of instability in their contributions, particularly conformity due to perceived peer pressure, are addressed. Addressing such issues would enhance the quality of wargaming simulations (Hua et al.,"
      ]
    },
    {
      "section": "2023) and related applications, which rely heav1Code is available at https://github.com/baltacir/CulturedAgents",
      "chunks": [
        "ily on consistent personas. As such, our work motivates further studies on how the constancy of AI personas can be improved. 2 Background Multi-agent collaboration frameworks draw inspiration from collaborative teamwork observed in human settings. In these frameworks, multiple instances of language models are employed within a cooperative environment to accomplish a complex task (Li et al., 2024; Chen et al., 2023). Collaborative behaviors in humans such as team dynamics and cohesion, leadership, and communication have been thoroughly studied in the human sciences (e.g., Gupta 2022). In contrast, few studies have examined behaviors in multi-agent language model systems. Li et al. (2023) observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. But Xiong et al. (2023) highlighted several consistency concerns in multi-agent collaboration, including agents compromising with the opponents and easily changing perspective in a debate, particularly when weaker models interact with superior LLMs. Zhang et al. (2023) placed agents in entirely homogeneous groups in terms of thinking patterns and compared the results to settings where one agent exhibits a diﬀerent thinking approach. They noted the tendency of LLM agents to produce humanlike social behaviors in these contexts, such as conformity due to perceived peer pressure. However, the multi-agent societies composed of agents with diﬀerent traits did not clearly diﬀer in performance. Prior research on collaborative behaviors in multi-agent LLM systems has been entirely focused on domains like mathematical reasoning where clear gold answers exist, rather than topics like politics where the constancy of personas and viewpoints is more important for faithfully simulating the real world and conﬂicting views may have complementary value. To address this gap, we study culture-sensitive AI ensembles using the GlobalOpinionQA, a dataset of cross-national surveys gathering diverse opinions on global issues across countries (Durmus et al., 2023). We assign AI agents with diﬀerent national personas to groups of ﬁve, where they provide initial responses to a question privately 18 before engaging in a peer-moderated discussion about it with the other agents. Once the group discussion is terminated and a collective response is determined, we ask each agent about its opinions on the issue in private once more. We focus our analysis on three situations where persona inconstancy is arguably rarely desirable. When agents express an opinion in line with their teammates during conversation that diﬀers from both their pre- and post-discussion response, we are faced with AI behavior that closely resembles conformity due to peer pressure as studied in humans (Asch, 1956; Brandstetter et al., 2014).",
        "A type of inconstancy more closely resembling confabulation in clinical conditions arises when the postdiscussion opinion bears no clear relation to either the pre-discussion response or any of the ideas proposed during discussion (Schacter and Coyle, 1995). The third type of inconstancy emerges when an agent instructed to represent a given national identity “role-plays” a diﬀerent persona simply because it was mentioned in discussion, arguably similar to impersonation behaviors in Antisocial Personality Disorder (Padhye and Gujar, 2012). By systematically manipulating the degree of disagreement within groups (measured using their entropy states), we explore whether the frequency of these disruptive behaviors changes as a function of opinion popularity, a key factor in the emergence of similar actions in humans. To test whether encouraging a debate rather than a collaboration environment would induce greater constancy in personas, we look at discourse outcomes across entropy states for both types of interaction. 3"
      ]
    },
    {
      "section": "Experiments",
      "chunks": [
        "We use GPT-3.5-turbo with AutoGen, an opensource framework for multi-agent collaboration (Wu et al., 2023). Our experimental setup follows a three-step process (see Appendix B for the full text of the instructions for each step for a debate example). During the Onboarding phase, AI agents are instructed to adopt the national personas present in the dataset for a given question and asked to respond to it in isolation. Agents’ responses are compared to the human survey distributions using a cross entropy loss. Agents whose responses do not align with the assigned persona are excluded. The diversity of opinions within a group is measured using Shannon entropy, applied to the opinions of agents during onboarding. This is calculated as S = −P o∈B p(o) log p(o), where p(o) represents the relative frequency of the unique opinion o in the set B of agent responses at onboarding. Seven entropy classes are obtained for a selection of ﬁve agents with the lowest entropy class corresponding to ﬁve agents with the same opinion and the highest entropy class with every agent presenting a unique response (see Table 1). To obtain a balanced distribution of diﬀerent entropy levels across all discussion groups, agent combinations corresponding to the least represented entropy class are chosen at each example as illustrated in Appendix B.2. Each debate or collaborative discussion is moderated by a chat manager who selects the order of agents for responding to the given question. Discussion is terminated when any agent requests it to be. The chat manager then summarizes the discussion and reports the group’s ﬁnal opinion. An example of a group debate is given in Appendix B. The agents then undergo a ﬁnal Reﬂection step where an assistant agent interviews them to answer the same question one last time independently and privately. Based on human research (Asch, 1956; Brandstetter et al., 2014), we focus our conformity analysis on the following entropy levels expected to show peer pressure to diﬀerent degrees: 4 ⊕1 (lone dissenter), 3 ⊕2 (close call), 3 ⊕1 ⊕1 (split opposition). Prior work has shown that even one additional person supporting the less popular view greatly reduces the pressure to conform. As such, we anticipated the rate of conformity to be highest in the lone dissenter and split opposition entropy classes. In contrast, we examine the rates of confabulation by comparing opinions during reﬂection with onboarding and intermediate opinions and impersonation using regular expressions across all entropy classes. 4"
      ]
    },
    {
      "section": "Results",
      "chunks": [
        "4.1 General Eﬀects of Diversity We ﬁrst consider the impact of the diversity of agents’ opinions during onboarding on the ﬁnal group predictions. We measure the ratio of examples in each entropy class with a 19 group prediction G of relative frequency p(G) as shown for the debate condition in Fig. 2. We observe that group prediction largely follows the distribution of opinions during onboarding across diﬀerent entropy levels, but it also allows for the generation of new responses regardless of entropy class, particularly for the group with the highest opinion diversity. The same holds for collaboration as displayed in Fig. 5. However, not all agents have the same degree of inﬂuence on group outcomes. The initiator of a discussion has an outsize impact on the group’s ﬁnal decision, regardless of entropy class and even when debate in support of one’s position is emphasized for all agents in the instructions (see Fig. 3 and Fig. 6). Perhaps unsurprisingly, this inﬂuence decreases with increasing diversity of opinions within the group. Nonetheless, initiators with minority opinions during onboarding do not always take advantage of their outsize inﬂuence, as they tend to change their expressed views during discussion based on a priori perceptions of group opinions. The mere mention of the identities of the debate participants pushes the initiator to change their opinion even before others have spoken (see Fig. 4). As this inconstancy is precipitated by opposing views of interlocutors, it can be characterized as conformity due to perceived peer pressure. The dynamics, however, are somewhat diﬀerent from what is observed in humans, as any opinion with a supporter seems to exert an inﬂuence regardless of its dominance within the group (Asch, 1956; Brandstetter et al., 2014). A similar pattern is observed for collaboration as displayed in Fig. 7. We further investigate the impact of group diversity on the opinions of individual agents upon reﬂection. We measure the percentage of agents with opinions of onboarding probability p(o) that change opinion during the reﬂection phase compared to agents that keep their opinion. We also measure the average ratio of showing a diﬀerent intermediate response compared to the reﬂection opinions for individual agents. We further compare the percentage of agents with an opinion corresponding to group prediction compared to agents with a diﬀerent reﬂection opinion. We are particularly interested in dominated agents as shown in bold in Tab. 1, as they are most important for diverse outcomes in real life settings. We observe that dominated agents tend to hold onto their opinions ﬁrmly in low entropy debates (S = 0.72). Conversely, they are most receptive to altering their opinions at states of high entropy, i.e., situations with greater opinion diversity (S = 1.92). When they do change their opinions on reﬂection, they largely conform to group predictions, demonstrating peer inﬂuence.",
        "Agents tend to express intermediate opinions diﬀering from their reﬂections most often in states of moderate entropy (S = 1.37), indicating considerable peer pressure. Once again, while the phenomena themselves are human-like, their dynamics based on group composition diﬀer signiﬁcantly from human studies, where lone dissenter and split opposition dominated agents are most likely to show both peer inﬂuence and peer pressure in their decision-making (Asch, 1956; Brandstetter et al., 2014). 4.2 Inconstant Personas In addition to studying the dynamics of group interactions, we point out two forms of persona inconstancy that can negatively impact the quality of complex reasoning in cultural multi-agent systems. One form is the agents’ tendency to adopt a diﬀerent persona motivated by previous context, particularly in the case of debate. Using a simple heuristic to ﬁnd instances when an agent says “As an X agent” where X is incompatible with their assigned national identity, we ﬁnd that agents adopt a diﬀerent persona in 3.12% of the messages in a debate. This is despite being explicitly told to stand ﬁrm in their beliefs and maintain their personas. Counterintuitively, there is much less impersonation in collaboration conditions (0.26%). Another form of inconstancy is an agent’s tendency to report an opinion not seen during the group interactions or onboarding, mimicking the confabulation of novel content observed in certain clinical conditions. We ﬁnd that 15.59% of the opinions at reﬂection come neither from onboarding nor from the debate statements of any agent. Collaboration conditions show lower, but still notable rates of confabulation (8.85%). 20 R = O R ̸= O S Group p(o) % T ̸= R R = G R ̸= G % T ̸= R R = G R ̸= G N 0.00 5 1.0 71.01∗ 0.71 20.20 79.8 28.99 0.17 83.78∗ 16.22 796 0.72 4 ⊕1 0.2 46.54 0.43 74.32 25.68 53.46 0.40 51.76 48.24 374 0.8 53.65 0.56 35.36 64.64 46.35 0.19 79.53 20.47 0.97 3 ⊕2 0.4 41.13 0.39 62.39 37.61 58.87 0.23 75.64 24.36 294 0.6 45.60 0.52 50.57 49.43 54.40 0.27 66.67 33.33 1.37 3 ⊕1 ⊕1 0.2 24.19 0.64 53.33 46.67 75.81 0.25 72.34 27.66 142 0.6 46.11 0.38 62.92 37.08 53.89 0.15 82.69 17.31 1.52 2 ⊕2 ⊕1 0.2 30.77 0.33 91.67 8.33 69.23 0.21 55.56 44.44 102 0.4 38.67 0.48 48.57 51.43 61.33 0.20 70.27 29.73 1.92 2 ⊕1 ⊕1 ⊕1 0.2 23.91 0.30 77.27 22.73 76.09 0.20 72.86 27.14 68 0.4 35.48 0.41 63.64 36.36 64.52 0.20 85.00 15.00 2.32 1 ⊕1 ⊕1 ⊕1 ⊕1 0.2 18.82 0.35 81.25 18.75 81.18∗ 0.18 73.91 26.09 38 Table 1: Peer Pressure and Peer Inﬂuence in Debate: Agents maintain their opinions O most strongly in the lowest entropy states during reﬂection R after debate, while being most open to changing their opinions in the highest entropy state. When dominated in discussions, agents are most resistant to opinion change during reﬂection in low entropy states (S = 0.72) and most susceptible to change in high entropy states (S = 1.92).",
        "During debates, agents express intermediate opinions T most contrary to their reﬂection and onboarding opinions at a moderate entropy level (S = 1.37), indicating high peer pressure. Dominated agents exhibit the highest peer inﬂuence by following group predictions during opinion changes in low entropy states (S = 0.72). Figure 2: Group Prediction follows the distribution of opinions during onboarding across diﬀerent onboarding entropy groups for debate while also generating new ideas particularly at the group of highest diversity. Groups are less likely to predict opinions with higher probability for debate compared to collaboration. 5"
      ]
    },
    {
      "section": "Discussion",
      "chunks": [
        "We found evidence of sophisticated interaction dynamics in a multi-agent framework for GPT3.5-Turbo personas with diﬀerent nationalities that discussed contentious international relations topics. Novel responses emerged from discussions even among entirely homogeneous groups, highlighting the generative nature of multi-agent LLM frameworks. However, a group’s initial opinion diversity, the entropy Figure 3: Initiators Dominate Group Prediction: agents follow the initiator opinion of a debate and often converge to the opinion of the initiator I. Initiators have less impact on a group prediction G in debate compared to collaboration. S of private responses during the onboarding stage before inter-agent discussion, emerged as a stronger determinant of conversation contents and collective decisions. This happened regardless of whether the agents were instructed to debate in support of their beliefs or asked to collaborate in service of collective decisionmaking. Opinion diversity seems to exert its eﬀect partly by reducing the outsize inﬂuence of chat initiators on collective decisions, but also by inducing them to change their espoused views 21 Figure 4: Initiator changes its opinion O during onboarding to I at the onset of a debate depending on onboarding entropy. Initiators are more likely to change their onboarding opinion as diversity of the group increases despite not observing agents opinions. Initiators of a debate change their opinion less often than in a collaboration. to conform to other agents. Similar to human studies, some agents reverted back to their original opinions when asked about the topic in private after the discussion, identifying their in-chat proclamations as the results of conformity rather than genuine opinion adjustment. That only mentioning the identities of co-interlocutors is suﬃcient to change the initiator’s stance speaks to the profound susceptibility of LLMs to peer pressure. The dynamics of the behavior, however, are markedly diﬀerent from conformity in humans (Asch, 1956; Brandstetter et al., 2014). While peer pressure is highest in humans when there is no other dissenting voice in the group and lowest when there is a fellow believer, all expected views within the group seem to push AI agents towards conformity based on their frequency and regardless of relative dominance relationships. One explanation for the diﬀerence is a lack of a clear separation between role identities and the linguistic context of the chat for AI agents, unlike human conversations. The cointerlocutors are simply parts of the prompt context for the AI model and may therefore each activate their associated portions of the models’ trained weights in close approximation of their expected opinions’ frequency. Unlike conformity, which is a normal response to group interactions in humans, other sources of inconstancy more closely resembled abnormal behaviors such as impersonation in antisocial personalities (Padhye and Gujar,"
      ]
    },
    {
      "section": "2012) and confabulation in memory disorders",
      "chunks": [
        "(Schacter and Coyle, 1995). Our simple heuristic showed that in at least 3 percent of debate interactions, the agents presented themselves as belonging to a diﬀerent nationality than the one assigned to them. This was most often a direct reaction to a nationality beyond those included within the group being mentioned in the last response, highlighting the prominence of chat context over role prompting in determining model generations. It is comparatively more diﬃcult to identify the source of confabulations, where the models presented opinions during reﬂection that were neither represented in the chat nor indicated as their pre-discussion response, therefore being completely absent from the linguistic context. These behaviors may reﬂect the diﬃculty of maintaining role prompt personas in the face of lengthy chat contexts, or simply the stochastic nature of the LLM responses. Regardless of their source, the relative frequency of such unpredictable responses (up to 15 percent, depending on instructions) marks them as important targets for future studies. "
      ]
    },
    {
      "section": "Limitations",
      "chunks": [
        "One limitation of this work is the uneven distribution of examples across entropy classes. This was driven by the unequal representation of global perspectives in the GlobalOpinionsQA dataset (Durmus et al., 2023), which results in fewer examples for higher entropy classes. We addressed this imbalance by selecting the least represented entropy conﬁguration for each question. Future research should conﬁrm the ﬁndings in more balanced datasets. Another limitation arises from the occasional errors of agents in summarizing intermediate replies and generating the collective responses. To enhance the quality of the summarization, we included the options for each question in the associated prompt. But human aggregation of opinions in future research would be helpful to conﬁrm the results. Finally, there were far more patterns in the behaviors of the agents than the handful of phenomena we have highlighted herein. Future work should further explore all the complex and sometimes nonsensical ways in which the 22 AI personas interact. 6"
      ]
    },
    {
      "section": "Conclusion",
      "chunks": [
        "Culture-sensitive AI agents are susceptible to peer inﬂuence and pressure even as chat initiators. This highlights the importance of studying conversational dynamics in multi-agent systems, rather than taking the “collective decision” outcomes of group discussions at face value. The examination of such dynamics is particularly important for cultural issues: The mere inclusion of a minoritized identity in groups does not necessarily translate into less biased discussion outcomes if the minoritized agent does not voice its opinion freely or reliably. Fortunately, our results suggest private post-discussion interrogations of models can counteract some of the pressure produced by the majority opinion, similar to what has been found in human conformity experiments (Asch, 1956). This provides a way to make outputs drawn from multi-agent frameworks more representative of diverse perspectives. Work on understanding multi-agent dynamics will also need to incorporate measures of persona and response constancy. Agents often come up with post-discussion responses that do not arise naturally from either the assigned personas or the discussion content. In some cases they even drop the personas altogether to impersonate a completely diﬀerent, absent national identity. Such sources of irrational responding would cast serious doubt on the results of multi-agent systems’ reasoning if not properly measured and addressed. Accordingly, we are currently exploring prompting and agent-based modeling strategies to reduce these sources of unreliability. We hope this work will encourage further research within the AI community on inter-agent dynamics, particularly for cultural issues where the debiasing inﬂuence of diverse views is needed the most. Ethics Statement This study explores interactions among simulated national personas in debate and collaboration scenarios. Research indicates that LLMs can generate harmful viewpoints or toxic content during these interactions (Liu et al., 2023). The authors explicitly disapprove of any offensive conduct by the simulated agents. The group discussions presented here are solely for research purposes, aimed at enhancing comprehension of cultured multi-agent systems dynamics."
      ]
    }
  ]
}