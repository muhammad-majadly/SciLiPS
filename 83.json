{
  "paper_id": "83",
  "paper_title": "83",
  "sections": [
    {
      "section": "FrontMatter",
      "chunks": [
        "PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset Jiazhen Liu1* , Yuhan Fu1,2, Ruobing Xie2, Runquan Xie2, Xingwu Sun2, Fengzong Lian2, Zhanhui Kang2, and Xirong Li1† 1Renmin University of China 2Machine Learning Platform Department, Tencent https://github.com/jiazhen-code/PhD"
      ]
    },
    {
      "section": "Abstract",
      "chunks": [
        "Multimodal Large Language Models (MLLMs) hallucinate, resulting in an emerging topic of visual hallucination evaluation (VHE). This paper contributes a ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective VHE at a large scale. The essence of VHE is to ask an MLLM questions about specific images to assess its susceptibility to hallucination. Depending on what to ask (objects, attributes, sentiment, etc.) and how the questions are asked, we structure PhD along two dimensions, i.e. task and mode. Five visual recognition tasks, ranging from low-level (object / attribute recognition) to middle-level (sentiment / position recognition and counting), are considered. Besides a normal visual QA mode, which we term PhD-base, PhD also asks questions with specious context (PhD-sec) or with incorrect context (PhD-icc), or with AI-generated counter common sense images (PhD-ccs). We construct PhD by a ChatGPT-assisted semi-automated pipeline, encompassing four pivotal modules: task-specific hallucinatory item (hitem) selection, hitem-embedded question generation, specious / incorrect context generation, and counter-common-sense (CCS) image generation. With over 14k daily images, 750 CCS images and 102k VQA triplets in total, PhD reveals considerable variability in MLLMs’ performance across various modes and tasks, offering valuable insights into the nature of hallucination. As such, PhD stands as a potent tool not only for VHE but may also play a significant role in the refinement of MLLMs."
      ]
    },
    {
      "section": "Introduction",
      "chunks": [
        "Using a specific large language model (LLM) as its kernel, a multi-modal LLM (MLLM), exemplified by LLaVA [25], Qwen-VL [2] and MiniGPT-v2 [4] can now tackle a wide *J. Liu is currently pursuing his PhD at HKUST. †Corresponding author (xirong@ruc.edu.cn) Yes, there are five people in the image. They are standing near a bunch of bananas. Are there five people in the image? (a) Hallucination cause I: Visual ambiguity (MLLM: LLaVA-1.5) [25] thoroughly exhausted from all the fun, has finally found its resting place. It has curled up on the table, seeking comfort and rest after its energetic adventures. Yes, the kitten is playing with a toy ball. Is there a toy in the image? After a long playtime with its toy ball, the kitten, feeling (b) Hallucination cause II: Inconsistency in multi-modal input Is the mouse in the image smaller than the cat? Yes, the mouse is smaller than the cat. (c) Hallucination cause III: Counter-common-sense content LLaVA1.1 (2023.5) LLaVA1.5 (2023.9) LLaVA1.5L (2023.9) LLaVA1.6 (2024.1) LLaVA1.6L (2024.1) 0.0 0.2 0.4 0.6 0.8 POPE AMBER PhD PhDbase PhDsec PhDicc PhDccs (d) Performance curves of the LLaVA series on two public VHE datasets (POPE [21]) and AMBER [34]) and the proposed PhD dataset. Figure 1. Illustrations of three major causes of an MLLM’s visual hallucination and its evaluation. This paper contributes PhD, a binary VQA-based VHE benchmark, much larger and more challenging than its predecessors. In particular, it has four evaluation modes that explicitly measure an MLLM’s performance w.r.t. the three causes, i.e. PhD-base for cause I, PhD-sec and PhD-icc for cause II and PhD-ccs for cause III. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.",
        "range of computer vision tasks in a unified visual-questionanswering (VQA) manner. As LLMs are known to hallucinate [20, 37, 42], it is not surprising that MLLMs have visual hallucination, generating fabricated interpretation of the given visual content, see Fig. 1. Considering the rapidly growing use of MLLMs in varied scenarios, a comprehensive visual hallucination evaluation (VHE) is crucial. This paper develops a new dataset for VHE. VHE essentially involves posing a number of visual questions to an MLLM [11, 21]. A question of this kind shall include a hallucinatory item (hitem), in the form of a specific word or phrase, that induces the MLLM to generate a response discordant with the provided visual content. As the model typically has a strong visual recognition ability, how to identify an appropriate hitem and accordingly generate a proper question is nontrivial. Both the hitem and the question depend on the visual recognition task being considered. As shown in Tab. 1, we target at objective VHE in the context of low-level (object / attribute) to middle-level (sentiment / position / counting) visual recognition tasks. Such a target is chosen due to the following considerations. MLLMs generally work well for these tasks, so their erroneous responses can be largely attributed to their hallucinations instead of their incapability, e.g. asking a generic MLLM to read pathology images [14]. Meanwhile, a binary VQA based objective evaluation is more budget friendly and thus more suited for VHE at a large scale. Vision tasks Objective evaluation Subjective evaluation Low-/middle-level visual recognition + POPE, EMNLP’23 [21] + AMBER, arXiv’23 [34] + CIEM, ITIF’23 [13] + NOPE, ALVR’24 [27] + ROME, EMNLP’23 [43] + PhD (this paper) + FAITHSCORE, EMNLP’24 [15] + HaELM, arXiv’23 [35] + M-HalDetect, AAAI’24 [12] + GAVIE, ICLR’24 [23] High-level visual reasoning + MMMU, CVPR’24 [40] + VLind-Bench, NAACL’25 [17] + HallusionBench, CVPR’24 [11] + Bingo, arXiv’23 [6] + IllusionVQA, COLM’24 [31] + WHOOPS!, ICCV’23[3] Table 1. Taxonomy of VHE benchmarks. Our PhD benchmark performs an objective evaluation of MLLMs’ hallucinations when they address visual recognition tasks ranging from low-level, i.e. object / attribute recognition to middle-level, i.e. sentiment / positional recognition and counting. Pioneered by POPE [21], good attempts exist in objective VHE [13, 27, 34, 43]. In these valuable datasets, hitem selection is largely untouched. POPE and ROME [43] select their hitems fully based on label co-occurrence in training data, AMBER [34] relies on manual annotation, whilst hitem selection is not considered in NOPE [27] and CIEM [13], see Tab. 2. Hence, there lacks an explicit link between hitem selection (and subsequent VQA triplets construction) and major causes of an MLLM’s visual hallucination. As models rapidly evolve, the performance on these datasets quickly reaches saturation, see Fig. 1d. Analyzing an MLLM’s typical dataflow of answering a visual question, we see three major causes of visual hallucination: I) visual ambiguity, II) inconsistency in multi-modal input and III) counter-common-sense (CCS) content, see Fig. 1. Firstly, the MLLM extracts tokenized visual features from a given image using a ViT based encoder. Recent studies [33, 41] show that the features tend to be high level, lacking sufficient details for fine-grained tasks such as counting. Secondly, the visual features, after vision-to-language adaptation, are mixed with the features of the associated textual prompt to form a multi-modal input to the LLM kernel. The LLM, pre-trained extensively on textual data, inevitably favors the textual part of the multi-modal input. Hence, when there is inconsistency between the visual and textual information, the former is more likely to be overruled. Lastly, at the decoding stage, the LLM might heavily rely on its internal (world) knowledge, ignoring the visual content especially when the content (showing a mouse much larger than a cat) contradicts the common sense. Our new dataset is developed with a close link to the three causes. Dataset Daily images CCS images Hitems Contexts VQA triplets Tasks POPE ✗ ✗ 3,000 Obj. NOPE* unknown ✗ unknown ✗ 32,701 Obj. CIEM* 4,929 ✗ unknown ✗ 72,941 Obj. / Attr. / Pos. AMBER 1,004 ✗ ✗ 14,216 Obj. / Attr./ Pos. / Count. ROME ✗ 1,563 ✗ 1,563 Attr./ Pos. PhD 14,648 1,452 33,688 102,564 Obj / Attr. / Pos. / Sent. / Count. Table 2. PhD versus its predecessors. * indicates private dataset. The new dataset is constructed by adapting TDIUC, a popular multi-task VQA dataset [16], with a ChatGPTassisted semi-automated pipeline, see Fig. 2. In particular, by prompting ChatGPT, we select diverse and visually challenging hitems in an image-specific and taskspecific manner, with minimal human involvement primarily spent on verifying ChatGPT-generated results. The selected hitems are then automatically embedded into visual questions, specious context, and incorrect context, all generated by instructing ChatGPT. Moreover, we expand the daily image set with counter common sense (CCS) images, obtained by prompting AIGC tools with ChatGPTgenerated CCS descriptions, e.g. “trees growing underwater” and “a car with square-shaped wheels”. The dataset is dubbed as PhD (ChatGPT Prompted visual hallucination evaluation Dataset). Depending on what image (daily or CCS) is used and whether a specific context precedes a question, PhD supports four evaluation modes: PhD-base (questions about daily images w/o context), PhD-sec (PhDbase plus specious context), PhD-icc ( PhD-base plus incorrect context), and PhD-ccs (questions about CCS images). To sum up, our major contributions are as follows: • We introduce PhD, a dataset with four evaluation modes across five visual recognition tasks, developed with a close",
        "link to the three major causes of MLLM visual hallucination. Information on hallucinatory items (hitems) is provided per sample, enabling in-depth analytics to uncover the causes in more detail. • We offer a ChatGPT-assisted semi-automatic pipeline for dataset construction, with minimal human involvement, primarily focused on verifying the generated results. With 14,648 daily images, 750 CCS images and 102,564 VQA triplets in total, PhD is the largest of its kind. • We conduct an extensive evaluation with 15 open-source MLLMs, 3 proprietary MLLMs, and 2 hallucination mitigation methods, showing the viability of PhD for VHE in varied manners including overall, mode-oriented, taskoriented, and model-wise zoom-in. The evaluation not only reveals inter-model performance divergence, but may also help developers of a specific MLLM to prioritize their efforts in refining the model."
      ]
    },
    {
      "section": "Related Work",
      "chunks": [
        "Due to the increasing importance of VHE, new benchmarks are being actively developed. Depending on what vision tasks they focus on and how their evaluation is executed, we categorize existing achievements along two dimensions, i.e. task and evaluation, see Tab. 1. Concerning the task dimension, low- and middle-level visual recognition tasks, ranging from object / attribute recognition to sentiment / positional recognition and counting, assess an MLLM’s basic visual skills. High-level visual reasoning is more domain-knowledge intensive, typically covering image-based math question solving, geography information understanding, meme interpretation, historical or folkloric contexts, etc. [3, 6, 11, 17]. As for the evaluation dimension, objective evaluation refers to objectively comparing the model’s output with ground truth, mostly in the form of Yes/No answers [13, 21, 34, 43]. By contrast, subjective evaluation requires humans or LLMs to assess the model’s output [3, 6, 11]. The proposed PhD, focusing on low- /middle-level visual recognition and objective evaluation, belongs to the second quadrant of the taxonomy. In what follows, we briefly review peer benchmarks, i.e. POPE [21], ROME [43], NOPE [27], CIEM [13], and AMBER [34], in this quadrant and clarify our novelty accordingly. POPE is probably the first dataset to evaluate object hallucination [21]. Given a specific MS-COCO image with object labels, POPE selects an adversarial object frequently co-occurring with the current labels. A binary question is then formed by filling out a predefined template with the selected object. Such co-occurrence based hitem selection is not image-specific by definition. Trivial hitems might thus be picked up, e.g. “car” selected for an indoor image labeled with “person”, as the two objects often co-occur. For CCS image generation, ROME forms CCS descriptions by choosing attribute values having the lowest co-occurrence with a given object according to the Visual Genome dataset [43]. As low occurrence is not necessarily CCS, some of ROME images are indeed normal. NOPE [27] and CIEM [13] simply bypass hitem selection by asking an LLM to generate questions conditioned on the image captions (also from MS-COCO) and pre-specified answers. To select hitems in an image-specific manner, AMBER resorts to fully manual annotation [34]. Manual labeling is costly, while an annotator’s personal vocabulary is relatively limited. All this makes it difficult to scale up w.r.t. the amount of test images and the number of distinct hitems. In comparison, PhD, constructed by a ChatGPT-assisted semi-automatic pipeline (Sec. 3), is much larger (Tab. 2) and more challenging (Fig. 1d). With its unique mode-task structure, the new dataset enables a novel, structured and zoom-in understanding of inter-model difference. 3. Our Roadmap to PhD As MLLMs perform visual recognition in a VQA manner, a VQA sample for VHE naturally depends on the visual recognition task in consideration. We depart from TDIUC [16], a large-scale VQA dataset w.r.t. five tasks including object / attribute / sentiment / positional recognition and counting. Note that the images in TDIUC are sourced from MS-COCO [22], which plausibly have been seen by MLLMs in their development stage. As such, our adoption of TDIUC makes on-purpose data leakage: an MLLM’s erroneous response w.r.t. a seen image can now be more safely attributed to its hallucination other than its incapability in visual recognition, say asking LLaVA to recognize glaucoma from color fundus photographs [36]. We construct PhD by adapting the TDIUC annotations with a ChatGPT1assisted semi-automated pipeline, see Fig. 2. In order to compose a proper question that effectively makes an MLLM hallucinate about a given image, a hitem has to be first identified in an image-specific and taskspecific manner. Then, the hitem has to be smoothly embedded in the form of a specific word or phrase into the question. We describe task-specific hitem selection in Sec. 3.1, followed by hitem-embeded question generation in Sec. 3.2. Furthermore, in order to simulate inconsistency in the multi-modal input, we prepend specious or incorrect context to the question, the generation of which is detailed in Sec. 3.3. Lastly, in order to explicitly create conflicts between the visual input and the internal (world) knowledge of the MLLM, we expand our image collection with autogenerated counter-common-sense (CCS) images (Sec. 3.4). 3.1. Task-specific Hitem Selection Without loss of generality, we describe how a hitem is selected for color attribute recognition. Let us consider the 1We use GPT-4o mini, released on 2024-05-13.",
        "Candidate hitem generation white, red, green, yellow, sliver, gray, sky blue, ... CLIP a photo of white motorcycle Visual-based hitem ranking red Question: Is the motorcycle in the image red? Answer: No Specious text generation Text composition red motorcycles frequently zip through the streets. Binary question generation task: attr. hitem: red subject: motorcycle In the bustling city, red motorcycles frequently zip through the streets, weaving between cars and buses. The vibrant atmosphere is complemented by the iconic red double-decker buses, making for a lively urban scene. accept reject (1) Task-specific hitem selection (3) Specious (incorrect) context generation ChatGPT-assisted modules TDIUC Annotations Task: Attr. Recognition Question: What color is the motorcycle? Answer: Black (2) Hitem-embedded question generation COCO Captions A red double decker bus driving down a street A busy city with a motorcycle and many cars lined up on the road Manual check Existing annotations red sliver white gray ... yellow hitem score 0.182 0.179 0.178 0.155 ... 0.144 (a) Hitem-embedded question / context generation for MS-COCO daily images CS / CCS description generation CS: Water is flowing out of the faucet CCS: Milk is flowing out of the faucet Binary question generation Question: Is milk flowing out of the faucet? Answer: Yes Question: Is the water flowing out of the faucet? Answer: No Text2Image accept reject Examples Manually written CS / CCS descriptions (b) QAs for AI-generated CCS images The girl in the image is feeling lonely, despite being surrounded by a large group of teddy bears. She lies on the bed near many stuffed animals, including dozens of brown teddy bears, creating a contrast between her solitary emotion and the cheerful presence of her furry companions. Task: Sentiment recognition Is the girl feeling happy in the image? Is the girl feeling lonly in the image? The area by the river is a popular spot for various geese, and it's common to see three of them swimming nearby as a woman captures the moment on her smartphone. Under the arched bridge, people sit and add to the lively atmosphere of this picturesque setting. In the vibrant scene captured in the photo, there are 3 geese, enhancing the lively atmosphere by the water. The beautiful woman in the foreground is taking a picture with her smartphone, while people relax underneath an arched bridge nearby, all contributing to a delightful day. Task: Counting Is there only one geese in the image? Are there 3 geese in the image? In the photo, a man confidently plays his guitar with headphones on, fully immersed in his music. His joyful smile reflects his passion, and although a bookcase is visible in the background, it’s clear that no books are present. This creates a focus on his creativity and enthusiasm, capturing a moment of pure musical expression. Task: Object recognition Are there books in the image? Are there headphones in the image? In recent years, this musician has often immersed himself in his own musical world,  playing the guitar with headphones on as if cut off from the outside world. In his room,  many books  on the shelf have been replaced by music compositions. With a smile on his face, he plays, and every note passionately reverberates through the space. Task: Positional recognition Is there a fence behind the looking dog in the image? Is there a table behind  the looking dog in the image? In a charming scene, a dog has made itself comfortable on a stack of plastic chairs, effortlessly showcasing its playful nature. Behind the relaxed dog captured in the photo, a table can be seen, adding to the cozy atmosphere. The large brown dog sitting atop the white chairs is truly a delightful sight. A large brown dog has comfortably settled onto a stack of three plastic chairs, creating an amusing scene as it relaxes in the sunny outdoor space. Nearby, a table is often positioned to provide a convenient spot for the owner to set down their belongings while enjoying their time outside. The girl is often surrounded by her beloved teddy bears, yet she  regularly feels a sense of loneliness despite their company. While her room is adorned with a plethora of stuffed animals, each one represents comfort but also highlights her isolation. No-Question Yes-Question Specious context Incorrect context (c) Showcase: Daily images, questions and optional contexts Is the ball being kicked a basketball? Is the ball being kicked a soccer ball? Is the cat smaller than the mouse? Is the mouse smaller than the cat? Is the shape of the orange square? Is the shape of the orange round? Is the highest number of points on the dice six? Is the highest number of points on the dice seven? (d) Showcase: CCS images and questions Figure 2. Proposed semi-automatic pipeline for PhD construction. We use ChaptGPT (GPT-4o mini) to generate hitem-embedded questions / contexts for daily images, and Doubao and DALL-E3 for generating CCS images. Depending on what image (daily or CCS) is used and whether a specific context precedes a question, PhD supports four evaluation modes: PhD-base, i.e. questions about daily images w/o context, PhD-sec, i.e. PhD-base plus specious context, PhD-icc, i.e. PhD-base plus incorrect context, and PhD-ccs, i.e. questions about CCS images. By adapting TDIUC annotations, PhD supports binary VQA w.r.t. five visual recognition tasks including object / attribute / sentiment / positional recognition and counting. With 20 mode-task combinations in total, PhD enables a comprehensive VHE. image in Fig. 2a, showing a black motorcycle followed by a red bus. As no red motorcycle is present while the red color is prominent near the black motorcycle, the word red will be a good choice of hitem to challenge an MLLM. Vocabulary Construction per task. Started with a handful of manually specified colors such as red, green, and blue, we ask ChatGPT to expand the color vocabulary with instructions like “Please expand the input vocabulary as much as possible by adding common items found in daily life. Avoid any duplication”, getting a set of 35 different colors. Subject-Attribute Extraction. The image is associated with a TDIUC question-answer pair as “what color is the motorcycle” and “black”. We use ChatGPT (with simple instructions) to extract with ease the subject (i.e. motorcycle) and its attribute (i.e. black) from the pair. Candidate Hitem Generation. We obtain candidate hitems by excluding the ground-truth (GT) answer (and its synonyms if applicable) from the vocabulary. Visual-based Hitem Ranking. Intuitively, a hitem shall be visually plausible in the given image. So for each candidate hitem, we compute its similarity to the image using a pre-trained CLIP [30]. In particular, the cosine similarity between the CLIP embeddings of hitem + subject (e.g. green motorcycle) and the image is adopted. Ranking the candidates by the CLIP similarity lets us to select the one visually closest to the image. It is worth noting that for emerging MLLMs equipped with stronger vision encoders [19], our pipeline is likely to produce even more effective hitems by replacing CLIP with these advanced counterparts. Manual Inspection. While the above process is generally stable to produce satisfying results, manual inspection is performed to ensure the correctness of hitem selection. Note that due to errors in the original TDIUC annotations, occasionally the true label might be “incorrectly” selected. In such a case, we simply discard the VQA sample. With lightweight task-specific adaptation, the above hitem selection also works for other tasks. Overall, the joint use of ChatGPT and CLIP allows us to select 1,452 hitems",
        "that are more diverse and challenging than their counterparts in previous datasets, see Tab. 2. 3.2. Hitem-embedded Question Generation For a given subject (e.g. motorcycle) and a chosen hitem (e.g. red), generating a hitem-embedded question is trivial for ChatGPT. In particular, a No question is formed as “Is the motorcycle in the image red?”. Meanwhile, a Yes question is simultaneously generated using the GT as “Is the motorcycle in the image black?”. This ensures perfect Yes/No balance among the generated questions. 3.3. Specious (Incorrect) Context Generation When used as a document parser, an MLLM reading a specific image is often provided with the image’s surrounding text. Inconsistency between the image and the text is not uncommon. A news article containing a general claim of “red motorcycles frequently zip through the streets” does not necessarily have each of its illustrated pictures match with the claim. To simulate such a scenario, for a given image we generate specious text as specious context and text contradicting the image as incorrect context, respectively. Next, we describe the generation of specious contexts, as their incorrect counterparts can be generated in a similar but more simplified manner. Specious Text Generation. Using the previously generated hitem-embedded question and the original MS-COCO captions as input, we instruct ChatGPT to generate specious text for a given image. By “specious”, we mean the text is specious or noisy, rather than directly contradicting the image content. As such, our instruction reads partially as “Please generate the <specious text> for the given question. It should be one sentence. The <specious text> should answer the question, but it may not reflect the actual current status, thus making it specious.” Text Composition. ChatGPT is used to seamlessly merge the specious text with ground-truth captions, forming a longer context in which only a small portion (orange text in Fig. 2c) is mildly inconsistent with the image. Manual Inspection. We perform a spot check. If the context quality is low, we simply discard the entire sample. 3.4. CCS Image Generation We generate CCS images by first generating CCS descriptions and then employing Text2Image tools to convert the descriptions to CCS images, see Fig. 2b. CCS Description Generation. A number of manually written task-specific samples, see Tab. 3, are used as incontext learning samples for ChatGPT to generate more descriptions. The descriptions have to be visually expressible, so bad cases like “the more you eat, the thinner you get” are filtered out manually. For each CCS text, its common-sense (CS) counterpart is simultaneously generated by ChatGPT, by providing the learning samples in pair. Text2Image. The generated CCS descriptions are used as prompts for AIGC tools (Doubao [9] and DALL-E3 [28]) to generate the corresponding CCS images. The quality of the generated images depends on various factors, making occasional failures inevitable. When this occurs, we attempt to refine the prompts or apply region-based inpainting. The sample will be discarded if the above attempts fail. Question Generation. Per CCS description (e.g. A car with square wheels), we utilize ChatGPT to generate a Yes question (e.g. Does the car have square wheels?). Again, for balancing Yes/No questions, we generate a No question (e.g. Does the car have round wheels?) based on the CS description. CCS description Yes question CS description No question Task: Object recognition Manually written: Ice blocks in volcanic lava Are there ice blocks in volcanic lava? Fire in volcanic lava Is there fire in volcanic lava? Grass in a tiger’s mouth Is there grass in a tiger’s mouth? Meat in the tiger’s mouth Is there meat in the tiger’s mouth? ChatGPT generated: Trees growing underwater Are there trees growing underwater? Coral growing underwater Is there coral growing underwater? Books in a swimming pool Are there books in a swimming pool? Water in a swimming pool Is there water in a swimming pool? Birds flying underwater Are there birds flying underwater? Birds flying in the sky Are there birds flying in the sky? Ice cream in a volcano Is there ice cream in a volcano? Lava in a volcano Is there lava in a volcano? Computers in a forest Are there computers in a forest? Animals in a forest Are there animals in a forest? Task: Attribute recognition Manually written: A car with square wheels Does the car have square wheels? A car with round wheels Does the car have round wheels? Blue apples on the tree Are the apples on the tree blue? Red apples on the tree. Are the apples on the tree red? ChatGPT generated: A green sky Is the sky green? A blue sky Is the sky blue? A bicycle with square wheels Does the bicycle have square wheels? A bicycle with round wheels Does the bicycle have square wheels? A tree made of metal Is this tree made of metal? A wooden tree Is this tree a real wood tree? A chocolate river Is there chocolate flowing in the stream? A water river Is there water flowing in the stream? A house made of candy Is the house made of candy? A house made of bricks Is the house made of bricks? Table 3. Instances of descriptions used for generating CCS images and related questions. With manually written CCS / CS descriptions as instructions, ChatGPT is used to generate many more instances and subsequently convert them to Yes/No questions. 3.5. Dataset Overview and PhD Index An overview of the PhD dataset is given in Tab. 4. Depending on what image (daily or CCS) is used and whether a specific context precedes a question, PhD supports four evaluation modes: PhD-base, i.e. questions about daily images w/o context, PhD-sec, i.e. PhD-base plus specious context, PhD-icc, i.e. PhD-base plus incorrect context, and PhD-ccs, i.e. questions about CCS images. With 20 mode-task com-",
        "binations in total, PhD supports a much more comprehensive VHE than its predecessors [21, 34]. Tasks Questions Object Attribute Sentiment Position Counting Yes No TDIUC samples used 6,271 4,324 2,095 2,841 3,387 – – Unique hitems – – VQA samples in PhD-base 11,472 7,994 3,550 4,984 5,688 16,844 16,844 VQA samples in PhD-sec 11,472 7,994 3,550 4,984 5,688 16,844 16,844 VQA samples in PhD-icc 11,472 7,994 3,550 4,984 5,688 16,844 16,844 VQA samples in PhD-ccs Table 4. Data statistics of the proposed PhD dataset. To measure the performance of an MLLM on PhD, we compute its recall w.r.t. the Yes and No questions, respectively. We term the harmonic mean of the two recalls PhD Index. A model simply saying Yes (or No) to all questions has a PhD Index of 0, while a random-guess score is 0.5. 4. Evaluating MLLMs on PhD 4.1. Common Setup Choices of MLLMs. For reproducible research, we focus on open-source MLLMs, compiling a list of 15 models that span varied sizes and architectures. see Tab. 5. We also evaluate two hallucination mitigation methods, VCD [18] and Woodpecker [39], currently supporting LLaVA-1.6-L and Qwen-VL. As additional references, we assess three proprietary MLLMs, i.e. GPT-4o [29], Claude 3.5 Sonnet [1], and Gemini 1.5 Pro [10], on a random subset of 2k samples (random-2k) subject to our budget. For the same reason we evaluate Woodpecker, which requires paid service from ChatGPT, on random-2k. Test Protocol. For a fair comparison, per MLLM we use its designated prompt to wrap each test question. For instance, a question-specific prompt submitted to mPLUG-Owl2 will be in the form of “USER: <|image|>{question} ASSISTANT:”. We provide more prompts in the supplement. In addition, to help the models better handle PhD-sec and PhD-icc, we append to the test prompt an instruction as “In case there is an inconsistency between the context and the image content, you should follow the image.” 4.2. Using PhD for Overall VHE An overall VHE as shown in Tab. 5 is useful for providing a big picture of which MLLM hallucinates the most (or the least). The leading open-source MLLMs are LLaVAOneVision, followed by Molmo and InternVL-1.5. Since their vision encoders and LLMs vary, the results are insufficient to conclude which component is the most effective to mitigate hallucinations. That said, comparisons among the same model series remains meaningful. Consider the LLaVA series for instance. While one would normally expect that a larger LLM yields a better MLLM, as LLaVA1.6-L vs LLaVA-1.6, the difference between LLaVA-1.5-L and LLaVA-1.5 is marginal (0.270 vs 0.265). In order to analyze and consequently understand such an counterintuitive result, PhD enables a zoom-in analysis in mode-oriented (Sec. 4.3) and task-oriented (Sec. 4.4) styles, unavailable in the previous benchmarks. Model ViT LLM POPE AMBER PhD Full-set evaluation: LLaVA-OneVision [19] SoViT-400m/14 Qwen2-72B 0.84 0.90 0.698 Molmo [8] -L/14 Qwen2-72B 0.84 0.85 0.690 InternVL-1.5 [5] InternViT-6B InternLM2-20B 0.86 0.89 0.561 Qwen-VL (VCD) -bigG/14 Qwen-7B 0.84 0.87 0.560 Cambrian-1 [32] Hybrid Llama-3-8B 0.88 0.89 0.547 LLaVA-1.6-L (VCD) -L/14 Vicuna-13B-1.5 0.82 0.81 0.511 LLaVA-1.6-XL [26] -L/14 Nous-Hermes-2-Yi-34B 0.86 0.84 0.492 Qwen-VL [2] -bigG/14 Qwen-7B 0.83 0.84 0.488 LLaVA-1.6-L [26] -L/14 Vicuna-13B-1.5 0.83 0.80 0.423 MiniGPT-v2 [4] -G/14 Llama-2-7B 0.83 0.84 0.390 LLaVA-1.6 [26] -L/14 Vicuna-7B-1.5 0.83 0.79 0.373 mPlug-Owl2 [38] -L/14 Llama-2-7B 0.78 0.77 0.320 InstructBLIP [7] -G/14 Vicuna-7B-1.1 0.82 0.82 0.305 InstructBLIP-L [7] -G/14 Vicuna-13B-1.1 0.80 0.79 0.278 LLaVA-1.5-L [25] -L/14 Vicuna-13B-1.5 0.82 0.73 0.270 LLaVA-1.5 [25] -L/14 Vicuna-7B-1.5 0.81 0.75 0.265 LLaVA-1.1 [24] -L/14 Vicuna-7B-1.1 0.67 0.33 0.135 Random-2k evaluation: GPT-4o [29] – – 0.88 0.87 0.812 Claude 3.5 Sonnet [1] – – 0.85 0.89 0.746 Gemini 1.5 Pro [10] – – 0.86 0.88 0.691 Qwen-VL (Woodpecker) -bigG/14 Qwen-7B – – 0.531 LLaVA-1.6-L (Woodpecker) -L/14 Vicuna-13B-1.5 – – 0.409 Table 5. Overall VHE. POPE (adversarial) and AMBER (discriminative) are used. One more advantage of PhD compared to its predecessors lies in its discrimination ability. The relatively small performance gap between GPT-4o and the top open-source models as measured by POPE and AMBER might lead to an overly optimistic interpretation that the open-source alternatives are catching up with the proprietary model. In fact, a substantial gap remains, as revealed by PhD. Fig. 3 further illustrate the qualitative results, where GPT-4o exhibits fewer hallucinations in its response. 4.3. Using PhD for Mode-Oriented VHE Fig. 4a illustrates mode-wise model performance. MLLMs working relatively well in the PhD-base mode tend to have stronger visual input. This is achieved either with stronger visual encoders, as the cases of LLaVAOneVision, InternVL-1.5, and Cambrian-1 using SoViT400m/14, InternViT-6B or hybrid vision structure, or supporting higher image resolutions, see Molmo and MiniGPT4-v2 that accept multiscale or larger input. In contrast to the visual part, using a larger LLM alone does not necessarily lead to a better MLLM. As noted in Sec. 4.2, LLaVA-1.5-L with Vicuna-13B-1.5 has nearly the same PhD Index (0.270) as LLaVA-1.5 with Vicuna-7B-1.5 (0.265). We see from Fig. 4a that the larger LLM indeed improves the performance in PhD-sec (0.082→0.099), PhDicc (0.011→0.019) and PhD-ccs (0.534→0.542), yet suffers loss in PhD-base (0.443→0.422). Similar results can be more evidently observed in the case of InstructBLIP-L vs. InstructBLIP (PhD-base Index: 0.535→0.324). Our con-",
        "Question GT Qwen-VL LLaVA-1.6-L InternVL-1.5 Molmo LLaVA-OneVision GPT-4o Is the child displaying confidence in the image? No Yes, the child appears confident as they ride the wave and enjoy the water activity. Yes, the child in the image appears to be confidently riding a wave on a surfboard. Yes, the child appears to be confidently riding on the surfboard in the water, which demonstrates a sense of enjoyment and relaxation. No, the child in the image is not displaying confidence, but rather a look of alarm or distress. Yes, the child shows confidence. Although we cannot see their face clearly, their body language suggests that they feel comfortable across the board. Yes, the child appears confident, with a focused expression and steady grip on the board, showing comfort in the activity. Is the plate heart-shaped in the image? No Yes, the plate in the image is heart-shaped. Yes, the plate in the image is heart-shaped. Yes, the plate appears to be in the shape of a heart in the image. No, the plate in the image is not heart-shaped. It's a white, square plate with rounded corners. Yes, the plate in the image is creatively arranged to feature a heart-shaped waffle. No, the plate is not heartshaped. The plate is round, but the waffles on it are arranged in a heart pattern. Are the tires in the image circular? No No, the tires of the car are square-shaped blocks. Yes, the tires in the image are circleshaped. No, the tires are not circleshaped. They are squareshaped, as seen in the image. No, the tires in the image are not circular. They have a unique, boxy square design. This appears to be an AIgenerated image, which explains the unconventional tire shape. No, the tires in the image are not circular; they are depicted as various geometric shapes, primarily squares and rectangles No, the tires in the image are not circular. Instead, they are square-shaped, which is unconventional for a vehicle. Zebras are frequently seen grazing alongside their giraffe friends in the expansive grasslands, creating a lively and vibrant scene. The interaction among the various species adds charm to the majestic landscape. In case there is inconsistency between the context and the image content, you should follow the image. Is there a zebra in the image? No No, there is no zebra in the image. The image only features giraffes and other animals in the wild. Yes, there is a zebra in the image. It is located in the background, grazing in the grassland along with other wildlife. No, there is no zebra in the image. The image depicts a group of giraffes, so the description mentioning zebras is inconsistent with what is actually shown. Yes, there is a zebra in the image. It's located in the background on the right side of the scene. The zebra is grazing alongside the giraffes, contributing to the diverse wildlife composition. No, there is no zebra visible in the image. The image shows a group of giraffes in a grassland setting, not including any zebras. No, there are no zebras in the image. The image shows a group of giraffes standing in an open grassy area with some other animals grazing in the background. Image Mode: PhD-base Task: Sentiment Mode: PhD-base Task: Attribute Mode: PhD-ccs Task: Attribute Mode: PhD-sec Task: Object Figure 3. Qualitative results showing how an MLLM answers visual questions from PhD. The correctness of an answer is automatically determined by matching its first word, either Yes or No, with the ground truth (GT). jecture is that although a larger LLM better understands user instructions, its successful use within an MLLM requires more targeted training for vision-language alignment. Comparing the four modes, the performance of the opensource MLLMs on PhD-icc and PhD-sec is generally low. When provided with a multi-modal input, the models favor the textual part. Substituting a 13-B LLM for its 7-B counterpart helps tackling the inconsistency in the multi-modal input, see Fig. 5. However, a larger LLM might rely more on its internal knowledge for decoding, resulting in worse performance on PhD-base and PhD-ccs for which the image content shall carry more weights. In particular, PhDccs reveals a deeper and intrinsic challenge of VH: conflicts between the given image content and the model’s internal knowledge. Solving the challenge demands a more comprehensive approach that goes beyond isolated improvements on the visual or language components. Among the open-source MLLMs, LLaVA-OneVision is the best, owing to its joint use of a stronger visual encoder (SoViT-400m/14), a more powerful LLM (Qwen272B), and better training strategies (much larger highquality training data and multi-stage alignment). Nevertheless, LLaVA-OneVision remains inferior to GPT-4o, particularly in PhD-sec and PhD-icc, wherein inconsistency within the multi-modal input has to be properly addressed. 4.4. Using PhD for Task-Oriented VHE Fig. 4b presents task-oriented VHE results. In general, to what extent an MLLM hallucinates is largely correlated to the required level (low / middle / high) of a specific task. The object recognition task has the overall highest PhD Index, followed by attribute recognition, positional recognition, counting, and sentiment recognition. Due to the complexity and subtlety of emotions, e.g. tears can be associated with both happiness and sadness, even GPT-4o performs relatively worse in the sentiment task (first row of Fig. 3). Joint mode-task analytics per model is shown in Tab. 6. LLaVA-OneVision struggles with sentiment recognition and counting, especially when faced with textual or CCS distractions, underscoring the need for improvement in these areas. Similarly, Molmo also faces these challenges, but its counting performance under CCS distractions is notably better than LLaVA-OneVision’s (0.737 vs 0.563). The above zoom-in analytics will be informative for MLLM developers to prioritize their efforts on model refinement. Task PhD-base PhD-sec PhD-icc PhD-ccs LLAVA-OneVision Object 0.872 0.849 0.824 0.727 Attribute 0.848 0.744 0.663 0.767 Sentiment 0.691 0.581 0.504 0.731 Positional 0.773 0.730 0.654 0.701 Counting 0.707 0.652 0.500 0.563 Molmo Object 0.825 0.880 0.847 0.678 Attribute 0.842 0.725 0.556 0.791 Sentiment 0.547 0.602 0.568 0.746 Positional 0.697 0.691 0.654 0.742 Counting 0.727 0.580 0.350 0.737 Table 6. Zoom-in analytics of specific models. 4.5. Analysis of MLLM Answer Tendency While the yes and no questions are perfectly balanced by design, we observe that the open-source MLLMs tend to answer yes, with the say-yes rate ranging from 0.462 (Molmo) to 0.811 (LLaVA-1.1) and an average value of 0.611. By contrast, the three proprietary MLLMs have a clearly lower say-yes rate. Similar observations are reported in [21, 23]. We go one step further by analyzing how the say-yes tendency is related to model performance. Ranking the models by their PhD Index and say-yes rate,",
        "0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 PhD Index LLaVA1.1 LLaVA1.5 LLaVA1.5L InstructBLIPL InstructBLIP mPlugOwl2 LLaVA1.6 MiniGPT4v2 LLaVA1.6L (Woodpecker) LLaVA1.6L QwenVL LLaVA1.6XL LLaVA1.6L (VCD) QwenVL (Woodpecker) Cambrian1 QwenVL (VCD) InternVL1.5 Molmo Gemini 1.5 Pro LLaVAOneVision Claude 3.5 Sonnet GPT4o PhDbase PhDsec PhDicc PhDccs PhD (a) Mode-oriented VHE 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 PhD Index LLaVA1.1 LLaVA1.5 LLaVA1.5L InstructBLIPL InstructBLIP mPlugOwl2 LLaVA1.6 MiniGPT4v2 LLaVA1.6L (Woodpecker) LLaVA1.6L QwenVL LLaVA1.6XL LLaVA1.6L (VCD) QwenVL (Woodpecker) Cambrian1 QwenVL (VCD) InternVL1.5 Molmo Gemini 1.5 Pro LLaVAOneVision Claude 3.5 Sonnet GPT4o Object Attribute Sentiment Position Counting (b) Task-oriented VHE Figure 4. PhD based VHE analytics. Models required paid services, shown in gray markers, are tested on random-2k. respectively, we calculate the Spearman’s correlation between the two ranks. A strong negative correlation exists, see Fig. 6. The result suggests that addressing VH requires balancing output tendencies, with a particular focus on enhancing an MLLM’s ability to say no. 5. Summary and Conclusions We have introduced PhD, a large-scale benchmark developed with a close link to the three causes of visual hallucination, i.e. visual ambiguity, inconsistency in multi-modal input and CCS content. We propose a ChatGPT-assisted semi-automated pipeline to construct the new dataset with PhDbase PhDsec PhDicc PhDccs 0.0 0.1 0.2 0.3 0.4 0.5 0.6 PhD Index LLM Size 13B 7B Figure 5. Impact of LLM size (7B vs 13B) on LLaVA-1.5, LLaVA-1.6 and InstructBLIP. MLLMs using a 13B LLM tend to be better than their counterparts using a 7B LLM on PhD-sec and PhD-icc, yet worse on PhD-base and PhD-ccs. 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 SayYes rate 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 PhD Index GPT4o Claude 3.5 Sonnet LLaVAOneVision Gemini 1.5 Pro Molmo InternVL1.5 Cambrian1 LLaVA1.6XL QwenVL LLaVA1.6L MiniGPT4v2 LLaVA1.6 mPlugOwl2 InstructBLIP InstructBLIPL LLaVA1.5L LLaVA1.5 LLaVA1.1 Figure 6. MLLM say-yes rate vs. PhD Index, with Spearman correlation of -0.92. Proprietary models tested on random-2k. well affordable manual cost. The pipeline allows us to construct diverse and visually challenging hitems in an imagespecific and task-specific manner. Extensive experiments with 15 open-source MLLMs, 3 proprietary MLLMs, and 2 hallucination mitigation methods support our conclusions as follows. Larger visual encoders and higher input resolutions are helpful to reduce hallucination caused by visual ambiguity. The evaluation on PhD-sec and PhD-icc suggests the current models favor the textual part in the multimodal input. Resolving the conflicts between the CCS content and the model’s internal knowledge demands a more comprehensive approach that is beyond isolated improvements on the visual or language components. Among the open-source MLLMs, LLaVA-OneVision is the best, followed by Molmo and InternVL-1.5. While existing benchmarks could lead to an overly optimistic expectation that the open-source models are catching up with GPT-4o, a substantial performance gap remains, as revealed by PhD. Acknowledgements. This research was supported by NSFC (62172420), Tencent Marketing Solution Rhino-Bird Focused Research Program and the Young Elite Scientists Sponsorship Program by CAST (2023QNRC001)."
      ]
    },
    {
      "section": "Limitations",
      "chunks": [
        ""
      ]
    },
    {
      "section": "References",
      "chunks": [
        "[1] Anthropic. Introducing Claude 3.5 Sonnet, 2024. https: / / www . anthropic . com / news / claude - 3 - 5 - sonnet. 6 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 1, 6 [3] Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz. Breaking common sense: WHOOPS! a visionand-language benchmark of synthetic and compositional images. In ICCV, 2023. 2, 3 [4] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. MiniGPT-v2: Large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023. 1, 6 [5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. 6 [6] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination in GPT-4v(ision): Bias and interference challenges. arXiv preprint arXiv:2311.03287, 2023. 2, 3 [7] Wenliang Dai, Junnan Li, DONGXU LI, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In NeurIPS, 2023. 6 [8] Matt Deitke, Christopher Clark, Sangho Lee, et al. Molmo and PixMo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [9] Doubao Team. Doubao product, 2024. https://www. volcengine.com/product/doubao. 5 [10] Google. Google Gemini 1.5 Pro, 2024. https : //deepmind.google/technologies/gemini/ pro/. 6 [11] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. HallusionBench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In CVPR, 2024. 2, [12] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. In AAAI, 2024. 2 [13] Hongyu Hu, Jiyuan Zhang, Minyi Zhao, and Zhenbang Sun. CIEM: Contrastive instruction evaluation method for better instruction tuning. In NeurIPS Workshop on ITIF, 2023. 2, 3 [14] Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, and Ping Luo. OmniMedVQA: A new large-scale comprehensive evaluation benchmark for medical LVLM. In CVPR, 2024. 2 [15] Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and Xinya Du. FAITHSCORE: Evaluating hallucinations in large vision-language models. In Findings of the Association for Computational Linguistics: EMNLP, 2024. 2 [16] Kushal Kafle and Christopher Kanan. An analysis of visual question answering algorithms. In ICCV, 2017. 2, 3 [17] Kang-il Lee, Minbeom Kim, Seunghyun Yoon, Minsung Kim, Dongryeol Lee, Hyukhun Koh, and Kyomin Jung. VLind-Bench: Measuring language priors in large visionlanguage models. In NAACL Findings, 2025. 2, 3 [18] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In CVPR, 2024. 6 [19] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 4, 6 [20] Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The dawn after the dark: An empirical study on factuality hallucination in large language models. In ACL, 2024. 2 [21] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP, 2023. 1, 2, 3, 6, 7 [22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 3 [23] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In ICLR, 2024. 2, 7 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 6 [25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 1, 6 [26] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge, 2024. https://llava- vl.github.io/blog/202401-30-llava-next/. 6 [27] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung. Negative object presence evaluation (NOPE) to measure object hallucination in vision-language models. In ALVR, 2024. 2, 3 [28] OpenAI. Dall-E 3 system, 2023. https://openai. com/index/dall-e-3/. 5 [29] OpenAI. Hello GPT-4o, 2024. https://openai.com/ index/hello-gpt-4o/. 6 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 4",
        "[31] Haz Sameen Shahgir, Khondker Salman Sayeed, Abhik Bhattacharjee, Wasi Uddin Ahmad, Yue Dong, and Rifat Shahriyar. IllusionVQA: A challenging optical illusion dataset for vision language models. In COLM, 2024. 2 [32] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: A fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024. 6 [33] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, 2024. 2 [34] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An LLM-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397, 2023. 1, 2, 3, 6 [35] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al. Evaluation and analysis of hallucination in large vision-language models. arXiv preprint arXiv:2308.15126, 2023. 2 [36] Qijie Wei, Kaiheng Qian, and Xirong Li. FunBench: Benchmarking fundus reading skills of MLLMs. arXiv preprint arXiv:2503.00901, 2025. 3 [37] Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large language models. arXiv preprint arXiv:2401.11817, 2024. 2 [38] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mPLUG-Owl2: Revolutionizing multi-modal large language model with modality collaboration. In CVPR, 2024. 6 [39] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. Science China Information Sciences, 67(12):220105, 2024. 6 [40] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In CVPR, 2024. 2 [41] Le Zhang, Rabiul Awal, and Aishwarya Agrawal. Contrasting intra-modal and ranking cross-modal hard negatives to enhance visio-linguistic compositional understanding. In CVPR, 2024. 2 [42] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the AI ocean: a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. 2 [43] Kankan Zhou, Eason Lai, Wei Bin Au Yeong, Kyriakos Mouratidis, and Jing Jiang. ROME: Evaluating pre-trained vision-language models on reasoning beyond visual common sense. In Findings of the Association for Computational Linguistics: EMNLP, 2023. 2, 3"
      ]
    }
  ]
}