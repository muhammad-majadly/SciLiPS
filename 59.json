{
  "paper_id": "59",
  "paper_title": "59",
  "sections": [
    {
      "section": "FrontMatter",
      "chunks": [
        "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 977–996 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Unique Hard Attention: A Tale of Two Sides Selim Jerad Anej Svete Jiaoda Li Ryan Cotterell {sjerad, anej.svete, jiaoda.li, ryan.cotterell}@ethz.ch “A wonderful fact to reflect upon, that leftmost and rightmost unique hard attention are constituted to be profoundly distinct.”"
      ]
    },
    {
      "section": "Abstract",
      "chunks": [
        "Understanding the expressive power of transformers has recently attracted attention, as it offers insights into their abilities and limitations. Many studies analyze unique hard attention transformers, where attention selects a single position that maximizes the attention scores. When multiple positions achieve the maximum score, either the rightmost or the leftmost of those is chosen. In this paper, we highlight the importance of this seeming triviality. Recently, finite-precision transformers with both leftmost- and rightmost-hard attention were shown to be equivalent to Linear Temporal Logic (LTL). We show that this no longer holds with only leftmost-hard attention— in that case, they correspond to a strictly weaker fragment of LTL. Furthermore, we show that models with leftmost-hard attention are equivalent to soft attention, suggesting they may better approximate real-world transformers than right-attention models. These findings refine the landscape of transformer expressivity and underscore the role of attention directionality."
      ]
    },
    {
      "section": "Introduction",
      "chunks": [
        "Much work has recently been done on understanding the capabilities and limitations of transformers (Vaswani et al., 2017). Collectively, the body of work on the representational capacity of transformers has provided a nuanced picture of the landscape (Pérez et al., 2021; Hahn, 2020; Chiang and Cholak, 2022; Hao et al., 2022; Merrill et al., 2022a; Merrill and Sabharwal, 2023; Chiang et al., 2023; Yang et al., 2024a; Strobl et al., 2024; Svete and Cotterell, 2024; Nowak et al., 2024; Li and Cotterell, 2025, inter alia).1 Any such investigation begins by choosing an idealization of the architecture. A 1App. A holds a more detailed overview of related work. common modeling decision is to use unique hard attention (UHA), which selects a single position maximizing the attention scores (Hahn, 2020; Hao et al., 2022; Barceló et al., 2024). In one of the first exact descriptions of UHA expressivity, Yang et al. (2024a) show that UHA transformers (UHATs) with no positional encodings, strict future masking, and either leftmost or rightmost tiebreaking are equivalent to linear temporal logic, LTL. This connects UHATs to well-understood formalism such as the star-free languages and counter-free automata. An incautious reading of Yang et al.’s result could lead one to generalize the equivalence to all UHATs. However, we zoom in on the overlooked choice of tiebreaking and show that it markedly impacts the model’s expressivity. We show UHATs with only leftmost tiebreaking are strictly less expressive by relating them to a fragment of LTL, LTL[3 −]. We do so by adapting Yang et al.’s (2024a) proofs: We describe a variant of the B-RASP programming language defined therein, restricted to leftmost tiebreaking, and show it to be equivalent to LTL[3 −]. Further, leveraging the recent results by Li and Cotterell (2025) that characterize finite-precision future-masked softattention transformers as equivalent to LTL[3 −], we establish the equivalence of leftmost UHATs to standard softmax transformers. Moreover, we give explicit LTL[3 −] formulas and partially ordered finitestate automata (FSAs) describing leftmost UHATs. Transformer Idealization This section introduces the idealization of the transformer analyzed throughout the paper. Finite Precision. Implemented on modern hardware, computations performed by transformers rely on a fixed number of bits. This makes finiteprecision a more realistic assumption than unbounded or growing (w.r.t. input length) precision (Barceló et al., 2024; Hahn, 2020; Hao et al., 2022; Merrill et al., 2022b; Merrill and Sabharwal, 2023).",
        "No positional encodings. We wish to study what we consider to be a barebone idealization of the transformer, because this enables us to understand the exact expressive power of UHA. Moreover, finite-precision positional encodings correspond precisely to monadic predicates in LTL formulas (Yang et al., 2024a), yielding a predictable and well-understood extension to our analysis. Unique hard attention. While the original transformer uses soft attention (Vaswani et al., 2017), theoretical work largely analyzes hard attention (Merrill et al., 2022b; Hao et al., 2022; Yang et al., 2024a; Barceló et al., 2024, inter alia). The precise implications of this modeling decision are still unclear, but our work, combined with Li and Cotterell’s (2025) results, reduces the gap between both models: Soft-attention transformers are equivalent to leftmost UHATs while rightmost UHATs are more expressive. We contextualize our findings with some more related results in Tab. 1. Strict future masking. Future masking is standard in transformer-based language models. We focus on strict masking (where a position cannot attend to itself) as non-strict masking is known to reduce expressive power (Yang et al., 2024a). Moreover, residual connections still allow the model to incorporate information vertically across layers. The Best of UHA, The Worst of UHA This section provides a high-level overview, intuition, roadmap, and key implications of our results. 3.1 Separation We begin by building an intuition as to why, with future masking, UHA with leftmost tiebreaking ◀is strictly less expressive than with rightmost tiebreaking ▶. We illustrate this in B-RASP, a Booleanvalued programming language (Yang et al., 2024a), as the intermediary between LTL and UHATs. To follow the coming examples, we only need familiarity with the following attention operation: P(t) = ◀t′[t′ < t, s(t′)] v(t′) : d(t) (1) ◀t′[t′ < t, s(t′)] denotes choosing (attending to) the leftmost (◀) position t′ for which t′ < t holds (Future masking) and s(t′) = 1. If such a t′ exists, P(t) is assigned the value of the predicate v(t′), and otherwise, it is assigned a default value d(t). This emulates leftmost UHA, where t′ < t corresponds to strict future masking, s(t′) = 1 corresponds to maximizing the attention score, and v(t′) corresponds to the value vector. We define the rightmost operation ▶analogously. We now note two facts: (i) Every leftmost operation ◀ P(t) = ◀t′[t′ < t, s(t′)] v(t′) : d(t) (2) can be simulated by ▶attentions in two steps. First, we gather all positions t that have preceding positions t′ < t such that s(t′) = 1: P1(t) = ▶t′[t′ < t, s(t′)] s(t′) : 0. (3) Then, the leftmost position is just the single position t′ with s(t′) = 1 but P1(t′) = 0: P2(t) = ▶t′[t′ < t, s(t′) ∧¬P1(t′)] v(t′) : d(t). (4) (ii) There exist operations that ▶can perform and ◀cannot. For instance, ▶attention can read the value immediately to the left of the current position, i.e., v(t −1), as follows: P(t) = ▶t′[t′ < t, 1] v(t′) : 0, (5) but ◀attention cannot, as we would need t′ = t−1 to be the only position with t′ < t and s(t′) = 1 for all t ∈[T], which is impossible. This establishes a separation between B-RASPF ◀, which is limited to leftmost tiebreaking ◀and future masking F, and the full B-RASP, leading to the following: Theorem 3.1 (Informal). Finite-precision futuremasked ◀UHATs are weaker than ▶UHATs. 3.2 Characterizations In the remainder of the paper, we show that B-RASPF ◀is equivalent to the fragment of LTL with only the 3 −operator, denoted by LTL[3 −], which in turn is equivalent to partially ordered FSAs (POFA). This provides an exact characterization of leftmost UHATs: Theorem 3.2 (Informal). Finite-precision futuremasked ◀UHATs are equivalent to LTL[3 −]. In §6, we formalize the theorem with proofs intentionally made analogous to Yang et al.’s (2024a), in order to highlight the difference between ◀and ▶. Additionally, in §7, we provide alternative proofs that directly translate ◀UHATs to LTL[3 −] formulas and POFA.",
        "Transformer LTL FO logic Regex Monoid Automata Note T F ▶, T P ◀ LTL[3 −, 3 + , S, U] FO[<] star-free aperiodic counter-free Yang et al. (2024a, Thm. 5) future-masked soft attention LTL[3 −] PFO2[<] R-expression R-trivial POFA Li and Cotterell (2025) T F ◀ LTL[3 −] PFO2[<] R-expression R-trivial POFA Thms. 6.1 and 6.2 T P ▶ LTL[3 + ] FFO2[<] L-expression L-trivial RPOFA Thm. E.1 Table 1: Known equivalences of finite-precision transformers with no positional encodings to different formalism. T F ▶future-masked rightmost UHATs. T F ◀, T P ◀, and T P ▶are defined analogously for past masking and leftmost UHA. 3.3 Implications Combining with Li and Cotterell’s (2025) results that show that soft and average hard attention transformers2 are equivalent to LTL[3 −] as well, we discover the peculiar fact that with fixed precision, soft attention, average hard attention, and leftmost UHA are all equally expressive. This insight could shed light on certain observed phenomena in soft-attention transformers. For instance, Liu et al. (2023) find that transformers struggle with the flip-flop language, where the symbol following a “read” instruction must match the symbol following the most recent “write” instruction. Our results suggest that this difficulty arises because leftmost UHATs and thereby soft-attention transformers lack the ability to locate the most recent—rightmost—write instruction. Furthermore, the fact that rightmost UHA is strictly more expressive than other variants of transformers may partly explain the empirical success of positional encodings that bias attention toward recent tokens, such as ALiBi (Press et al., 2022), as they help approximate rightmost tiebreaking. Linear Temporal Logic An alphabet Σ is a finite, non-empty set of symbols. The Kleene closure of Σ is Σ∗= S∞ n=0 Σn, the set of all strings, where Σ0 def= {ε} contains only the empty string. A language L is a subset of Σ∗. We treat a language recognizer as a function R: Σ∗→{0, 1} whose language is L(R) def= {w ∈Σ∗| R(w) = 1}. Two recognizers R1 and R2 are equivalent if and only if L(R1) = L(R2). Linear temporal logic LTL[3 −, 3 + , S, U] is an extension of Boolean logic that considers events over time and can express time-dependent properties (Pnueli, 1977). We define LTL over (finite) strings. Formulas in LTL[3 −, 3 + , S, U] are composed of atomic formulas πa for a ∈Σ, Boolean 2Average hard attention divides attention mass equally among positions maximizing the attention score. connectives ∧, ¬,3 and four temporal operators −(past), 3 + (future), S (since) and U (until). We denote by LTL[3 −] the fragment with only the 3 − operator and Boolean connectives and by LTL[3 + ] the fragment with only the 3 + operator and Boolean connectives. Given a string w = w1 · · · wT , LTL formulas are interpreted at some position t ∈[T]. We write w, t |= ψ to denote that ψ is TRUE on w at position t. The semantics for LTL[3 −] are:4 • w, t |= πa ⇐⇒wt = a; • w, t |= ψ1 ∧ψ2 ⇐⇒w, t |= ψ1 ∧w, t |= ψ2; • w, t |= ¬ψ ⇐⇒w, t ̸|= ψ; • w, t |= 3 −ψ ⇐⇒∃t′ < t : w, t′ |= ψ. To define string acceptance, we denote by T + 1 a position outside of the string and define w |= ψ ⇔w, T + 1 |= ψ. (6) B-RASP and B-RASPF ◀ We now introduce B-RASP in more detail. The input to a B-RASP program is a string w ∈Σ∗with |w| = T. On such an input, a B-RASP program computes a sequence of Boolean vectors of size T, with entries indexed by t ∈[T] in parentheses, denoted as P(t). Each w ∈Σ gives rise to an atomic Boolean vector Qw, defined as follows. For each t ∈[T]: Qw(t) = 1 ⇐⇒wt = w (7) To streamline notation, we denote the first |Σ| vectors of the program Qw by P1, · · · , P|Σ|. The (i + 1)th vector Pi+1 def= P ′ is computed inductively using one of the following operations: (1) Position-wise operation: P ′(t) is a Boolean combination of zero or more of {Pi′(t)}i i′=1. 3∨can be defined using ∧and ¬. 4We refer to App. B for more details on LTL[3 −, 3 + , S, U] and its equivalent formalisms.",
        "(2) Attention operation: P ′(t) can be one of: P ′(t) = ◀t′[m \u0000t, t′\u0001 , s(t, t′)] v(t, t′) : d(t) (8a) P ′(t) = ▶t′[m \u0000t, t′\u0001 , s(t, t′)] v(t, t′) : d(t) (8b) where: • The mask predicate m is defined as either m(t, t′) def= 1 {t′ < t} for strict future masking (F), or m(t, t′) def= 1 {t′ > t} for strict past masking (P). Notice that the inequalities are strict, meaning the current position is excluded from attention. This detail has been shown to increase expressivity compared to non-strict masking (Yang et al., 2024a; Li and Cotterell, 2025).5 • The score predicate s(t, t′) is a Boolean combination of {Pi′(t)}i i′=1 ∪{Pi′(t′)}i i′=1. • the value predicate v(t, t′) is defined analogously, and • the default value predicate d(t) is a Boolean combination of values in {P1(t), . . . , Pi(t)}. We use ◀to denote leftmost tiebreaking and ▶to denote rightmost tiebreaking. For t ∈[T], define the set of valid positions as: N(t) def= {t′ ∈[T] | m \u0000t, t′\u0001 = 1 and s(t, t′) = 1}. (9) The unique position to attend to is then selected as: t∗def= ( min N(t) if ◀ max N(t) if ▶. (10) Finally, the semantics of the attention operation are given by: P ′(t) def= ( v(t, t∗) if |N(t)| > 0 d(t) otherwise . (11) Note that, by Lem. 12 and Prop. 11 of Yang et al. (2024a), we can rewrite every P to an equivalent program where every attention operation only uses unary scores and unary values, i.e., s(t, t′) and v(t, t′) depend only on t′. B-RASPF ◀is the restricted version of B-RASP with only leftmost tiebreaking and future masking. To define P’s language, we designate a final output vector Y and T as the output position such that Y (T) = 1 signals acceptance. B-RASP is equivalent to finite-precision futuremasked rightmost UHATs, T F ▶(Yang et al., 2024a, Thms. 3 and 4). A similar claim, proved in App. C, holds for B-RASPF ◀and leftmost UHATs, T F ◀. 5Such UHATs can still access information at the current position via the residual connection; similarly, B-RASP programs can do so using the default predicate d. Theorem 5.1. For any UHAT in T F ◀, there exists an equivalent B-RASPF ◀program. For any B-RASPF ◀ program, there exists an equivalent UHAT in T F ◀. B-RASPF ◀Is Equivalent to LTL[3 −] We now formalize the equivalence of B-RASPF ◀ and LTL[3 −]. It rests on the following two theorems. The proofs are provided in App. C. They are adapted from Yang et al. (2024a), with the differing parts highlighted in red. Theorem 6.1. For any formula ψ of LTL[3 −], there is a B-RASPF ◀program with a Boolean vector Pψ such that, for any input w of length T and all t ∈[T], we have w, t |= ψ ⇐⇒Pψ(t) = 1. Theorem 6.2. For any Boolean vector P of a B-RASPF ◀program P, there is a formula ψP of LTL[3 −] such that for any input w of length T and all t ∈[T], we have P(t) = 1 ⇐⇒w, t |= ψP . Yang et al. (2024a, Thm. 15) establish an alternative proof of the equivalence between B-RASP and full LTL[3 −, 3 + , S, U] via counter-free automata, which recognize the class of star-free languages. Analogously, we demonstrate that B-RASPF ◀corresponds to POFAs, a subclass of counter-free automata that characterize LTL[3 −]. A translation from POFAs to B-RASPF ◀is provided in App. C.1. Direct Descriptions of T F ◀ We now describe an alternative description of T F ◀ that directly translates it to LTL[3 −] and POFAs. 7.1 Describing T F ◀with LTL[3 −] In a transformer, the contextual representation at layer ℓ, x(ℓ) t , determines a function that computes the next representation, x(ℓ+1) t , given the unmasked symbols using the attention mechanism. In UHATs, this function is particularly simple: It computes x(ℓ+1) t by selecting the symbol with the highest attention score (as per the tiebreaking mechanism), x(ℓ) t∗, and combines it with x(ℓ) t via the residual connection: x(ℓ+1) t = x(ℓ) t + x(ℓ) t∗; see Fig. 1. This invites the interpretation of transformer layers collecting progressively richer representations of individual symbols by selecting a new representation to append at each layer. We translate this idea into a set of LTL[3 −] formulas of the form ϕ(ℓ) x(ℓ) t ←x(ℓ) t∗that keep track of the fact that the representation x(ℓ) t was updated with the representation",
        "x(ℓ) x(ℓ) · · · x(ℓ) t∗ · · · x(ℓ) t−1 x(ℓ) t att + x(ℓ+1) t Figure 1: Unique hard attention. x(ℓ) t∗is combined with x(ℓ) t to compute x(ℓ+1) t = x(ℓ) t + x(ℓ) t∗. x(ℓ) t∗at layer ℓ. The full formula is presented in the proof of the following theorem in App. D.2. Theorem 7.1. Let T ∈T F ◀be a transformer. Then, there exists an equivalent formula ψ ∈LTL[3 −]. 7.2 Describing T F ◀with POFAs To provide an automata-theoretic take on the result, we directly express T F ◀transformers with POFAs in the proof of the following theorem in App. D.3. Theorem 7.2. Let T ∈T F ◀be a transformer. Then, there exists an equivalent POFA. Discussion and Conclusion We establish the equivalence of future-masked finite-precision leftmost UHATs with no positional encodings, T F ◀, to a fragment of linear temporal logic, LTL[3 −]. Together with Yang et al.’s and Li and Cotterell’s results, this largely completes the picture of finite-precision transformer expressivity. Equivalence to soft-attention transformers. §6 not only provides a characterization of T F ◀in terms of LTL[3 −], but also establishes its equivalence to future-masked, finite-precision soft-attention transformers, in conjunction with the results of Li and Cotterell’s (2025) (summarized in Tab. 1). This equivalence yields a compelling interpretation of T F ◀as a principled abstraction of soft-attention transformers—one that is more appropriate than T F ▶due to the expressivity gap between soft attention and rightmost UHA. Consequently, this motivates further investigation of T F ◀as a simplified yet faithful analog of more complex soft-attention architectures. Dot-depth hierarchy. The dot-depth hierarchy (Cohen and Brzozowski, 1971) classifies star-free languages based on the minimal alternation depth of concatenation and Boolean operations in the regular expressions defining them. The hierarchy is infinite and reflects increasing expressive power. Brzozowski and Ellen (1980) show that the class of languages definable in LTL[3 −] forms a strict subclass of dot-depth 2, while being incomparable to dot-depth 1. In a related line of work, Bhattamishra et al. (2020) empirically find that transformers struggle to generalize to star-free languages with dot-depth greater than 1. Until Hierarchy. An alternative (infinite) hierarchy spanning the star-free languages is that of until hierarchy (Etessami and Wilke, 1996; Thérien and Wilke, 2001), which stratifies the family according to the required number of U (or equivalently, S) operations in an LTL formula required to define a language. Our and Li and Cotterell’s (2025) results naturally place leftmost UHA and soft-attention transformers within the 0th layer of this hierarchy. Abilities and Limitations. Exact characterizations of transformers allow us to derive precise conclusions about models’ abilities and limitations. Our results, in particular, mean that T F ◀transformers, like their soft-attention counterparts, cannot model simple languages such as (i) strictly local languages and n-gram models, which have been linked to infinite-precision UHATs (Svete and Cotterell, 2024); (ii) locally testable languages (which require detecting contiguous substrings); (iii) languages of nested parentheses of bounded depth (bounded Dyck languages) which have also been linked to infinite-precision transformers (Yao et al., 2021); and (iv) any non-star-free languages such as PARITY. In contrast, the equivalence to LTL[3 −] means that T F ◀can model simple languages such as those whose string membership depends on the presence of not necessarily contiguous subsequences or on the string prefix. Li and Cotterell (2025) find strong empirical evidence that this theoretical equivalence faithfully translates into the practical performance of trained transformers on formal languages. A duality. We finally note that past masking and rightmost UHA has a natural characterization with LTL[3 + ], the fragment of LTL with only the 3 + operator. This duality is summarized in Tab. 1 and elaborated on in App. E."
      ]
    },
    {
      "section": "Limitations",
      "chunks": [
        "We limit ourselves to a purely theoretical investigation of the expressivity of a particular model of a transformer. In particular, our results hold for finite-precision transformers with leftmost hard",
        "attention and no positional encodings. This is important to consider as the representational capacity of transformers depends on the choices made for all these components. We do not consider learnability and training, which are opening up to be an exciting area of study and promise to bring our understanding of the representational capacity of transformers closer to what we observe in practice (Hahn and Rofin, 2024). In fact, due to the discrete nature of the hard-attention mechanism, training UHATs in practice is infeasible. Nevertheless, recent work shows how temperature scaling and unbounded positional encodings can be used to simulate hard attention with soft attention (Yang et al., 2024b). We leave the empirical investigation of contrasting the performance of T F ◀, T F ▶, T P ▶on various language classes to future work."
      ]
    },
    {
      "section": "Ethics Statement",
      "chunks": [
        "We used AI-based tools (ChatGPT and GitHub Copilot) for writing assistance. We used the tools in compliance with the ACL Policy on the Use of AI Writing Assistance."
      ]
    },
    {
      "section": "Acknowledgments",
      "chunks": [
        "We would like to thank Andy Yang and Michael Hahn for useful discussions and feedback on early versions of this work. Anej Svete and Jiaoda Li are supported by the ETH AI Center Doctoral Fellowship."
      ]
    },
    {
      "section": "References",
      "chunks": [
        "Pablo Barceló, Alexander Kozachinskiy, Anthony Widjaja Lin, and Vladimir Podolskii. 2024. Logical languages accepted by transformer encoders with hard attention. In The Twelfth International Conference on Learning Representations. Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. 2020. On the Ability and Limitations of Transformers to Recognize Formal Languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7096–7116, Online. Association for Computational Linguistics. Janusz Antoni Brzozowski and Faith Ellen. 1980. Languages of R-trivial monoids. Journal of Computer and System Sciences, 20(1):32–49. Ashok K. Chandra, Larry Stockmeyer, and Uzi Vishkin. 1984. Constant depth reducibility. SIAM Journal on Computing, 13(2):423–439. David Chiang and Peter Cholak. 2022. Overcoming a theoretical limitation of self-attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7654–7664, Dublin, Ireland. Association for Computational Linguistics. David Chiang, Peter Cholak, and Anand Pillay. 2023. Tighter bounds on the expressivity of transformer encoders. In Proceedings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org. Rina Cohen and Janusz Antoni Brzozowski Brzozowski. 1971. Dot-depth of star-free events. Journal of Computer and System Sciences, 5(1):1–16. K. Etessami and T. Wilke. 1996. An Until hierarchy for temporal logic. In Proceedings 11th Annual IEEE Symposium on Logic in Computer Science, pages 108–117. Dov Gabbay, Amir Pnueli, Saharon Shelah, and Jonathan Stavi. 1980. On the temporal analysis of fairness. In Proceedings of the 7th ACM SIGPLANSIGACT Symposium on Principles of Programming Languages, POPL ’80, page 163–173, New York, NY, USA. Association for Computing Machinery. Michael Hahn. 2020. Theoretical limitations of selfattention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156–171. Michael Hahn and Mark Rofin. 2024. Why are sensitive functions hard for transformers? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14973–15008, Bangkok, Thailand. Association for Computational Linguistics. Yiding Hao, Dana Angluin, and Robert Frank. 2022. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. Transactions of the Association for Computational Linguistics, 10:800–810. William Hesse. 2001. Division is in uniform TC0. In Automata, Languages and Programming, pages 104– 114, Berlin, Heidelberg. Springer Berlin Heidelberg. Hans Kamp. 1968. Tense Logic and the Theory of Linear Order. Ph.D. thesis, UCLA. Kenneth Krohn and John Rhodes. 1965. Algebraic theory of machines. I. Prime decomposition theorem for finite semigroups and machines. Transactions of The American Mathematical Society - TRANS AMER MATH SOC, 116. Jiaoda Li and Ryan Cotterell. 2025. Characterizing the expressivity of transformer language models. arXiv preprint arXiv:2505.23623. Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. 2023. Exposing attention glitches with flip-flop language modeling. In",
        "Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS ’23, Red Hook, NY, USA. Curran Associates Inc. Robert McNaughton and Seymour Papert. 1971. Counter-Free Automata. M.I.T. Press research monographs. M.I.T. Press. William Merrill and Ashish Sabharwal. 2023. A logic for expressing log-precision transformers. In Thirtyseventh Conference on Neural Information Processing Systems. William Merrill, Ashish Sabharwal, and Noah A. Smith. 2022a. Saturated transformers are constant-depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843–856. William Merrill, Ashish Sabharwal, and Noah A. Smith. 2022b. Saturated transformers are constant-depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843–856. Franz Nowak, Anej Svete, Alexandra Butoi, and Ryan Cotterell. 2024. On the representational capacity of neural language models with chain-of-thought reasoning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12510–12548, Bangkok, Thailand. Association for Computational Linguistics. Jorge Pérez, Pablo Barceló, and Javier Marinkovic. 2021. Attention is turing-complete. Journal of Machine Learning Research, 22(75):1–35. Amir Pnueli. 1977. The temporal logic of programs. In 18th Annual Symposium on Foundations of Computer Science (sfcs 1977), pages 46–57. Ofir Press, Noah Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations. Lena Strobl. 2023. Average-hard attention transformers are constant-depth uniform threshold circuits. arXiv preprint arXiv:2308.03212. Lena Strobl, William Merrill, Gail Weiss, David Chiang, and Dana Angluin. 2024. What formal languages can transformers express? a survey. Transactions of the Association for Computational Linguistics, 12:543–561. Anej Svete and Ryan Cotterell. 2024. Transformers can represent n-gram language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 6845–6881, Mexico City, Mexico. Association for Computational Linguistics. Denis Thérien and Thomas Wilke. 2001. Temporal Logic and Semidirect Products: An Effective Characterization of the Until Hierarchy. SIAM Journal on Computing, 31(3):777–798. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Andy Yang, David Chiang, and Dana Angluin. 2024a. Masked hard-attention transformers recognize exactly the star-free languages. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Andy Yang, Lena Strobl, David Chiang, and Dana Angluin. 2024b. Simulating hard attention using soft attention. Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. 2021. Self-attention networks can process bounded hierarchical languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3770–3785, Online. Association for Computational Linguistics.",
        "A"
      ]
    },
    {
      "section": "Related Work",
      "chunks": [
        "Existing work has established a rich landscape of results on the expressivity of transformers. Lower and upper bounds for UHA. Hahn (2020) show that UHA transformers with unbounded precision, left attention, and no masking can not recognize PARITY (bit strings with an odd number of ones) nor DYCK-1 (language of correctly nested parentheses of one type). Hao et al. (2022) refine this result by showing that UHA transformers with unbounded precision and left attention can recognize at most languages in AC0, the family of circuits of constant depth, polynomial size, and unbounded fan-in. Maybe surprisingly, this suggests such transformers can not recognize even simple languages outside AC0 such as PARITY and MAJORITY (all bit strings in which more than half of bits are 1s). Other problems not in AC0 include sorting, integer multiplication (Chandra et al., 1984), and integer division (Hesse, 2001). Barceló et al. (2024) show that UHA transformers augmented with arbitrary positional encodings are lower bounded by an extension of FO[<] with all possible monadic numerical predicates (which includes all regular languages in AC0). Yang et al. (2024a) further refine the understanding of the relationship between FO[<] and finite-precision transformers by proving the equivalence between the two when the transformers are equipped with strict future masking. Average-hard attention. Average-hard attention (AHA) differs from UHA in that when confronted with several positions with equal scores, they average their values to compute the next contextual representation. It can be seen as a special case of soft-attention (Li and Cotterell, 2025). Hao et al. (2022) show AHA unbounded precision transformers are more expressive than UHA transformers, as they can recognize languages outside AC0, such as PARITY and DYCK-1. Merrill et al. (2022b) show that AHA transformers with floating-point activations can be simulated in TC0 (the extension of AC0 with majority gates, which output 1 iff at least half of the inputs are 1), while Strobl (2023) extend this result by tightening the upper bound to L-UNIFORM TC0 (which consists of TC0 circuits with the additional constraint that there exists a deterministic Turing Machine that runs in logarithmic space that can describe the circuits). Transformers and logic. Previous results relating transformers to logic include Chiang et al. (2023), who show that finite-precision softmax transformers are upper-bounded by a generalization of first-order logic with counting quantifiers and modular arithmetic over input position indices. On the other hand, they show this logic to be a lower bound on the expressivity of unbounded-precision transformers. Merrill and Sabharwal (2023) contribute by characterizing a more expressive variant of transformers—they allow precision logarithmic in the input length and show an upper bound of first-order logic extended with majority-vote quantifiers. Equivalence of right-attention transformers to LTL[3 −, 3 + , S, U]. Yang et al. (2024a) show the equivalence of T F ▶and T P ◀to LTL[3 −, 3 + , S, U]. In their constructions, the operators S and U in LTL[3 −, 3 + , S, U] can only be expressed by transformers in T F ▶, T P ◀respectively (and vice-verse, T F ▶and T P ◀require the operators S,U respectively in their LTL[3 −, 3 + , S, U] formulation). Moreover, by Gabbay et al. (1980), the fragment of LTL[3 −, 3 + , S, U] consisting of only U and the Boolean connectives, denoted by LTL[U] (or, in the analogous symmetric case, LTL[S]), is sufficient for equivalence FO[<] (which is equivalent to LTL[3 −, 3 + , S, U] by Kamp (1968)). Equivalence of soft-attention transformers to LTL[3 −]. Li and Cotterell (2025) relates LTL[3 −] and PFO2[<] (the fragment of FO[<] which considers at most two distinct variables at the same time where any bound variable can only peek into the past of a free variable) to languages with R-trivial monoids (Brzozowski and Ellen, 1980). These are described by a set of equivalent formalisms such as R-expressions, R-trivial monoids, and partially ordered automata. Li and Cotterell (2025) use this equivalence to show strict future-masked softmax transformers are equivalent to LTL[3 −]. The upper bound is proven through the equivalent PFO2[<]: They show PFO2[<] can simulate a sum of a finite number of fixed-precision floating-point numbers (and thus the weighted sum in softmax), while other transformations (computations of keys and queries, dot-products, etc.) can be computed with the standard Boolean connectives. They show the lower bound of LTL[3 −] by translating the standard Boolean",
        "operations into feedforward networks, and the 3 −operation with the future-masked attention mechanism. Together with Hao et al.’s (2022) results, this illuminates the complexity of transformer expressivity: While in the unbounded-precision regime, AHA transformers strictly subsume UHA transformers, in the finite-precision regime, the relationship depends on the direction of attention mechanism. The models are equivalent if leftmost tiebreaking is used and in the case of rightmost tiebreaking, UHA is strictly more expressive than AHA, the reverse of the unbounded-precision case. B",
        "This section provides the necessary background on the formalisms used in the paper and details some concepts introduced in the main part of the paper. B.1 Finite-State Automata Definition B.1. A semiautomaton A is a 3-tuple (Σ, Q, δ) where Σ is an alphabet, Q is a finite set of states and δ: Q × Σ →Q is a transition function. We further define an initialized semiautomaton as a semiautomaton with an initial state. Definition B.2. A deterministic finite automaton (DFA) A is a 5-tuple (Σ, Q, qι, F, δ) where (Σ, Q, δ) is a semiautomaton, qι ∈Q is an initial state, and F ⊆Q is a set of final states. Definition B.3. Let δ∗: Q × Σ∗→Q be the transitive closure of δ, defined as δ∗(q, w) = δ(q, w), for w ∈Σ (12a) δ∗(q, w1 · · · wT ) = δ(δ∗(q, w1 · · · wT−1), wT ) (12b) with δ∗(q, ε) = q for any q ∈Q. A partially ordered DFA (POFA) is a DFA A = (Σ, Q, qι, F, δ) where there is a partial order relation ⪯on Q defined as q ⪯p if and only if δ∗(q, w) = p for some string w ∈Σ∗. Intuitively, POFAs are acyclic DFAs with possible self-loops, resulting in partially ordered states. Definition B.4. Let a DFA A = (Σ, Q, qι, F, δ) and L be the language it accepts. We define the reverse automaton AR as the automaton that recognizes the reverse language LR consisting of all strings in L but reversed. Definition B.5. A DFA A is a partially ordered reverse automaton (RPOFA) if AR is a POFA. Definition B.6. Let B1 = (Σ, Q1, δ1) be a semiautomaton. Let B2 = (Q1 × Σ, Q2, δ2) be another, possibly partial,6 semiautomaton such that for every q1 ∈Q1, q2 ∈Q2, either δ(q2, ⟨q1, w⟩) is defined for every w ∈Σ or undefined for every w ∈Σ. The cascade product B1 ◦B2 is the semiautomaton C = (Σ, Q, δ) such that: • Q = {(q1, q2) : δ2(q2, ⟨q1, w⟩) is defined} • δ(⟨q1, q2⟩, w) = (δ1(q1, w), δ2(q2, ⟨q1, w⟩)) Definition B.7. For n ≥0, an n-way fork is an initialized semiautomaton (Σ, {q0, q1, · · · , qn}, δ) where Σ = Σ0 ∪Σ1 · · · ∪Σn, the Σi’s are non-empty and pairwise disjoint, δ(q0, a) = qi for all a ∈Σi, and δ(qi, a) = qi for all a ∈Σ. A half-reset (Fig. 2) is a 1-way fork. Definition B.8. A surjection ϕ: Q →Q′ is an automaton homomorphism from the semiautomata A = (Σ, Q, δ) to A′ = (Σ′, Q′, δ′) if for every q ∈Q, w ∈Σ: ϕ(δ(q, w)) = δ′(ϕ(q), w) (13) In this case, we say A′ is homomorphic to A, or that A′ is the homomorphic image of A. 6A partial automaton is one in which some symbol–state combinations could lead to an undefined transition.",
        "q0 q1 Σ1 Σ0 Σ Figure 2: A 1-way fork B.2 Syntactic Monoids Definition B.9. A monoid M is a set equipped with a binary operation and an identity element. For instance, the free monoid is the set Σ∗equipped with the concatenation operation and the empty string ε as identity. Definition B.10. A monoid M is R-trivial if for all w1, w2, w3 ∈M, w1w2w3 = w1 implies w1w2 = w1. A monoid M is L-trivial if for all w1, w2, w3 ∈M, w3w2w1 = w1 implies w2w1 = w1. More details about R-trivial monoids can be found in Brzozowski and Ellen (1980). Definition B.11. The syntactic congruence ⪯L is the equivalence relation on Σ∗given the language L such that for all x, y ∈Σ∗, we have x⪯Ly if and only if: sxz ∈L ⇐⇒syz ∈L ∀s, z ∈Σ∗ (14) Definition B.12. The quotient monoid Σ∗/⪯L is the syntactic monoid of L. B.3 Regular Expressions A regular language can be described by regular expressions, which are elements of the closure of ∅, ε, and all w ∈Σ under concatenation, concatenation, and Kleene star. A regular language is star-free if the regular expression that describes the language does not require the Kleene star operator. Definition B.13. An R-expression is a finite union of regular expressions of the form Σ∗ 0w1Σ∗ 1 · · · wnΣ∗ n where wt ∈Σ, Σt ⊆Σ and wt /∈Σ∗ t−1 for 1 ≤t ≤n. An L-expression is defined analogously with the dual constraint of wt /∈Σ∗ t . For instance, aΣ∗is an R-expression, while Σ∗a is an L-expression. B.4 Linear Temporal Logic We now present all possible semantics in LTL[3 −, 3 + , S, U]. The semantics are defined inductively: • w, t |= πa ⇐⇒wt = a; • w, t |= ψ1 ∨ψ2 ⇐⇒w, t |= ψ1 ∨w, t |= ψ2; • w, t |= ψ1 ∧ψ2 ⇐⇒w, t |= ψ1 ∧w, t |= ψ2; • w, t |= ¬ψ ⇐⇒w, t ̸|= ψ; • w, t |= 3 −ψ ⇐⇒∃t′ < t : w, t′ |= ψ; • w, t |= 3 + ψ ⇐⇒∃t′ > t : w, t′ |= ψ; • w, t |= ψ1Sψ2 ⇐⇒∃t′ < t : w, t′ |= ψ2 and w, k |= ψ1 for all k with t′ < k < t; • w, t |= ψ1Uψ2 ⇐⇒∃t′ > t : w, t′ |= ψ2 and w, k |= ψ1 for all k with t < k < t′. LTL[3 −, 3 + , S, U] defines exactly the class of star-free languages (Kamp, 1968; McNaughton and Papert, 1971). LTL[3 −] and LTL[3 + ], the fragments of LTL with only the 3 −and 3 + operators, respectively, are strictly less expressive than LTL[3 −, 3 + , S, U] (neither can recognize aΣ∗b, which can be recognized by LTL[3 −, 3 + , S, U]), and have characterizations in terms of monoids, automata, and first-order logic (App. B.6).",
        "B.5 First-Order Logic First-order logic with the < relation, denoted by FO[<], extends the usual propositional logic with predicates and quantifiers. We consider free variables x, y, z, · · · that represent positions over a string w of size T. FO[<] includes unary predicates πw for w ∈Σ, where πw(x) = TRUE ⇐⇒there is a w at position x. As with LTL[3 −, 3 + , S, U], we can inductively define formulas from πw using the standard Boolean operators, the binary predicate < between positions, and the existential quantifier ∃. By Kamp (1968), FO[<] is equivalent to LTL[3 −, 3 + , S, U] and by McNaughton and Papert (1971), it is equivalent to the star-free languages. FO2[<] is the fragment of FO[<] that can only consider two distinct variables at the same time. Li and Cotterell (2025) further define PFO2[<] as the past fragment of FO2[<] that can only “peek into the past.” Namely, any single-variable formula ϕ(x) can only have bounded existential quantifiers of the form \"∃y < x\". We analogously have the future fragment of FO2[<], FFO2[<], where we only allow peeking into the future with quantifiers of the form \"∃y > x\". B.6 Equivalence Between Formalisms Due to Brzozowski and Ellen (1980) and Li and Cotterell (2025), we have the two dual theorems: Theorem B.1. Let L ⊆Σ∗be a regular language, M be its syntactic monoid, and A be the minimal DFA accepting it. The following conditions are equivalent: (i) M is R-trivial, (ii) L can be denoted by an R-expression, (iii) A is a POFA, (iv) A is the homomorphic image of a cascade product of half-resets, (v) L can be recognized by a formula in PFO2[<], (vi) L can be recognized by a formula in LTL[3 −]. Theorem B.2. Let L ⊆Σ∗be a regular language, M be its syntactic monoid, and A be the minimal DFA accepting it. The following conditions are equivalent: (i) M is L-trivial, (ii) L can be denoted by an L-expression, (iii) A is a RPOFA (equivalently, AR is a POFA), (iv) AR is the homomorphic image of a cascade product of half-resets, (v) L can be recognized by a formula in FFO2[<], (vi) L can be recognized by a formula in LTL[3 + ]. C Proofs of T F ◀and LTL[3 −] Equivalence The equivalence between T F ◀and B-RASPF ◀follows directly from Theorems 3 and 4 of Yang et al. (2024a). Theorem 5.1. For any UHAT in T F ◀, there exists an equivalent B-RASPF ◀program. For any B-RASPF ◀ program, there exists an equivalent UHAT in T F ◀. Proof. Yang et al. (2024a, Thms. 3 and 4) and the supporting lemmata (Lemmata 21 and 24) treat ◀and ▶separately. Translations constrained to ◀are thus special cases of their proofs. ■ The following proofs largely follow the proofs of Lemmata 13 and 14 of Yang et al. (2024a). We highlight the part that distinguishes ◀and ▶in red. Theorem 6.1. For any formula ψ of LTL[3 −], there is a B-RASPF ◀program with a Boolean vector Pψ such that, for any input w of length T and all t ∈[T], we have w, t |= ψ ⇐⇒Pψ(t) = 1. Proof. We proceed by induction. Base case. The atomic formulas πa can be represented by initial Boolean vectors Qa for a ∈Σ.",
        "Inductive step. Assume ψ1 and ψ2 can be converted to B-RASPF ◀vectors Pψ1 and Pψ2, respectively. We distinguish three cases of building a new formula from ψ1 and ψ2: • Case (1): ψ = ¬ψ1. Add a position-wise operation: Pψ(t) = ¬Pψ1(t). (15) • Case (2): ψ = ψ1 ∧ψ2. Add a position-wise operation: Pψ(t) = Pψ1(t) ∧Pψ2(t). (16) • Case (3): ψ = 3 −ψ1. Add an attention operation with future masking and ◀tiebreaking: Pψ(t) = ◀t′[t′ < t, Pψ1(t′)] Pψ1(t′) : 0. (17) ■ Theorem 6.2. For any Boolean vector P of a B-RASPF ◀program P, there is a formula ψP of LTL[3 −] such that for any input w of length T and all t ∈[T], we have P(t) = 1 ⇐⇒w, t |= ψP . Proof. We proceed by induction. Base case. Each atomic vector Qa(t) can be translated to the atomic formula πa. Induction step. Assume vectors P1, . . . , Pi−1 can be translated to ψP1, . . . , ψPi−1. We distinguish different options of building a new vector out of P1, . . . , Pi−1: • Case (1): Pi(t) is a position-wise operation: Pi(t) = f(P1(t), . . . , Pi−1(t)), (18) where f is a Boolean function. We can translate Pi(t) into ψi = f(ψ1, . . . , ψi−1). • Case (2): Pi(t) is an attention operation that uses leftmost tiebreaking and future masking, that is, Pi(t) = ◀t′[t′ < t, s(t′)] v(t′) : d(t). (19) By the inductive hypothesis, there are LTL[3 −] formulas ψS, ψV , and ψD corresponding to s, v, and d. We can thus write Pi(i) as: ψi = (3 −(ψS ∧¬3 −ψS ∧ψV )) ∨(¬(3 −ψS) ∧ψD). (20) where ψS ∧¬3 −ψS identifies the leftmost position that satisfies ψS in the string. • Case (3): We now treat the attention operation with rightmost tiebreaking and future masking, i.e., Pi(t) = ▶t′[t′ < t, s(t′)] v(t′) : d(t). (21) In this case, we need to find the rightmost position satisfying ψS not in the entire string w, but before the current position, which can only be realized using the S operator: ψi = (¬ψSS(ψS ∧ψV )) ∨(¬(3 −ψS) ∧ψD). (22) ■",
        "C.1 POFAs as T F ◀ The Krohn–Rhodes theorem (Krohn and Rhodes, 1965) states that any deterministic automaton is the homomorphic image of a cascade of simple automata whose transitions induce resets or permutations of the states. Thm. B.1 by Brzozowski and Ellen (1980) provides an analogous decomposition for POFAs, namely that they are homomorphic to a cascade of half-resets. In a very similar manner to Yang et al. (2024a)’s Thm. 15, we can express this decomposition in a B-RASPF ◀program, which can further be expressed by a transformer in T F ◀. We first formalize how B-RASP can compute sequence-to-sequence functions Σ∗→Γ∗if for instance, we want a B-RASP program to simulate an automaton (i.e., output the same states traversed by the automaton when ran on the same input). We designate a set of output vectors Yγ, for all γ ∈Γ, and the sequence-to-sequence function then maps t to γ if Yγ(t) is true. We then say a B-RASPF ◀program P simulates an initialized semiautomaton A = (Σ, Q, δ) with initial state qs iff for every input string w, the output vectors of P encode the sequence of states traversed by A when ran on w with initial state qs. Lemma C.1. Let H = (Σ0 ∪Σ1, {q0, q1}, q0, δ) be a half-reset. Then there exists a B-RASP program PH that simulates H started in start state q0. Proof. We first define two B-RASP programs B0, B1, which return 1 if and only if the half-reset is in state q0 or q1, respectively. Namely: B0(t) = ◀t′[t′ < t, _ w∈Σ1 Qw(t′)] 0 : 1 B1(t) = 1 −B0(t) (23) In half-resets, the presence or absence at any instance of symbols in Σ1 determines whether we are in q0 or q1. It suffices to check if at the current index t, there has been a past w ∈Σ1, which can be done by computing W w∈Σ1 Qw(t′) for all indices t′ satisfying future masking. ■ Lemma C.2. Let A = (Σ, Q1, δ1) be a semiautomaton that can be simulated from state s1 by a B-RASP program PA. Let H = (Q1 × Σ, Q2, δ2) be a half-reset and let C = A ◦H. Then, there exists a program PC that simulates C started in state (s1, s2) for an arbitrary s2 ∈Q2. Proof. We now use for all q ∈Q1, the predicates BA,q that denote whether A is in some state q ∈Q1 at time t when started at s1. By the assumption that A can be simulated by a B-RASPF ◀program, we have access to such predicates. We now define predicates for q ∈Q1, w ∈Σ: Q′ (q,w)(t) = BA,q(t) ∧Qw(t) (24) These formulas encode the presence of an element in Q1 × Σ (a state in the first semiautomaton A and an alphabet symbol) for the half-reset H. As H is a half-reset, we can classify every tuple (q, w) ∈Q1 × Σ into one of two sets Σ0 or Σ1 (depending on if they reset to q0 or q1 in H). As in Lem. C.1, we can define predicates BH,q(t) using the predicates Q′ (q,w) and the known classification of elements in Q1 × Σ into some state q0 or q1. To finally simulate the cascade product C = A ◦H, we define predicates that compute for every state (q, p), q ∈Q1, p ∈Q2, whether C is in it: C(q,p)(t) = BA,q(t) ∧BH,p(t) (25) ■ Theorem C.1. Let A be a POFA. Then, there exists an equivalent transformer T ∈T F ◀.",
        "Proof. Let C = B0 ◦· · · ◦Bk the semiautomaton A is homomorphic to. Let ϕ: Q′ →Q be the homomorphism from C to A (where Q are the states of A, Q′ are the states of C). By Lem. C.2, we can iteratively define formulas Cq′ for all q′ ∈Q′ that simulate the semiautomaton C. If we write instructions describing the homomorphism ϕ, we can then write formulas that yield the states traversed by A as: Aq(t) = _ p∈Q′,ϕ(p)=q Cp(t) (26) Aq(t), however, describes the state before reading the symbol at t (by strict masking). We want predicates that yield the state after reading at the symbol position t. We thus write: Yq(t) = _ p∈Q,w∈Σ δ(p,w)=q Ap(t) ∧Qw(t) (27) Finally, we denote by F the set of final states in A. We thus define the output vector Y by: Y (t) = _ q∈F Yq(t) (28) Y (T) = 1 if and only if A is in one of the final states when reading the entire string. This concludes the translation of A to B-RASPF ◀, showing the existence of a T ∈T F ◀equivalent to A. ■ D Direct Translations of T F ◀ D.1 A Normal Form for T F ◀Transformers The representational capacity of transformers depends tightly on the modeling assumptions. §6 studies the same architecture as Yang et al. (2024a). In this section, we outline a formalization of T F ◀transformers in a form that allows us to describe direct translations to LTL[3 −] and POFAs in §7 more easily. The idea of this is similar to the normal form of Hao et al. (2022). D.1.1 Finite Precision and Simplifications As Yang et al. (2024a), we work with finite-precision transformers with no positional encodings. Further, we focus on strict future masking and leftmost hard attention, defined formally below. In our formalization, we omit element-wise transformations such as the query, key, and value transformations and element-wise MLPs. Instead, we directly aggregate the original representations into (constant-size) vectors that can take increasingly many values as the number of layers increases.7 The intuition and motivation for this stems from the use of hard attention: At any layer, hard attention augments the current contextual representation with the representation of one (preceding) symbol—the hardmax. Thus, all information that the transformer can capture is already captured if the contextual representations keep around the identities of the elements that were returned by hardmax. The element-wise representations, in this case, do not provide any additional representational power. We elaborate on this in App. D.1.3. Note that we do not use layer normalization in our transformers and focus on a single-head attention mechanism. Both can be incorporated into our proofs analogously to the solutions presented in Yang et al. (2024a) (see Yang et al. (2024a, §4.1) for a discussion of multi-head attention and Yang et al. (2024a, §4.3) for layer normalization). D.1.2 The Attention Mechanism The central component of a transformer is the transformer layer. Definition D.1. A transformer layer L: \u0000RD\u0001+ → \u0000RD\u0001+ is a length-preserving function defined as L((x1, . . . , xT )) def= (yt)T t=1 (29a) (y1, . . . , yT ) def= att((x1, . . . , xT )) + (x1, . . . , xT ) 7This is formalized in Lem. D.1.",
        "We will use Lt def= projt ◦L for the function that computes L((x1, . . . , xT )) and extracts the contextual representation of the tth symbol by projecting out that dimension. The attention mechanism att is specified with the following components: • A scoring function score: RD × RD →R. • A masking function m: N × N →{0, 1} that determines the positions attended to. We use future and past masking as in B-RASP. We write M(t) def= {t′ | m(t, t′) = 1}. • A normalization function norm: RT →∆T−1 that normalizes the attention scores. We then define the attention mechanism as: att((x1, . . . , xT )) def= (y1, . . . , yT ) (30a) yt def= X t′∈M(t) st′xt′ (30b) s def= norm((score(xt′, xt))t′∈M(t)) (30c) The P-notation in Eq. (30b) can naturally be thought of as collecting information from the unmasked positions. Intuitively, if the space of contextual representations is large enough, this can be interpreted as concatenating the representations together. This will be particularly useful in our UHA formulation, where only a single xt′ will be selected and the finiteness of the representation space will mean that all relevant information about the string w≤t will be stored in xt. See also App. D.1.3. Let L ∈N≥1 and L1, . . . , LL be transformer layers. A transformer T: Σ∗→(RD)+ is a composition of transformer layers: T def= LL ◦· · · ◦L1 ◦embed (31) where embed: Σ∗→ \u0000RD\u0001+ is a position-wise embedding function that maps symbols to their static representations. We also write (x(ℓ) 1 , . . . , x(ℓ) T ) = (Lℓ◦· · · ◦L1 ◦embed)(w) (32) for some layer index ℓ∈[L] and string w = w1 · · · wT ∈Σ∗. We call x(ℓ) t the contextual representation of symbol wt at layer ℓ. A transformer computes the contextual representations of the string w = w1 · · · wT EOS as (x(L) 1 , . . . , x(L) T , x(L) EOS) def= T(w). (33) We take x(ℓ) EOS to be the representation of the entire string. This motivates the definition of the transformer encoding function h: h(w) def= x(L) EOS. (34) This allows us to define a transformer’s language. This is usually defined based on a linear classifier based on h(w); L(T) def= {w ∈Σ∗| θ⊤h(w) > 0} (35) for some θ ∈RD. Since we are working with finite-precision transformers, the set of possible h(w) is finite (in our normal form, it is a subset of Σ2L). We can thus equate the condition θ⊤h(w) > 0 with h(w)’s inclusion in a subset of Σ2L and define the language of a transformer as follows. Definition D.2. Let T ∈T F ◀. We define its language L(T) as L(T) def= {w ∈Σ∗| h(w) ∈FT} (36) where FT is a set of accepting final representations.",
        "D.1.3 Unique Hard Attention Let C ∈{min, max} and let s ∈RN. We define hardmax(s)n def= ( if n = C(argmax(s)) otherwise (37) Here, argmax s denotes the set of indices attaining the maximum value in the vector s. The function C selects a unique index from this set: C = max corresponds to rightmost tiebreaking ▶and C = min corresponds to leftmost tiebreaking ◀. Definition D.3. Unique hard attention is computed with the hardmax projection function: hardmax(s) = hardmax(s) (38) With some abuse of notation, we sometimes write hardmax(s) for the position C(argmax(s)). We denote future or past masking with F or P, respectively. T F ◀, for example, denotes the class of transformers with future masking and leftmost attention. The following lemma is a restatement of Yang et al. (2024a, Lem. 22). Lemma D.1. Let T be a UHA transformer over Σ. Denote with x(ℓ) t the contextual representation of the symbol wt at layer ℓ. The following holds: |{x(ℓ) t | w ∈Σ∗, t ∈[|w|]}| ≤|Σ|2ℓ. (39) Proof. We prove the statement by induction on the number of layers. Base case: ℓ= 1. In the first layer, as we have static representations for symbols, the embedding at some position t is uniquely determined by the symbol wt at that position. We thus have exactly |Σ| possible representations for a given position, regardless of the length of the string. Inductive step: ℓ> 1. Suppose that the invariance holds for ℓ−1: |{x(ℓ−1) t | w ∈Σ∗, t ∈[|w|]}| ≤ |Σ|2ℓ−1. For any position t in the string, at layer ℓ, we will compute x(ℓ) t = att((x1, . . . , xT )) + (x1, . . . , xT ) where x(ℓ−1) t is the representation of the symbol at the previous layer ℓ−1. By the induction hypothesis, the element x(ℓ−1) t takes one out of at most |Σ|2ℓ−1 possible values. Moreover, the attention mechanism selects one element from the previous layer ℓ−1, which holds one out of at most |Σ|2ℓ−1 possibles values by the induction hypothesis. The element at holds thus one out of |Σ|2ℓ−1 × |Σ|2ℓ−1 = |Σ|2ℓrepresentations, concluding the induction step and the proof. ■ Contextual representations as elements of a finite set. Lem. D.1 allows us to simplify notation: Any representation of a symbol at layer ℓis uniquely identified by 2ℓsymbols, i.e., x(ℓ) ∈Σ2ℓ. We think of this as the collection of representations attended to at each of layer ℓ′ < ℓ, since each selects a single position to be added to the current representation. We will thus refer to x(ℓ) as elements of Σ2ℓ. D.1.4 An Invariance In this section and in App. D.2, we use Σ for the alphabet of input symbols and Ξ for a general alphabet (finite set) of relevance. Later, Ξ will correspond to sets of the form Σ2ℓfor some ℓ∈N. Definition D.4. Let Ξ be an alphabet and w ∈Ξ∗. We define the symbol order ω(w) of w as the string obtained from w by keeping only the first occurrence of each symbol. We define the following relation on Ξ∗: w ≃ω w′ ⇐⇒ω(w) = ω \u0000w′\u0001 . (40) It is not hard to verify that ≃ω is an equivalence relation on Ξ∗and to verify that |Ξ∗/ ≃ω | = |OS(Ξ)|, where OS(Ξ) is the (finite) set of all ordered subsets of Ξ. We denote the equivalence class of w ∈Ξ∗by [w] ∈Ξ∗/ ≃ω. We have the following important invariance.",
        "Lemma D.2 (Attention invariance). Let L be an T F ◀transformer layer over Ξ. For any w, w′ ∈Ξ∗, if w ≃ω w′, then L|w|(w) = L|w′|(w′). Proof. This follows directly from the definition of leftmost hard attention: Additional occurrences of symbols to the right of the first occurrence do not change the position attended to, meaning that the output at the final symbol is the same. ■ D.2 T F ◀as LTL[3 −] We view a transformer layer as a function that takes a contextual representation x(ℓ) t and returns a function that takes in an ordered set of contextual representations ω(w) with w ∈Ξ∗and returns the representation chosen by the unique hard attention mechanism. L(ℓ) : Ξ →Map(OS(Ξ), Ξ), (41a) L(ℓ) : x 7→λx. (41b) λx is the function that takes an ordered set of contextual representations X = (x1, . . . , xN) and returns the representation chosen by UHA:8 λx : OS(Ξ) →Ξ, (42a) λx : X 7→xhardmax((score(xt,xt′))xt′ ∈X ). (42b) We also define x′′ ⪯x x′ ⇐⇒score \u0000x, x′′\u0001 ≤score \u0000x, x′\u0001 (43a) x′′ ≃x x′ ⇐⇒score \u0000x, x′′\u0001 = score \u0000x, x′\u0001 (43b) x′′ ≺x x′ ⇐⇒x′′ ⪯x x′ and ¬ \u0000x′′ ≃x x′\u0001 . (43c) Theorem 7.1. Let T ∈T F ◀be a transformer. Then, there exists an equivalent formula ψ ∈LTL[3 −]. Proof. We define an LTL[3 −] formula representing T ∈T F ◀with the attention mechanism implemented by the function λx (cf. Eq. (42b)) by specifying a set of formulas representing each layer, starting with the initial one, and continuing inductively. At a high level, we define the formulas ϕ(ℓ) x←y to mean that the contextual representation y is added to the contextual representation x at layer ℓ, i.e., that y is the maximizer of the query-key score when x is the query: λx(X) = y for X representing the current string. We construct the formulas layer by layer. Base case: ℓ= 1. We begin by specifying first-layer formulas, which work over Σ. We define for a, b ∈Σ: ϕ(1) a←b def= πa ∧ ^ w∈Σ: b≺aw ¬3 −πw | {z } No previous symbols with higher scores than b ∧3 − \u0010 πb ∧ ^ w∈Σ\\{b}:b≃aw ¬3 −πw | {z } There is b in the past with no equally-scored symbols in its past \u0011 . (44) In words, b’s value is added to a’s static embedding if (i) there are no symbols in the past of a that have a higher score than b and (ii) there exists a position with b in the past such that there are no symbols with equal scores to its left (leftmost tiebreaking). 8We omit the dependence of λ on the layer index for readability.",
        "Inductive step: ℓ> 1. Let now L be the transformer layer at layer ℓ> 1 and assume we have correctly constructed ϕ(ℓ′) x←y for ℓ′ < ℓ. Firstly, we define the formulas π(ℓ) x that, analogously to π, specify the presence of contextual representations for elements in Σ2ℓ. Writing x ∈Σ2ℓas x = (z0, z1, . . . , zℓ−1) with z0 ∈Σ and zℓ′ ∈Σ2ℓ′ , ℓ′ ∈{0, . . . , ℓ−1}, we define: π(ℓ) x = πz0 ∧ ℓ−1 ^ ℓ′=1 ϕ(ℓ′) x≤ℓ′−1←zℓ′ | {z } Verify correct representations throughout layers are added to z0 ∈Σ (45) where x≤ℓ′ = (z0, . . . , zℓ′). This checks the presence of x ∈Σ2ℓas the contextual representation by asserting that the individual representations in x at lower levels were indeed added by checking the formulas ϕ(ℓ′) x≤ℓ′−1←zℓ′. We now define, for x, y ∈Ξ = Σ2ℓ: ϕ(ℓ) x←y = _ X∈OS(Ξ) h order(ℓ−1)(X) | {z } Identify correct ordered subset of representations in the previous layer ∧ best(ℓ−1)(x, y, X) | {z } Check if y is best representation for x given the correct ordered subset X i (46) Intuitively, the formula iterates over all possible ordered subsets of Ξ, checks which one describes the string in the past, and then asserts whether y is the best symbol to add to x given the set of contextual representations X. Here, order(ℓ) is a formula that checks whether ω(w) = X by making sure that the string in the past follows the same order as X: order(ℓ)(X) def= h −(π(ℓ) z1 ∧3 −(π(ℓ) z2 ∧· · · ∧(3 −π(ℓ) z|X|))) i | {z } Elements of X are present in correct order ∧ ^ z∈Ξ\\X ¬3 −π(ℓ) z | {z } Representations not in X are not present (47) Analogously to Eq. (44), best(ℓ) checks whether y is the best symbol to add to x given the set of contextual representations X by (i) asserting x is at the current position, (ii) there are no representations in the past of x given the current ordered subset X that have a higher score than y and (iii) there exists a position in X with y in the past such that there are no representations with equal scores to its left (leftmost tiebreaking). best(ℓ)(x, y, X) def= π(ℓ) x ∧ ^ z∈X: y≺xz ¬3 −π(ℓ) z | {z } No previous representations with higher scores than y ∧3 − \u0010 π(ℓ) y ∧ ^ z∈X\\{y}: y≃xz ¬3 −π(ℓ) z | {z } There is y in the past with no equally-scored representations in its past \u0011 . (48) Finally, let L be the final transformer layer and let F ⊆Σ2L be the set of representations for EOS that lead to string acceptance by T. The LTL[3 −] formula ψ representing T simply has to check whether the representation for EOS is in F: ψ = _ x∈F π(L) x . (49) ■ D.3 T F ◀as POFAs Theorem D.1. Let T ∈T F ◀be a transformer. Then, there exists an equivalent POFA.",
        "Proof. We will construct a semiautomaton A that will, after reading w≤t, store in its state the ordered subsets X (ℓ) ∈OS(Σ2ℓ) of contextual representations for all the transformer layers. In other words, it will hold L equivalence classes of the string w≤t, one for each layer. This state will be updated sequentially according to the self-attention mechanism implemented by the transformer. Formally, given a transformer T ∈T F ◀with the attention mechanism implemented by the function λx (cf. Eq. (42b)) over the alphabet Σ, we define the semiautomaton A = (Σ, Q, δ). We take the set of states Q to be Q def= OS(Σ) × · · · × OS(Σ2L) | {z } Ordered sets of representations of all layers. . (50) For clarity, we will explicitly write out the states q ∈Q in their components: q =      X (0) X (1) ... X (L)      (51) with X (ℓ) ∈OS(Σ2ℓ) for ℓ∈{0, . . . , L}. A will update X (ℓ) with new occurrences of contextual representations by “appending” new contextual representations. Let us describe how the transition function δ updates the state q upon reading the symbol w. We write q for the source state and X (ℓ) for its components, and q′ for the target state and X ′(ℓ) for its components. We then define x′(0) = w to mean that the static representation of this symbol is the symbol itself. For ℓ≥1, we define x′(ℓ) def= \u0012 x′(ℓ−1) λx′(ℓ−1) \u0000X (ℓ−1)\u0001 \u0013 (52) which simulates the ℓth layer of T by (1) copying the symbol’s representation x′(ℓ−1) from the previous layer into the first component (residual connection) and (2) computing the attended-to representation based on all the contextual representations seen so far at the previous layer (X (ℓ−1)) and the symbol’s representation at the previous layer (x′(ℓ−1)). Crucially, Eq. (52) can be computed in advance (knowing the scoring function score) for any X (ℓ) ∈OS(Σ2ℓ) and x ∈Σ2ℓfor all ℓ∈{0, . . . , L} due to the finiteness of all the considered states. We then update the set of observed contextual representations as X ′(ℓ) def= X (ℓ) ∪{x′(ℓ)} (53) to incorporate the information about the new contextual representations into the ordered sets of seen contextual representations at each layer ℓ∈{0, . . . , L}. The union in Eq. (53) is to be interpreted as adding an element to an ordered set. Defining q′ =      X ′(0) X ′(1) ... X ′(L)      (54) and setting δ(q, w) = q′, for all choices of q and w, we have defined the transition function δ that updates each state with the new contextual representations at each layer. The set of observed contextual representations X satisfies a partial order as the concatenation of a new representation at a new position at every step implies we transition into novel states. This further implies that the semiautomaton is partially ordered. We construct an automaton from A by setting the initial and final states. We set the initial state to be the one with empty ordered subsets of observed representations: X (ℓ) def= ∅for all ℓ∈{0, . . . , L}. A subset of OS(Σ2L) will lead to T accepting a string. We set the states with X L in that set to be final, leading to an equivalent automaton. ■",
        "E Duality with T P ▶, LTL[3 + ] The class of transformers T F ◀, the main focus of the paper, has a natural symmetric characterization in T P ▶: While T F ◀can only peek strictly into the past, T P ▶can symmetrically only peek strictly into the future using strict past masking and rightmost UHA. T P ▶can be informally seen as transformers in T F ◀that instead read symbols right-to-left, only considering the future when updating a symbol representation, analogously to the duality between R-trivial and L-trivial languages (Brzozowski and Ellen, 1980). Thus, all results for T F ◀and LTL[3 −] apply analogously to the symmetric case of T P ▶and LTL[3 + ]. LTL[3 + ] is the dual of LTL[3 −]—it only permits peeking into the future rather than the past. Similarly, partially ordered reverse automata (RPOFA) are semiautomata whose reversal (automaton constructed by inverting the directionality of the transitions) are POFAs. The reverse of a RPOFA is then homomorphic to a cascade product of half-resets. We thus can write the dual statement of Thm. 3.2. Theorem E.1. Let T ∈T P ▶be a transformer. Then, there exists an equivalent formula ψ ∈LTL[3 + ]. Let ψ ∈LTL[3 + ] be a formula. Then, there exists an equivalent transformer T ∈T P ▶."
      ]
    }
  ]
}